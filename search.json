[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Website Contruction",
    "section": "",
    "text": "This is how I created this website for the SWELTR temperature manipulation experiment using a) Quarto to build the site and b) GitHub Pages to host the site.\nYou will need to jump back and forth between the RStudio IDE and a terminal. You can use the terminal in RStudio if you wish, it is up to you.\nI will try to point out which code you run in the RStudio console and which you run in a terminal. I will use different code block colors to help you differentiate between the two.\nWhen you see a code block that looks like this, the command(s) must be executed in the RStudio console.\nWhen you see a code block that looks like this, the command(s) must be executed in a terminal."
  },
  {
    "objectID": "about.html#overview",
    "href": "about.html#overview",
    "title": "Website Contruction",
    "section": "Overview",
    "text": "Overview\nThere are two main options for a GitHub Pages website.\n\nA user or organization site.\n\nA project site.\n\nThe difference between the two is that a user or organization site is a single site, like https://microbes.github.io/, and individual pages on this site are indexed like so:\nhttps://microbes.github.io/about.html.\nA project site on the other hand is like a collection of sites within a main site. A project site would have the same root URL (e.g., https://microbes.github.io/) but each project would have it’s own unique extension, like these examples:\nhttps://microbes.github.io/deep-sea/ or https://microbes.github.io/forest-soils/.\nIndividual pages within a project site would be indexed like this: https://microbes.github.io/deep-sea/about.html\nAs mentioned above, the instructions here are for a GitHub Pages project site.\nFor more information see the GitHub Pages documentation. You can also check out A guide to using Github Pages."
  },
  {
    "objectID": "about.html#key-steps",
    "href": "about.html#key-steps",
    "title": "Website Contruction",
    "section": "Key Steps",
    "text": "Key Steps\nHere is an overview of what we will be doing in this tutorial:\n\nCreate organization on GitHub.\nCreate two GitHub repos for the site, one to host the organization URL and another to host the raw code plus the gh-pages branch.\nClone the code repo to your computer.\nCreate a gh-pages branch.\nBuild the initial site with Quarto for R.\nCustomize the default _quarto.yml file.\nRebuild the site.\nPush changes to GitHub."
  },
  {
    "objectID": "about.html#github-setup",
    "href": "about.html#github-setup",
    "title": "Website Contruction",
    "section": "1. GitHub Setup",
    "text": "1. GitHub Setup\nCreate organization on GitHub\n\nIf you already have an organization skip to the next step.\n\na) In the upper right-hand corner of your GitHub page click on your avatar and select Your organizations.\n\n\n\n\n\n\nb) On the page that opens, hit New organization.\n\n\n\n\n\n\nc) Next, give your organization a name, provide an email address, and indicate whether this is for a personal or institutional account. Then hit Next.\n\nThe name of the organization does not need to be the same as the website you create later.\n\n\n\n\n\n\n\n\n\n\nd) Here you can add members. For now, just hit Complete setup.\n\n\n\n\n\n\n\n\nCreate repos for the site\nIn this section, we need to create two repositories for the site: i) one to host the organization URL (i.e., https://sweltr.github.io/) and ii) another to host the raw code (e.g. .qmd files) plus the gh-pages branch. More on that in a minute. I am including instructions for things you MUST do in order to get the rest of the tutorial to work properly.\na) Hit the Create a new repository button.\n\n\n\n\n\n\n\n\n\nb) First, we create the github.io repo to host the main site.\nThere are a few things to do here:\n\nName the repo, Here we call it sweltr.github.io. it doesn’t matter what you call this but it must be unique and have the .github.io extension.\nMake sure repo is public.\n\nYou must Initialize this repository with a README File.\nHit the Create repository button. This creates the main branch of the repo.\n\n\nNote: you can host a site on GitHub without the github.io extension but that is a more advanced skill and beyond the scope of this tutorial.\n\n\n\n\n\n\n\n\n\n\nc) Next, we create the build repo to host the project source code. These steps are similar to those described above.\n\nName the repo, here we call it high-temp. This will be the extension of the project site. So this project’s URL will be https://sweltr.github.io/high-temp/.\nMake sure the repo is public.\n\nYou must Initialize this repository with a README File.\nHit the Create repository button. This creates the main branch of the repo.\n\n\nNote: you can make these repos private, however that is a bit more advanced and beyond the scope of the tutorial provided here :)\n\n\n\n\n\n\n\n\n\nReview\nHere is what we have so far:\nAn organization at https://github.com/sweltr/ that contains 2 repos, one for the GitHub Pages root URL, https://github.com/sweltr/sweltr.github.io and another for the source code, https://github.com/sweltr/high-temp. At this point both repos should only contain README files.\nWe also have two URLs, one for the organiztion site (https://sweltr.github.io/) and another for the project site (https://sweltr.github.io/high-temp/)."
  },
  {
    "objectID": "about.html#local-setup",
    "href": "about.html#local-setup",
    "title": "Website Contruction",
    "section": "2. Local Setup",
    "text": "2. Local Setup\nNow that we have the main pieces in place of GitHub, it is time to setup everything on our local machine.\nClone the Code Repo\nLet’s take a look at the high-temp repo page. There are two important things to point out. First, in the upper left corner is a box that says main. This is the primary branch for the repo and contains the source code for the site build (e.g., the .qmd files). This will become important in a moment when we create a branch called gh-pages that will link the build files (e.g., .html files) to the organization URL. More on that in a minute.\nFirst, we need to clone the high-temp repo from GitHub to our local machine. There are several ways to do this, but I will show you how I do it. Navigate to the repo page. At the time of creation, it looked like this:\n\n\n\n\n\n\n\n\n\nSee the green button that says Code. Click on that button and copy the URL.\n\n\n\n\n\n\n\n\n\nOpen a terminal window and navigate to a place where you want the repo to live on your computer. We will use the command git clone with the URL you just copied. Run the clone command and then cd into the directory.\ngit clone https://github.com/sweltr/high-temp.git\ncd high-temp/\nNow have a look at the contents of the directory using the ls command. We want to append the command with the -al flag. The l option provides extended details about each file/directory and the a option lists hidden files—files/directories that begin with a period and are hidden by default.\nls -al\nSo we have the README.md file that was generated when we created the repo and a hidden directory called .git. The .git directory is super, super important because this is how the remote repo (on GitHub) and the local repo (on your computer) keep track of changes.\ntotal 8\ndrwxr-xr-x@  4 scottjj  923590601  128 May 13 08:25 .\ndrwxr-xr-x@  5 scottjj  923590601  160 May 13 08:25 ..\ndrwxr-xr-x@ 12 scottjj  923590601  384 May 13 08:25 .git\n-rw-r--r--@  1 scottjj  923590601   11 May 13 08:25 README.md\nCreate a gh-pages branch\nGreat. We have the high-temp repo cloned on our local machine. For historical reason, a GitHub Pages site needs to be built to a directory called public/. But we do not need to upload the public/ build directory to the main branch on our repo. So, we are going to create a .gitignore file and add public/ to that file. The .gitignore tells git to ignore whatever is listed in that file when it pushes local changes to the remote repository. As you build your site, you will need to add additional items to the .gitignore file.\necho public/ >> .gitignore # Add items as needed.\nIf you run ls -al again you should see the .gitignore file is now listed in the directory. If you type nano .gitignore you can see the contents of the file.\nOK, now let’s push the changes we made to the local branch (origin) to the remote branch (main). First, run:\ngit status\nHopefully you see the .gitignore file listed in red along with some other details. This means there are untracked files in your repo. Go ahead and run:\ngit add --all\ngit status\nNow you should see the same files in green. This means the files are staged and ready to commit. Now run:\ngit commit -a -m \"initial commit\"\ngit push origin main\nIf everything worked OK, your files have been pushed to the GitHub repo. Have a look at the code repo on GitHub to confirm your changes have been pushed.\n\n\n\n\n\n\n\n\n\nNow, it is important that there is no public/ directory yet. Just to make sure there is no public/ directory, we can run the remove command in our terminal.\nrm -rf public \nNow we go through the steps to create the the gh-pages branch.\ngit checkout --orphan gh-pages\nSwitched to a new branch 'gh-pages'\ngit reset --hard\ngit commit --allow-empty -m \"Initializing gh-pages branch\"\n[gh-pages (root-commit) fd6028a] Initializing gh-pages branch\ngit push origin gh-pages\nEnumerating objects: 2, done.\nCounting objects: 100% (2/2), done.\nWriting objects: 100% (2/2), 180 bytes | 180.00 KiB/s, done.\nTotal 2 (delta 0), reused 0 (delta 0), pack-reused 0\nremote:\nremote: Create a pull request for 'gh-pages' on GitHub by visiting:\nremote:      https://github.com/sweltr/high-temp/pull/new/gh-pages\nremote:\nTo https://github.com/sweltr/high-temp.git\n * [new branch]      gh-pages -> gh-pages\ngit checkout main\nEnumerating objects: 2, done.\nCounting objects: 100% (2/2), done.\nWriting objects: 100% (2/2), 180 bytes | 180.00 KiB/s, done.\nTotal 2 (delta 0), reused 0 (delta 0), pack-reused 0\nremote:\nremote: Create a pull request for 'gh-pages' on GitHub by visiting:\nremote:      https://github.com/sweltr/high-temp/pull/new/gh-pages\nremote:\nTo https://github.com/sweltr/high-temp.git\n * [new branch]      gh-pages -> gh-pages\n(base) high-temp: scottjj$ git checkout main\nSwitched to branch 'main'\nYour branch is up to date with 'origin/main'.\ngit worktree add -B gh-pages public origin/gh-pages\nPreparing worktree (resetting branch 'gh-pages'; was at fd6028a)\nBranch 'gh-pages' set up to track remote branch 'gh-pages' from 'origin'.\nHEAD is now at fd6028a Initializing gh-pages branch\nNow run the following to see what is in the repo directory.\nls -al\ndrwxr-xr-x@  6 scottjj  923590601  192 May 13 08:41 .\ndrwxr-xr-x@  5 scottjj  923590601  160 May 13 08:25 ..\ndrwxr-xr-x@ 14 scottjj  923590601  448 May 13 08:41 .git\n-rw-r--r--@  1 scottjj  923590601    8 May 13 08:40 .gitignore\n-rw-r--r--@  1 scottjj  923590601   11 May 13 08:40 README.md\ndrwxr-xr-x@  3 scottjj  923590601   96 May 13 08:41 public\nThere should now be a public/ directory. Run the following to make sure this is a .git directory.\nls -al public/\ntotal 8\ndrwxr-xr-x@ 3 scottjj  923590601   96 May 13 08:41 .\ndrwxr-xr-x@ 6 scottjj  923590601  192 May 13 08:41 ..\n-rw-r--r--@ 1 scottjj  923590601  104 May 13 08:41 .git\nReview\nAt this point all the pieces are in place to build your site and host it on GitHub. In addition to the repos you have on GitHub, you should have a local directory of the code repo, in our case it is called high-temp. In this directory there should be a README.md file, a .gitignore file, a .git directory, and the public/ directory."
  },
  {
    "objectID": "about.html#site-build",
    "href": "about.html#site-build",
    "title": "Website Contruction",
    "section": "3. Site Build",
    "text": "3. Site Build\nTime to build the site. As mentioned in the intro, we will use Quarto to create the website. To keep things simple, we will generate the initial site in a temporary directory and copy the files to the git formatted directory at the end.\nOpen RStudio and hit File >> New Project. In the pop-up window hit New Project.\n\n\n\n\n\n\n\n\n\nThen in the New Project Wizard select the New Directory option.\n\n\n\n\n\n\n\n\n\nUnder Project Type, scroll down until you see Quarto Website and select that option.\n\n\n\n\n\n\n\n\n\nGive the Directory a name and hit Create Project. Consider this an intermediate step in the process. We need to create a project directory to generate the initial site. Eventually we will copy all of these file to the directory of the repo we cloned earlier. You can call this something like web if you wish, it doesn’t really matter since this is only a temporary directory.\n\n\n\n\n\n\n\n\n\nNow, if everything worked OK, you should see that five files were created and two of the files are open in your RStudio IDE.\n– index.qmd. This is the landing page of your site. This file is the first thing people see when they navigate to your site.\n– _quarto.yml. This is a YAML file and is used to configure your site. DO NOT erase this file.\n\n\n\n\n\n\n\n\n\nIn fact, we need to edit the _quarto.yml file to configure it for our site. As you move further into building your site, you will need to make many modifications to this file. For now however, we will just focus on the minimum changes you need to make to get your site online. Go ahead and click on that file. This is what the default file looks like.\n\n\n\n\n\n\n\n\n\nFirst, below the title line (nested under website) add the following lines.\nsite-url: https://sweltr.github.io/high-temp/\nrepo-url: https://github.com/sweltr/high-temp/\nThe site-url is the .github.io URL we created way back when we created the site repo. The repo-url is the repo we created to host the project source code.\nWhen you build or render your site, RStudio will dump all the site files (e.g., .html files) in the output_dir. The default output directory is _site, which RStudio generates when you render the site for the first time. I see no reason why you cannot leave the build directory as is but I prefer to call mine public_build/. Go ahead and add a line of the _quarto.yml file specifying the output-dir. This should be nested under project like so:\nproject:\n  type: website\n  output-dir: public_build\n\nGreat. One last thing to do. When you render your site, RStudio will try to add everything in the main directory to the output directory, in this case public_build/. There are many cases where this is not desirable. For example, let’s say we have you have a directory called sequence_data that contains a bunch of fastq files. We do not need these files to build the site (and we probably do not want these on GitHub) so we need to tell RStudio to ignore these files when it builds the site. For that, we add the line render nested under project. Here we can specify what is, and is not, rendered when the site is build. As you get deeper into building your site, you will likely have many files/directories that you want excluded.\n\nDon’t get confused here. The render line in the _quarto.yml file is different than the .gitignore file discussed earlier. They do completely different things. If this is unclear, go back and review the section on the .gitignore file.\n\nFor now, we will focus on the necessary things to include and exclude from the build. Items must be listed on separate lines, in double quotes. So let’s say we want to render all .qmd files but ignore a few directories. The way to code this is that anything we want to exclude should begin with an exclamation point (!). Go ahead and add these lines.\nproject:\n  type: website\n  output-dir: public_build\n  render:\n    - \"*.qmd\"\n    - \"!public/\"\n    - \"!public_build/\"\nOK, a little explanation. The first thing we need to exclude in the actual build directory public_build/ (or _site if you kept the default). If we do not include this, RStudio will get stuck in a loop where it continues to add public_build/ to the public_build/ directory. Next, remember that to host a gh-pages site, the build files (e.g., the .html files) must be in a directory called public/. We created this directory when we created the gh-pages branch.\n\nSo why don’t we just make the buld directory public/ instead of public_build/ or _site/?\n\nGood question. The reason is that public/ is a special directory that contains git related files (hidden by default). When we render a site, RStudio will first overwrite the output directory, thereby deleting the .git files, and then recreate the output directory add the new build files. This is not cool. To avoid this, we use an intermediate output directory, public_build/ (or whatever you call it), to generate the site. Than we can copy all the files from the output directory to the public/ directory without losing the git info. This is a little cumbersome but I have not yet found a workaround. This is still an issue as of this writing.\nIf you changed the name of the default build directory, go ahead and remove the default directory now. In your terminal type:\nrm -r _site\nNow, save all of your changes to the _quarto.yml file and in the RStudio Console run the following:\n\nquarto::quarto_render()\n\nYou should see a new directory called public_build/. Open that folder and double-click on index.html. The homepage of your new site should open in your default browser.\nReview\nSweet. Let’s review what we have so far. From the 1. GitHub Setup we created two repos, one to host the .github.io organization and another to host the raw code (e.g. .qmd files) plus the gh-pages branch. In 2. Local Setup we cloned the code repo and create a gh-pages branch in a directory called public/. In 3. Site Build we generated an initial site within a dummy directory, modified the _quarto.yml file, and re-rendered the site to an output directory called public_build/"
  },
  {
    "objectID": "about.html#integration",
    "href": "about.html#integration",
    "title": "Website Contruction",
    "section": "Integration",
    "text": "Integration\nTime to push our changes to GitHub. The first step is to copy all of the files in your dummy site directory to the directory of the cloned repo (high-temp).\n\n\n\n\n\n\n\n\n\nFirst, in your terminal, navigate to the cloned repo directory. Use the pwd command to confirm your location.\npwd\n/Users/scottjj/Dropbox (Smithsonian)/GITHUB_PROJECTS/SWELTR/high-temp\nAssuming your directory structure setup is similar to the image above, next run the following to copy all of the files.\ncp -r ../dummy/* .\nNow take a look at the contents of the current working directory.\nls -al\nIt should look something like this:\ndrwxr-xr-x  14 scottjj  923590601   448 May 13 09:18 .\ndrwxr-xr-x@  7 scottjj  923590601   224 May 13 09:15 ..\n-rw-r--r--@  1 scottjj  923590601  6148 May 13 09:18 .DS_Store\n-rw-r--r--   1 scottjj  923590601    50 May 13 09:18 .Rhistory\ndrwxr-xr-x   4 scottjj  923590601   128 May 13 09:18 .Rproj.user\ndrwxr-xr-x@ 14 scottjj  923590601   448 May 13 08:41 .git\n-rw-r--r--@  1 scottjj  923590601     8 May 13 08:40 .gitignore\n-rw-r--r--@  1 scottjj  923590601    11 May 13 08:40 README.md\n-rw-r--r--   1 scottjj  923590601   202 May 13 09:18 _quarto.yml\n-rw-r--r--   1 scottjj  923590601    58 May 13 09:18 about.qmd\n-rw-r--r--   1 scottjj  923590601   145 May 13 09:18 index.qmd\n-rw-r--r--   1 scottjj  923590601    17 May 13 09:18 styles.css\ndrwxr-xr-x@  3 scottjj  923590601    96 May 13 08:41 public\ndrwxr-xr-x   7 scottjj  923590601   224 May 13 09:18 public_build\n-rw-r--r--   1 scottjj  923590601   225 May 13 09:18 web.Rproj\nAt this point we need to do a few final housekeeping tasks. First, let’s add a few additional files to the .gitignore file.\nRun this command in your terminal. You should only see the public/ directory listed.\nnano .gitignore\nAdd the following lines, save the file, and exit nano.\n.Rproj.user\n.Rhistory\n.RData\n.Ruserdata\n.Rapp.history\n.DS_Store\nweb.Rproj\n\npublic_build/\nNow, copy all of the files from public_build/ to public/.\ncp -r public_build/* public/\nThen move into the public/ directory and run git status.\ncd public\ngit status\nAll the untracked or changed files should be in red.\nOn branch gh-pages\nYour branch is up to date with 'origin/gh-pages'.\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n    about.html\n    index.html\n    search.json\n    site_libs/\n    sitemap.xml\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nNow add the files to be committed and check the status again.\ngit add --all\ngit status\nAll the untracked or changed files should now be green.\nOn branch gh-pages\nYour branch is up to date with 'origin/gh-pages'.\n\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n    new file:   about.html\n    new file:   index.html\n    new file:   search.json\n    new file:   site_libs/anchor-4.2.2/anchor.min.js\n    new file:   site_libs/autocomplete-0.37.1/autocomplete.min.js\n    new file:   site_libs/bowser-1.9.3/bowser.min.js\n    new file:   site_libs/distill-2.2.21/template.v2.js\n    new file:   site_libs/font-awesome-5.1.0/css/all.css\n    new file:   site_libs/font-awesome-5.1.0/css/v4-shims.css\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.eot\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.svg\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.ttf\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.woff\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.woff2\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.eot\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.svg\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.ttf\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.woff\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.woff2\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.eot\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.svg\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.ttf\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.woff\n    new file:   site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.woff2\n    new file:   site_libs/fuse-6.4.1/fuse.min.js\n    new file:   site_libs/header-attrs-2.11/header-attrs.js\n    new file:   site_libs/headroom-0.9.4/headroom.min.js\n    new file:   site_libs/jquery-3.6.0/jquery-3.6.0.js\n    new file:   site_libs/jquery-3.6.0/jquery-3.6.0.min.js\n    new file:   site_libs/jquery-3.6.0/jquery-3.6.0.min.map\n    new file:   site_libs/popper-2.6.0/popper.min.js\n    new file:   site_libs/tippy-6.2.7/tippy-bundle.umd.min.js\n    new file:   site_libs/tippy-6.2.7/tippy-light-border.css\n    new file:   site_libs/tippy-6.2.7/tippy.css\n    new file:   site_libs/tippy-6.2.7/tippy.umd.min.js\n    new file:   site_libs/webcomponents-2.0.0/webcomponents.js\n    new file:   sitemap.xml\nCommit the files.\ngit commit -m \"Publishing to gh-pages initial commit\"\n[gh-pages f30f1ff] Publishing to gh-pages initial commit\n 37 files changed, 22182 insertions(+)\n create mode 100644 about.html\n create mode 100644 index.html\n create mode 100644 search.json\n create mode 100644 site_libs/anchor-4.2.2/anchor.min.js\n create mode 100644 site_libs/autocomplete-0.37.1/autocomplete.min.js\n create mode 100644 site_libs/bowser-1.9.3/bowser.min.js\n create mode 100644 site_libs/distill-2.2.21/template.v2.js\n create mode 100644 site_libs/font-awesome-5.1.0/css/all.css\n create mode 100644 site_libs/font-awesome-5.1.0/css/v4-shims.css\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.eot\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.svg\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.ttf\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.woff\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-brands-400.woff2\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.eot\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.svg\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.ttf\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.woff\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-regular-400.woff2\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.eot\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.svg\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.ttf\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.woff\n create mode 100644 site_libs/font-awesome-5.1.0/webfonts/fa-solid-900.woff2\n create mode 100644 site_libs/fuse-6.4.1/fuse.min.js\n create mode 100644 site_libs/header-attrs-2.11/header-attrs.js\n create mode 100644 site_libs/headroom-0.9.4/headroom.min.js\n create mode 100644 site_libs/jquery-3.6.0/jquery-3.6.0.js\n create mode 100644 site_libs/jquery-3.6.0/jquery-3.6.0.min.js\n create mode 100644 site_libs/jquery-3.6.0/jquery-3.6.0.min.map\n create mode 100644 site_libs/popper-2.6.0/popper.min.js\n create mode 100644 site_libs/tippy-6.2.7/tippy-bundle.umd.min.js\n create mode 100644 site_libs/tippy-6.2.7/tippy-light-border.css\n create mode 100644 site_libs/tippy-6.2.7/tippy.css\n create mode 100644 site_libs/tippy-6.2.7/tippy.umd.min.js\n create mode 100644 site_libs/webcomponents-2.0.0/webcomponents.js\n create mode 100644 sitemap.xml\nAnd finally push the files to the gh-pages branch.\ngit push origin gh-pages\nEnumerating objects: 55, done.\nCounting objects: 100% (55/55), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (45/45), done.\nWriting objects: 100% (54/54), 1.37 MiB | 348.00 KiB/s, done.\nTotal 54 (delta 1), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (1/1), done.\nTo https://github.com/sweltr/high-temp.git\n   fd6028a..f30f1ff  gh-pages -> gh-pages\nDo the same for the build files. Make sure you back out of the public directory with the cd ../ command. This time you are pushing to the main branch, not the gh-pages branch.\ncd ../\ngit status\ngit add --all\ngit status\ngit commit -m \"Publishing to gh-pages initial commit\"\ngit push origin main\nNow head over to your GitHub code repo and look at the two branches to make sure everything looks good. In this case, https://github.com/sweltr/high-temp. You should see the main branch of this repo—this contains all of the .qmd files.\n\n\n\n\n\n\n\n\n\nFind the button in the upper left that says main. Click on that and select gh-pages.\n\n\n\n\n\n\n\n\n\nThe gh-pages branch contains all of the .html files.\n\n\n\n\n\n\n\n\n\nWait a few minutes and navigate to your project webpage. In our case, https://sweltr.github.io/high-temp/.\nYou site should be LIVE!!!"
  },
  {
    "objectID": "about.html#adding-content",
    "href": "about.html#adding-content",
    "title": "Website Contruction",
    "section": "Adding content",
    "text": "Adding content\nMost of the steps up to this point should only need to be done once. Now it is time to add content to your site, customize the look, etc. From this point forward, whenever you make changes that you want to make public, you need to do the following:\n\nRender the site.\n\n\nquarto::quarto_render()\n\nOr to build individual pages run:\n\nquarto::quarto_render(\"new_page.qmd\")\n\n\nCopy files from public_build/ to public/\n\n\ncp -r public_build/* public/\n\nCommit and push the changes to the gh-pages branch.\n\ncd public/\ngit status\ngit add --all\ngit status\ngit commit -m \"added new page\"\ngit push origin gh-pages\n\nCommit and push the changes to the main branch.\n\ncd ../\ngit status\ngit add --all\ngit status\ngit commit -m \"added new page\"\ngit push origin main"
  },
  {
    "objectID": "alpha.html",
    "href": "alpha.html",
    "title": "5. Alpha Diversity Estimates",
    "section": "",
    "text": "Click here for setup information.\n\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(119)\n#library(conflicted)\nlibrary(phyloseq); packageVersion(\"phyloseq\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, hilldiv, \n               microbiome, phytools, phangorn, \n               pairwiseAdonis, naniar, \n               labdsv, patchwork, agricolae, ggpubr,\n               reactable, downloadthis, captioner,\n               install = FALSE, update = FALSE)\nsource(\"hack_code/div_test_plot_jjs.R\")\nsource(\"hack_code/div_test_jjs.R\")\n\noptions(scipen=999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))"
  },
  {
    "objectID": "alpha.html#workflow-input",
    "href": "alpha.html#workflow-input",
    "title": "5. Alpha Diversity Estimates",
    "section": "Workflow Input",
    "text": "Workflow Input\nFiles needed to run this workflow can be downloaded from figshare."
  },
  {
    "objectID": "alpha.html#calculate-hill-numbers",
    "href": "alpha.html#calculate-hill-numbers",
    "title": "5. Alpha Diversity Estimates",
    "section": "Calculate Hill Numbers",
    "text": "Calculate Hill Numbers\nTo calculate Hill numbers, we use the R package hilldiv (Alberdi and Gilbert 2019b). We calculate three metrics that put more or less weight on common species:\n\nObserved richness, where q-value = 0.\nShannon exponential, which weighs ASVs by their frequency, where q-value = 1.\nSimpson multiplicative inverse, which over weighs abundant ASVs, where q-value = 2.\n\nWe perform each analysis against the Full (unfiltered) data set as well as the Arbitrary, PERfect and PIME, filtered data sets using the function hill_div.\nThe command is as follows:\nhill_div(count = x, qvalue = i, tree = ultrametric_tree)\nwhere x is the sample by ASV table, i is the q-value corresponding to the metric of interest and tree is an ultrametric formatted phylogenetic tree if you choose to look at lineage, rather than ASV, diversity.\n\n\n\n\n\n\n\n\n\n\n\n\nWe first transform all the data to relative abundance values, and compute new trees.\n\nCodessu18_alpha_ds <- c(\"ssu18_ps_work\", \"ssu18_ps_filt\", \"ssu18_ps_perfect\", \"ssu18_ps_pime\")\nfor (i in ssu18_alpha_ds) {\n  tmp_ps <- transform_sample_counts(get(i), function(otu) otu/sum(otu))\n  tmp_ps@phy_tree <- NULL\n  tmp_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE,\n                      tip.label = taxa_names(tmp_ps))\n  tmp_ps_norm <- merge_phyloseq(tmp_ps, sample_data, tmp_tree)\n  tmp_asv <- data.frame(t(otu_table(tmp_ps_norm)))\n  tmp_ps_name <- purrr::map_chr(i, ~ paste0(., \"_norm\"))\n  assign(tmp_ps_name, tmp_ps_norm)\n  tmp_asv_name <- purrr::map_chr(i, ~ paste0(., \"_tu\"))\n  assign(tmp_asv_name, tmp_asv)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nNext, we run the analysis for all three metrics on the data sets (without a tree).\n\nCodeqvalue <- c(0,1,2)\nfor (i in qvalue) {\n  for (j in ssu18_alpha_ds) {\n     tmp_asv <- get(purrr::map_chr(j, ~ paste0(., \"_tu\")))\n     tmp_df <- data.frame(hill_div(tmp_asv, qvalue = i))\n     tmp_df <- tmp_df %>% dplyr::rename(\"tmp_name\" = 1) %>%\n                              tibble::rownames_to_column(\"SamName\")\n     tmp_name <- purrr::map_chr(j, ~ paste0(., \"_h\", i))\n     print(tmp_name)\n     assign(tmp_name, tmp_df)\n     rm(list = ls(pattern = \"tmp_\"))\n  }\n}\nobjects(pattern = \"_h\")\n\n\nAnd make summary tables to add back into each ps object.\n\nCodefor (i in ssu18_alpha_ds) {\n     tmp_obs <- get(purrr::map_chr(i, ~ paste0(., \"_h0\")))\n     tmp_sha <- get(purrr::map_chr(i, ~ paste0(., \"_h1\")))\n     tmp_sim <- get(purrr::map_chr(i, ~ paste0(., \"_h2\")))\n     tmp_hill <- dplyr::left_join(tmp_obs, tmp_sha, by = \"SamName\") %>%\n       dplyr::left_join(., tmp_sim, by = \"SamName\") %>%\n       dplyr::rename(\"Observed\" = 2, \"Shannon_exp\" = 3, \"InvSimpson\" = 4)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_hill\"))\n     assign(tmp_name, tmp_hill)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_hill\")\nobjects()\n\n\nAnd then create the new objects with the diversity data.\n\nCodefor (i in ssu18_alpha_ds) {\n     tmp_ps <- get(i)\n     tmp_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE,\n                      tip.label = taxa_names(tmp_ps))\n     tmp_ps <- merge_phyloseq(tmp_ps, sample_data, tmp_tree)\n     tmp_hill <- get(purrr::map_chr(i, ~ paste0(., \"_hill\")))\n     tmp_hill_samp <- dplyr::left_join(data.frame(sample_data(tmp_ps)),\n                                         tmp_hill, by = \"SamName\")\n     tmp_hill_samp$ID <- tmp_hill_samp$SamName\n     tmp_hill_samp <- tmp_hill_samp %>%  tibble::column_to_rownames(\"ID\")\n     tmp_ps2 <- merge_phyloseq(otu_table(tmp_ps),\n                                sample_data(tmp_hill_samp),\n                                tax_table(tmp_ps),\n                                phy_tree(tmp_ps))\n     assign(i, tmp_ps2)\n     tmp_path <- file.path(\"files/alpha/rdata/\")\n     saveRDS(tmp_ps2, paste(tmp_path, i, \".rds\", sep = \"\"))\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()"
  },
  {
    "objectID": "alpha.html#hill-numbers-summary",
    "href": "alpha.html#hill-numbers-summary",
    "title": "5. Alpha Diversity Estimates",
    "section": "Hill Numbers Summary",
    "text": "Hill Numbers Summary\nNow we summarize the data for each sample against all three metrics. The table contains the results of ASV diversity estimates from the full data set and the three filtered data sets.\nThe suffix _fi indicates metrics for the Arbitrary data set. The suffix _pe indicates metrics for the PERfect data set and the suffix _pet indicates the lineage diversity for the PERfect data set.\nThe suffix _pi indicates metrics for the PIME data set and the suffix _pit indicates the lineage diversity for the PIME data set.\n\n\n\n\n\n\nHill Summary\n(16S rRNA) Table 1 | Hill numbers for samples in the unfiltered & filtered data sets.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "alpha.html#normality-tests",
    "href": "alpha.html#normality-tests",
    "title": "5. Alpha Diversity Estimates",
    "section": "Normality Tests",
    "text": "Normality Tests\nBefore running significance tests, we need to know if data is normally distributed, which will tell use whether to use a parametric or non-parametric test. To test if the data are normally distributed, we use the Shapiro-Wilk Normality test and the Bartlett Test of Homogeneity of Variances.\nIf the p-values are both not significant (p > 0.05) from the tests, we accept the null hypothesis (that the results are normally distributed) and test for significance between samples using an ANOVA. If the p-values are both significant (p < 0.05), we reject the null hypothesis (that the results are normally distributed) and test for significance between samples using Kruskal-Wallis (non-parametric equivalent of ANOVA).\nThe commands are as follows:\nshapiro.test(x), where x is a numeric vector of alpha diversity values from the sample data table.\nbartlett.test(Value ~ Group, data = df) Where Value is the metric of interest, Group in the treatment to compare, and df is the data frame.\nFirst the Shapiro-Wilk Normality test.\n\nCodessu_tab_alpha_div\nssu18_div_tab <- ssu_tab_alpha_div\nssu18_shap_tests_asv <- c()\nfor (i in colnames(ssu18_div_tab[,7:ncol(ssu18_div_tab)])) {\n   tmp_name <- purrr::map_chr(i, ~ paste0(\"ssu18_shap_asv_\", .))\n   ssu18_shap_tests_asv <- append(ssu18_shap_tests_asv, tmp_name)\n   tmp_test <- eval(shapiro.test(ssu18_div_tab[[i]]))\n   tmp_test$data.name <- tmp_name\n   assign(tmp_name, tmp_test)\n   rm(list = ls(pattern = \"tmp_\"))\n}\n\nobjects(pattern = \"shap_\")\n\n\nAnd then the Bartlett Test of Homogeneity of Variances.\n\nCodessu18_div_tab_asv <- ssu_tab_alpha_div\nssu18_bart_tests_asv <- c()\nfor (i in colnames(ssu18_div_tab_asv[,7:ncol(ssu18_div_tab_asv)])) {\n   tmp_name <- purrr::map_chr(i, ~ paste0(\"ssu18_bart_asv_\", .))\n   ssu18_bart_tests_asv <- append(ssu18_bart_tests_asv, tmp_name)\n   tmp_test <- eval(bartlett.test(ssu18_div_tab_asv[[i]] ~ TEMP, data = ssu18_div_tab_asv))\n   tmp_test$data.name <- tmp_name\n   assign(tmp_name, tmp_test)\n   rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_bart_\")\n\n\nHere we see which Shapiro-Wilk Normality and Bartlett tests were significant and which were not. So wherever the value of both p-values in > 0.05 we can use an ANOVA, otherwise we use Kruskal-Wallis.\n\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 3 rows [1, 5, 9].\n\n\n(16S rRNA) Table 2 | Summary of Normality Tests for unfiltered & filtered data sets. P-values in red indicate significance (p-value < 0.05).\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "alpha.html#significance-tests",
    "href": "alpha.html#significance-tests",
    "title": "5. Alpha Diversity Estimates",
    "section": "Significance Tests",
    "text": "Significance Tests\nTo begin, we need to create a hierarchy variable; a two-column matrix specifying the relationship between samples (first column) and groups (second column).\n\nCodessu18_hill_hier <- ssu_tab_alpha_div\nssu18_hill_hier <- ssu18_hill_hier %>% dplyr::select(\"Sample_ID\", \"TEMP\") %>%\n  tibble::remove_rownames()\nssu18_hill_hier <- ssu18_hill_hier[order(ssu18_hill_hier$TEMP), ]\nssu18_hill_hier$TEMP = paste(ssu18_hill_hier$TEMP, 'C', sep='')\nssu18_hill_hier <- ssu18_hill_hier %>% tibble::remove_rownames()\nsaveRDS(ssu18_hill_hier, \"files/alpha/rdata/ssu18_hill_hier.rds\")\n\n\nAgain, we start by testing significance of ASV diversity for the data sets against each of the three metrics using the div_test function.\nThe command is as follows:\ndiv_test(countable = x, qvalue = i, hierarchy = hier, \n         tree = ultrametric_tree, posthoc = TRUE)\nwhere x is ASV by sample table, i is the q-value corresponding to the metric of interest, hier is the hierarchy matrix, tree is an ultrametric formatted phylogenetic tree (if you choose to explore lineage diversity), and posthoc indicates whether to run post hoc pairwise analyses.\n\nCodeqvalue <- c(0,1,2)\nfor (i in ssu18_alpha_ds) {\n     for (j in qvalue) {\n         tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_tu\")))\n         tmp_test <- div_test(tmp_get, qvalue = j,\n                      hierarchy = ssu18_hill_hier,\n                      posthoc = TRUE)\n         tmp_name <- purrr::map_chr(i, ~ paste0(., \"_q\", j, \"_adt\"))\n         print(tmp_name)\n         assign(tmp_name, tmp_test)\n         rm(list = ls(pattern = \"tmp_\"))\n }\n}"
  },
  {
    "objectID": "alpha.html#summary",
    "href": "alpha.html#summary",
    "title": "5. Alpha Diversity Estimates",
    "section": "Summary",
    "text": "Summary\n(16S rRNA) Table 3 | Summary of significance tests for unfiltered & filtered data sets. P-values in red for post hoc analysis indicate significance (p-value < 0.05).\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "alpha.html#posthoc-analyses",
    "href": "alpha.html#posthoc-analyses",
    "title": "5. Alpha Diversity Estimates",
    "section": "PostHoc Analyses",
    "text": "PostHoc Analyses\nFirst let’s check the results of each posthoc analysis.\n\n\n\nObserved (q-value = 0)\nShannon exp (q-value = 1)\nInverse Simpson (q-value = 2)\n\n\n\n\n\n[1] \"FULL (Observed)\"\n[1] \"Tukey post-hoc test\"\n        diff       lwr       upr      p.adj\n3C-0C -219.6  -956.946 517.74597 0.71329233\n8C-0C -721.0 -1458.346  16.34597 0.05546473\n8C-3C -501.4 -1238.746 235.94597 0.20658985\n\n\n\n\n[1] \"FILT (Observed)\"\n[1] \"Tukey post-hoc test\"\n        diff       lwr      upr      p.adj\n3C-0C   -6.0 -221.6963 209.6963 0.99696904\n8C-0C -283.8 -499.4963 -68.1037 0.01109212\n8C-3C -277.8 -493.4963 -62.1037 0.01267274\n\n\n\n\n[1] \"PIME (Observed)\"\n[1] \"Tukey post-hoc test\"\n        diff       lwr       upr          p.adj\n3C-0C  -46.6 -140.1306   46.9306 0.406733563435\n8C-0C -325.6 -419.1306 -232.0694 0.000002206313\n8C-3C -279.0 -372.5306 -185.4694 0.000011005557\n\n\n\n\n[1] \"PERfect (Observed)\"\n[1] \"Tukey post-hoc test\"\n        diff       lwr       upr      p.adj\n3C-0C    2.4 -191.8227 196.62267 0.99940103\n8C-0C -174.2 -368.4227  20.02267 0.08059254\n8C-3C -176.6 -370.8227  17.62267 0.07616681\n\n\n\n\n\n\n[1] \"FULL (Shannon exponential)\"\n[1] \"Tukey post-hoc test\"\n           diff       lwr        upr         p.adj\n3C-0C -246.4375 -442.9939  -49.88101 0.01492642034\n8C-0C -511.4071 -707.9635 -314.85066 0.00004295672\n8C-3C -264.9696 -461.5261  -68.41321 0.00950353162\n\n\n\n\n[1] \"FILT (Shannon exponential)\"\n[1] \"Tukey post-hoc test\"\n           diff       lwr        upr          p.adj\n3C-0C  -96.4627 -177.3128  -15.61258 0.019965252769\n8C-0C -283.1154 -363.9656 -202.26533 0.000002072881\n8C-3C -186.6527 -267.5029 -105.80262 0.000133670064\n\n\n\n\n[1] \"PIME (Shannon exponential)\"\n[1] \"Dunn test with Benjamini-Hochberg correction\"\n             Z     P.unadj       P.adj\n0C-3C 1.767767 0.077099872 0.077099872\n0C-8C 3.535534 0.000406952 0.001220856\n3C-8C 1.767767 0.077099872 0.115649808\n\n\n\n\n[1] \"PERfect (Shannon exponential)\"\n[1] \"Tukey post-hoc test\"\n            diff       lwr        upr          p.adj\n3C-0C  -82.87871 -155.6134  -10.14407 0.025808833615\n8C-0C -232.15045 -304.8851 -159.41581 0.000005488596\n8C-3C -149.27174 -222.0064  -76.53710 0.000385348747\n\n\n\n\n\n\n[1] \"FULL (Inverse Simpson)\"\n[1] \"Tukey post-hoc test\"\n           diff       lwr        upr         p.adj\n3C-0C -157.7331 -251.7760  -63.69027 0.00202540187\n8C-0C -249.3938 -343.4367 -155.35097 0.00003566326\n8C-3C  -91.6607 -185.7036    2.38217 0.05628905499\n\n\n\n\n[1] \"FILT (Inverse Simpson)\"\n[1] \"Tukey post-hoc test\"\n            diff       lwr        upr           p.adj\n3C-0C  -84.38551 -126.1609  -42.61016 0.0004422940804\n8C-0C -158.66732 -200.4427 -116.89197 0.0000008678593\n8C-3C  -74.28181 -116.0572  -32.50646 0.0012804120273\n\n\n\n\n[1] \"PIME (Inverse Simpson)\"\n[1] \"Tukey post-hoc test\"\n            diff        lwr       upr          p.adj\n3C-0C  -61.82853  -93.26452 -30.39255 0.000556162049\n8C-0C -122.59975 -154.03574 -91.16377 0.000000651479\n8C-3C  -60.77122  -92.20720 -29.33524 0.000643758728\n\n\n\n\n[1] \"PERfect (Inverse Simpson)\"\n[1] \"Tukey post-hoc test\"\n            diff       lwr        upr          p.adj\n3C-0C  -80.17581 -122.1927  -38.15894 0.000718157610\n8C-0C -146.57155 -188.5884 -104.55467 0.000002158578\n8C-3C  -66.39574 -108.4126  -24.37887 0.003172202683\n\n\n\n\n\n\nNow we can plot the results from the posthoc analyses for each metric and data set using the function div_test_plot_jjs. I modified the original function (div_test_plot) to control a little of the formatting.\nThe command is as follows:\ndiv_test_plot(divtest = x, chart = \"type\", colour = col.pal, \n              posthoc = TRUE, threshold = value))\nwhere x is the results from the div_test function, \"type\" is chart type (box, jitter, or violin), colour is is a color palette, posthoc indicates whether to run posthoc pairwise analyses, and value is the maximum p-value to show in pairwise posthoc results. WARNING if none of the posthoc results are below the specified threshold, the function will throw an error. Therefore, until this is fixed, all posthoc values are shown.\n\nCodesource(\"hack_code/div_test_plot_jjs.R\")\nrm(list = ls(pattern=\"_adt_plot\"))\nfor (i in objects(pattern=\"_adt\")) {\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_plot\"))\n     tmp_get <- get(i)\n     tmp_df <- div_test_plot_jjs(tmp_get, chart = \"box\",\n                                 colour = swel_col, posthoc = TRUE)\n     tmp_df <- ggpar(tmp_df, legend = \"none\")\n     print(tmp_name)\n     assign(tmp_name, tmp_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nssu18_ps_work_q0_adt_plot"
  },
  {
    "objectID": "alpha.html#alpha-diversity-plots",
    "href": "alpha.html#alpha-diversity-plots",
    "title": "5. Alpha Diversity Estimates",
    "section": "Alpha Diversity Plots",
    "text": "Alpha Diversity Plots\nPosthoc adjusted p-values given for each pairwise comparison.\n\n\n\n\n\n(16S rRNA) Figure 1 | Alpha diversity plots. Top row = Observed; middle row = Shannon exponential; bottom row = Inverse Simpson. From left to right, Full (unfiltered) data, Arbitrary filtered, PERfect filtered, PIME filtered."
  },
  {
    "objectID": "alpha.html#calculate-hill-numbers-1",
    "href": "alpha.html#calculate-hill-numbers-1",
    "title": "5. Alpha Diversity Estimates",
    "section": "Calculate Hill Numbers",
    "text": "Calculate Hill Numbers\nTo calculate Hill numbers, we use the R package hilldiv (Alberdi and Gilbert 2019b). We calculate three metrics that put more or less weight on common species:\n\nObserved richness, where q-value = 0.\nShannon exponential, which weighs ASVs by their frequency, where q-value = 1.\nSimpson multiplicative inverse, which over weighs abundant ASVs, where q-value = 2.\n\nWe perform each analysis against the Full (unfiltered) data set as well as the Arbitrary, PERfect and PIME, filtered data sets using the function hill_div.\nThe command is as follows:\nhill_div(count = x, qvalue = i, tree = ultrametric_tree)\nwhere x is the sample by ASV table, i is the q-value corresponding to the metric of interest and tree is an ultrametric formatted phylogenetic tree, however this really doesn’t apply for ITS data anyway.\nWe first transform all the data to relative abundance values, and compute new trees.\n\nCodeits18_alpha_ds <- c(\"its18_ps_work\", \"its18_ps_filt\", \"its18_ps_perfect\", \"its18_ps_pime\")\nfor (i in its18_alpha_ds) {\n  tmp_ps <- transform_sample_counts(get(i), function(otu) otu/sum(otu))\n  tmp_ps@phy_tree <- NULL\n  tmp_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE,\n                      tip.label = taxa_names(tmp_ps))\n  tmp_ps_norm <- merge_phyloseq(tmp_ps, sample_data, tmp_tree)\n  tmp_asv <- data.frame(t(otu_table(tmp_ps_norm)))\n  tmp_ps_name <- purrr::map_chr(i, ~ paste0(., \"_norm\"))\n  assign(tmp_ps_name, tmp_ps_norm)\n  tmp_asv_name <- purrr::map_chr(i, ~ paste0(., \"_tu\"))\n  assign(tmp_asv_name, tmp_asv)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nNext, we run the analysis for all three metrics on the data sets (without a tree).\n\nCodeqvalue <- c(0,1,2)\nfor (i in qvalue) {\n  for (j in its18_alpha_ds) {\n     tmp_asv <- get(purrr::map_chr(j, ~ paste0(., \"_tu\")))\n     tmp_df <- data.frame(hill_div(tmp_asv, qvalue = i))\n     tmp_df <- tmp_df %>% dplyr::rename(\"tmp_name\" = 1) %>%\n                              tibble::rownames_to_column(\"SamName\")\n     tmp_name <- purrr::map_chr(j, ~ paste0(., \"_h\", i))\n     print(tmp_name)\n     assign(tmp_name, tmp_df)\n     rm(list = ls(pattern = \"tmp_\"))\n  }\n}\nobjects(pattern = \"_h\")\n\n\nAnd make summary tables to add back into each ps object.\n\nCodefor (i in its18_alpha_ds) {\n     tmp_obs <- get(purrr::map_chr(i, ~ paste0(., \"_h0\")))\n     tmp_sha <- get(purrr::map_chr(i, ~ paste0(., \"_h1\")))\n     tmp_sim <- get(purrr::map_chr(i, ~ paste0(., \"_h2\")))\n     tmp_hill <- dplyr::left_join(tmp_obs, tmp_sha, by = \"SamName\") %>%\n       dplyr::left_join(., tmp_sim, by = \"SamName\") %>%\n       dplyr::rename(\"Observed\" = 2, \"Shannon_exp\" = 3, \"InvSimpson\" = 4)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_hill\"))\n     assign(tmp_name, tmp_hill)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_hill\")\nobjects()\n\n\nAnd then create the new objects with the diversity data.\n\nCodefor (i in its18_alpha_ds) {\n     tmp_ps <- get(i)\n     tmp_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE,\n                      tip.label = taxa_names(tmp_ps))\n     tmp_ps <- merge_phyloseq(tmp_ps, sample_data, tmp_tree)\n     tmp_hill <- get(purrr::map_chr(i, ~ paste0(., \"_hill\")))\n     tmp_hill_samp <- dplyr::left_join(data.frame(sample_data(tmp_ps)),\n                                         tmp_hill, by = \"SamName\")\n     tmp_hill_samp$ID <- tmp_hill_samp$SamName\n     tmp_hill_samp <- tmp_hill_samp %>%  tibble::column_to_rownames(\"ID\")\n     tmp_ps2 <- merge_phyloseq(otu_table(tmp_ps),\n                                sample_data(tmp_hill_samp),\n                                tax_table(tmp_ps),\n                                phy_tree(tmp_ps))\n     assign(i, tmp_ps2)\n     tmp_path <- file.path(\"files/alpha/rdata/\")\n     saveRDS(tmp_ps2, paste(tmp_path, i, \".rds\", sep = \"\"))\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()"
  },
  {
    "objectID": "alpha.html#hill-numbers-summary-1",
    "href": "alpha.html#hill-numbers-summary-1",
    "title": "5. Alpha Diversity Estimates",
    "section": "Hill Numbers Summary",
    "text": "Hill Numbers Summary\nNow we summarize the data for each sample against all three metrics. The table contains the results of ASV diversity estimates from the full data set and the three filtered data sets.\nThe suffix _fi indicates metrics for the Arbitrary data set. The suffix _pe indicates metrics for the PERfect data set and the suffix _pet indicates the lineage diversity for the PERfect data set.\nThe suffix _pi indicates metrics for the PIME data set and the suffix _pit indicates the lineage diversity for the PIME data set.\n\n\n\n\n\n\nHill Summary\n(ITS) Table 1 | Hill numbers for samples in the unfiltered & filtered data sets.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "alpha.html#normality-tests-1",
    "href": "alpha.html#normality-tests-1",
    "title": "5. Alpha Diversity Estimates",
    "section": "Normality Tests",
    "text": "Normality Tests\nBefore running significance tests, we need to know if data is normally distributed, which will tell use whether to use a parametric or non-parametric test. To test if the data are normally distributed, we use the Shapiro-Wilk Normality test and the Bartlett Test of Homogeneity of Variances.\nIf the p-values are both not significant (p > 0.05) from the tests, we accept the null hypothesis (that the results are normally distributed) and test for significance between samples using an ANOVA. If the p-values are both significant (p < 0.05), we reject the null hypothesis (that the results are normally distributed) and test for significance between samples using Kruskal-Wallis (non-parametric equivalent of ANOVA).\nThe commands are as follows:\nshapiro.test(x), where x is a numeric vector of alpha diversity values from the sample data table.\nbartlett.test(Value ~ Group, data = df) Where Value is the metric of interest, Group in the treatment to compare, and df is the data frame.\nFirst the Shapiro-Wilk Normality test.\n\nCodeits_tab_alpha_div\nits18_div_tab <- its_tab_alpha_div\nits18_shap_tests_asv <- c()\nfor (i in colnames(its18_div_tab[,7:ncol(its18_div_tab)])) {\n   tmp_name <- purrr::map_chr(i, ~ paste0(\"its18_shap_asv_\", .))\n   its18_shap_tests_asv <- append(its18_shap_tests_asv, tmp_name)\n   tmp_test <- eval(shapiro.test(its18_div_tab[[i]]))\n   tmp_test$data.name <- tmp_name\n   assign(tmp_name, tmp_test)\n   rm(list = ls(pattern = \"tmp_\"))\n}\n\nobjects(pattern = \"shap_\")\n\n\nAnd then the Bartlett Test of Homogeneity of Variances.\n\nCodeits18_div_tab_asv <- its_tab_alpha_div\nits18_bart_tests_asv <- c()\nfor (i in colnames(its18_div_tab_asv[,7:ncol(its18_div_tab_asv)])) {\n   tmp_name <- purrr::map_chr(i, ~ paste0(\"its18_bart_asv_\", .))\n   its18_bart_tests_asv <- append(its18_bart_tests_asv, tmp_name)\n   tmp_test <- eval(bartlett.test(its18_div_tab_asv[[i]] ~ TEMP, data = its18_div_tab_asv))\n   tmp_test$data.name <- tmp_name\n   assign(tmp_name, tmp_test)\n   rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_bart_\")\n\n\nHere we see which Shapiro-Wilk Normality and Bartlett tests were significant and which were not. So wherever the value of both p-values in > 0.05 we can use an ANOVA, otherwise we use Kruskal-Wallis.\n\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 3 rows [1, 5, 9].\n\n\n(ITS) Table 2 | Summary of Normality Tests for unfiltered & filtered data sets. P-values in red indicate significance (p-value < 0.05).\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "alpha.html#significance-tests-1",
    "href": "alpha.html#significance-tests-1",
    "title": "5. Alpha Diversity Estimates",
    "section": "Significance Tests",
    "text": "Significance Tests\nTo begin, we need to create a hierarchy variable; a two-column matrix specifying the relationship between samples (first column) and groups (second column).\n\nCodeits18_hill_hier <- its_tab_alpha_div\nits18_hill_hier <- its18_hill_hier %>% dplyr::select(\"Sample_ID\", \"TEMP\") %>%\n  tibble::remove_rownames()\nits18_hill_hier <- its18_hill_hier[order(its18_hill_hier$TEMP), ]\nits18_hill_hier$TEMP = paste(its18_hill_hier$TEMP, 'C', sep='')\nits18_hill_hier <- its18_hill_hier %>% tibble::remove_rownames()\nsaveRDS(its18_hill_hier, \"files/alpha/rdata/its18_hill_hier.rds\")\n\n\nAgain, we start by testing significance of ASV diversity for the data sets against each of the three metrics using the div_test function.\nThe command is as follows:\ndiv_test(countable = x, qvalue = i, hierarchy = hier, \n         tree = ultrametric_tree, posthoc = TRUE)\nwhere x is ASV by sample table, i is the q-value corresponding to the metric of interest, hier is the hierarchy matrix, tree is an ultrametric formatted phylogenetic tree (if you choose to explore lineage diversity), and posthoc indicates whether to run post hoc pairwise analyses.\n\nCodeqvalue <- c(0,1,2)\nfor (i in its18_alpha_ds) {\n     for (j in qvalue) {\n         tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_tu\")))\n         tmp_test <- div_test(tmp_get, qvalue = j,\n                      hierarchy = its18_hill_hier,\n                      posthoc = TRUE)\n         tmp_name <- purrr::map_chr(i, ~ paste0(., \"_q\", j, \"_adt\"))\n         print(tmp_name)\n         assign(tmp_name, tmp_test)\n         rm(list = ls(pattern = \"tmp_\"))\n }\n}"
  },
  {
    "objectID": "alpha.html#summary-1",
    "href": "alpha.html#summary-1",
    "title": "5. Alpha Diversity Estimates",
    "section": "Summary",
    "text": "Summary\n(ITS) Table 3 | Summary of significance tests for unfiltered & filtered data sets. P-values in red for post hoc analysis indicate significance (p-value < 0.05).\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "alpha.html#posthoc-analyses-1",
    "href": "alpha.html#posthoc-analyses-1",
    "title": "5. Alpha Diversity Estimates",
    "section": "PostHoc Analyses",
    "text": "PostHoc Analyses\nFirst let’s check the results of each posthoc analysis.\n\n\n\nObserved (q-value = 0)\nShannon exp (q-value = 1)\nInverse Simpson (q-value = 2)\n\n\n\n\n\n[1] \"FULL (Observed)\"\n[1] \"Tukey post-hoc test\"\n         diff       lwr        upr       p.adj\n3C-0C  -68.70 -337.2489  199.84893 0.768173881\n8C-0C -394.45 -662.9989 -125.90107 0.006201167\n8C-3C -325.75 -608.8254  -42.67457 0.025372568\n\n\n\n\n[1] \"FILT (Observed)\"\n[1] \"Tukey post-hoc test\"\n         diff       lwr       upr       p.adj\n3C-0C   11.00 -101.5783 123.57828 0.961373753\n8C-0C -173.25 -285.8283 -60.67172 0.004588355\n8C-3C -184.25 -302.9179 -65.58208 0.004327805\n\n\n\n\n[1] \"PIME (Observed)\"\n[1] \"Tukey post-hoc test\"\n        diff        lwr        upr          p.adj\n3C-0C  -28.3  -59.02587   2.425871 0.071121744588\n8C-0C -107.8 -138.52587 -77.074129 0.000006170051\n8C-3C  -79.5 -111.88791 -47.112088 0.000138893963\n\n\n\n\n[1] \"PERfect (Observed)\"\n[1] \"Tukey post-hoc test\"\n        diff       lwr       upr       p.adj\n3C-0C  -5.35 -38.46046  27.76046 0.898558605\n8C-0C -60.60 -93.71046 -27.48954 0.001378091\n8C-3C -55.25 -90.15149 -20.34851 0.003804791\n\n\n\n\n\n\n[1] \"FULL (Shannon exponential)\"\n[1] \"Tukey post-hoc test\"\n           diff        lwr       upr     p.adj\n3C-0C  68.52857  -31.79969 168.85684 0.1967805\n8C-0C  34.59038  -65.73789 134.91864 0.6257554\n8C-3C -33.93820 -139.69348  71.81708 0.6644870\n\n\n\n\n[1] \"FILT (Shannon exponential)\"\n[1] \"Tukey post-hoc test\"\n           diff       lwr      upr     p.adj\n3C-0C  42.89239 -10.62278 96.40756 0.1199483\n8C-0C  18.50838 -35.00679 72.02355 0.6240029\n8C-3C -24.38401 -80.79395 32.02593 0.4878559\n\n\n\n\n[1] \"PIME (Shannon exponential)\"\n[1] \"Tukey post-hoc test\"\n             diff       lwr      upr     p.adj\n3C-0C  15.1283988 -14.51712 44.77392 0.3777877\n8C-0C   0.5613833 -29.08414 30.20690 0.9985157\n8C-3C -14.5670155 -45.81614 16.68211 0.4381419\n\n\n\n\n[1] \"PERfect (Shannon exponential)\"\n[1] \"Tukey post-hoc test\"\n             diff        lwr       upr     p.adj\n3C-0C  13.7449005  -7.058747 34.548549 0.2152611\n8C-0C   0.3995566 -20.404091 21.203205 0.9984731\n8C-3C -13.3453440 -35.274314  8.583626 0.2639185\n\n\n\n\n\n\n[1] \"FULL (Inverse Simpson)\"\n[1] \"Tukey post-hoc test\"\n           diff       lwr      upr     p.adj\n3C-0C  30.12853 -10.93247 71.18953 0.1597780\n8C-0C  15.09562 -25.96538 56.15662 0.5888250\n8C-3C -15.03291 -58.31500 28.24918 0.6216188\n\n\n\n\n[1] \"FILT (Inverse Simpson)\"\n[1] \"Tukey post-hoc test\"\n            diff       lwr      upr     p.adj\n3C-0C  19.474580  -7.08637 46.03553 0.1601425\n8C-0C   8.362193 -18.19876 34.92314 0.6743506\n8C-3C -11.112388 -39.11009 16.88531 0.5422962\n\n\n\n\n[1] \"PIME (Inverse Simpson)\"\n[1] \"Tukey post-hoc test\"\n            diff        lwr       upr     p.adj\n3C-0C  8.4231399  -7.904659 24.750938 0.3705643\n8C-0C  0.3894783 -15.938320 16.717277 0.9976460\n8C-3C -8.0336616 -25.244672  9.177349 0.4372626\n\n\n\n\n[1] \"PERfect (Inverse Simpson)\"\n[1] \"Tukey post-hoc test\"\n           diff        lwr       upr     p.adj\n3C-0C  9.662568  -4.771842 24.096979 0.2078889\n8C-0C  1.449038 -12.985373 15.883449 0.9592809\n8C-3C -8.213530 -23.428735  7.001674 0.3405328\n\n\n\n\n\n\nNow we can plot the results from the posthoc analyses for each metric and data set using the function div_test_plot_jjs. I modified the original function (div_test_plot) to control a little of the formatting.\nThe command is as follows:\ndiv_test_plot(divtest = x, chart = \"type\", colour = col.pal, \n              posthoc = TRUE, threshold = value))\nwhere x is the results from the div_test function, \"type\" is chart type (box, jitter, or violin), colour is is a color palette, posthoc indicates whether to run posthoc pairwise analyses, and value is the maximum p-value to show in pairwise posthoc results. WARNING if none of the posthoc results are below the specified threshold, the function will throw an error. Therefore, until this is fixed, all posthoc values are shown.\n\nCodesource(\"hack_code/div_test_plot_jjs.R\")\nrm(list=ls(pattern=\"_adt_plot\"))\nfor (i in objects(pattern=\"_adt\")) {\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_plot\"))\n     tmp_get <- get(i)\n     tmp_df <- div_test_plot_jjs(tmp_get, chart = \"box\",\n                                 colour = swel_col, posthoc = TRUE)\n     tmp_df <- ggpar(tmp_df, legend = \"none\")\n     print(tmp_name)\n     assign(tmp_name, tmp_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nits18_ps_work_q0_adt_plot"
  },
  {
    "objectID": "alpha.html#alpha-diversity-plots-1",
    "href": "alpha.html#alpha-diversity-plots-1",
    "title": "5. Alpha Diversity Estimates",
    "section": "Alpha Diversity Plots",
    "text": "Alpha Diversity Plots\nPosthoc adjusted p-values given for each pairwise comparison.\n\n\n\n\n\n(ITS) Figure 1 | Alpha diversity plots. Top row = Observed; middle row = Shannon exponential; bottom row = Inverse Simpson. From left to right, Full (unfiltered) data, Arbitrary filtered, PERfect filtered, PIME filtered."
  },
  {
    "objectID": "alpha.html#data-availability",
    "href": "alpha.html#data-availability",
    "title": "5. Alpha Diversity Estimates",
    "section": "Data Availability",
    "text": "Data Availability\nData generated in this workflow and the Rdata need to run the workflow can be accessed on figshare at 10.25573/data.16826779."
  },
  {
    "objectID": "alpha.html#last-updated-on",
    "href": "alpha.html#last-updated-on",
    "title": "5. Alpha Diversity Estimates",
    "section": "Last updated on",
    "text": "Last updated on\n\n\n[1] \"2022-06-19 13:13:14 EST\""
  },
  {
    "objectID": "beta.html",
    "href": "beta.html",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "",
    "text": "Click here for setup information.\n\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(119)\nlibrary(phyloseq); packageVersion(\"phyloseq\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, \n               microbiome, phytools, phangorn, reactable, Matrix, \n               pairwiseAdonis, naniar, downloadthis,\n               labdsv, patchwork, agricolae, captioner, microeco,\n               install = FALSE, update = FALSE)\n\n#pacman::p_depends(agricolae, local = TRUE)  \n#pacman::p_depends_reverse(agricolae, local = TRUE)  \n\noptions(scipen=999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))"
  },
  {
    "objectID": "beta.html#workflow-input",
    "href": "beta.html#workflow-input",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "Workflow Input",
    "text": "Workflow Input\nFiles needed to run this workflow can be downloaded from figshare."
  },
  {
    "objectID": "beta.html#beta-dispersion",
    "href": "beta.html#beta-dispersion",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "Beta Dispersion",
    "text": "Beta Dispersion\nHere we create distance matrices for each metric, calculate the beta dispersion, and run a permutation test for homogeneity of multivariate dispersions.\n\nCodessu_dist <- c(\"jsd\", \"unifrac\", \"wunifrac\")\nfor (i in ssu_samp_ps) {\n    for (d in ssu_dist){\n       tmp_get <- get(purrr::map_chr(i, ~ paste0(i, \"_prop\")))\n       tmp_samp <- data.frame(sample_data(tmp_get))\n       tmp_df <- phyloseq::distance(tmp_get, method = d)\n       tmp_df_name <- purrr::map_chr(d, ~ paste0(i, \"_beta_dist_\", .))\n       assign(tmp_df_name, tmp_df)\n       tmp_df2 <- betadisper(tmp_df, tmp_samp$TEMP, bias.adjust = TRUE)\n       tmp_df_name2 <- purrr::map_chr(d, ~ paste0(i, \"_beta_dispersion_\", .))\n       assign(tmp_df_name2, tmp_df2)\n       tmp_df3 <- permutest(tmp_df2, pairwise = TRUE,\n                            permutations = 1000, binary = FALSE)\n       tmp_df_name3 <- purrr::map_chr(d, ~ paste0(i, \"_permutest_\", .))\n       assign(tmp_df_name3, tmp_df3)\n       rm(list = ls(pattern = \"tmp_\"))\n    }\n}\nobjects()\n\n\nDetailed results of Beta Dispersion & Permutation tests\n\n\nFULL\nArbitrary\nPERfect\nPIME\n\n\n\n\n\n\n#################################################### \nBETA DISPERSION significance test jsd distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm Pr(>F)\nGroups     2 0.018639 0.0093195 2.9882   1000 0.1089\nResiduals 12 0.037425 0.0031188                     \n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n         0        3      8\n0          0.465534 0.1638\n3 0.412469          0.0599\n8 0.159005 0.075347       \n\n#################################################### \nBETA DISPERSION significance test unifrac distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df    Sum Sq    Mean Sq      F N.Perm  Pr(>F)   \nGroups     2 0.0062292 0.00311458 5.3484   1000 0.00999 **\nResiduals 12 0.0069881 0.00058234                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n         0        3     8\n0          0.831169 0.012\n3 0.727926          0.032\n8 0.033203 0.048150      \n\n#################################################### \nBETA DISPERSION significance test wunifrac distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm   Pr(>F)   \nGroups     2 0.018783 0.0093917 7.3178   1000 0.005994 **\nResiduals 12 0.015401 0.0012834                          \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n         0        3     8\n0          0.957043 0.015\n3 0.938214          0.004\n8 0.024555 0.019472      \n\n\n\n\n\n\n\n#################################################### \nBETA DISPERSION significance test jsd distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm  Pr(>F)  \nGroups     2 0.019867 0.0099333 3.0824   1000 0.08891 .\nResiduals 12 0.038671 0.0032226                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n         0        3      8\n0          0.854146 0.0859\n3 0.843567          0.0869\n8 0.112793 0.090799       \n\n#################################################### \nBETA DISPERSION significance test unifrac distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm   Pr(>F)   \nGroups     2 0.025430 0.0127152 7.0788   1000 0.005994 **\nResiduals 12 0.021555 0.0017962                          \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n         0        3     8\n0          0.583417 0.011\n3 0.568432          0.018\n8 0.018174 0.023627      \n\n#################################################### \nBETA DISPERSION significance test wunifrac distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm   Pr(>F)   \nGroups     2 0.023408 0.0117041 6.7591   1000 0.004995 **\nResiduals 12 0.020779 0.0017316                          \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n         0        3     8\n0          0.565435 0.008\n3 0.528192          0.013\n8 0.023244 0.032852      \n\n\n\n\n\n\n\n#################################################### \nBETA DISPERSION significance test jsd distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm  Pr(>F)  \nGroups     2 0.027097 0.0135487 4.5971   1000 0.02098 *\nResiduals 12 0.035367 0.0029472                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n         0        3     8\n0          0.778222 0.044\n3 0.771551          0.027\n8 0.061934 0.045555      \n\n#################################################### \nBETA DISPERSION significance test unifrac distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm   Pr(>F)   \nGroups     2 0.034618 0.0173088 9.4781   1000 0.003996 **\nResiduals 12 0.021914 0.0018262                          \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n          0         3     8\n0           0.5684316 0.008\n3 0.5529721           0.008\n8 0.0088715 0.0096445      \n\n#################################################### \nBETA DISPERSION significance test wunifrac distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm   Pr(>F)   \nGroups     2 0.020738 0.0103692 7.5898   1000 0.002997 **\nResiduals 12 0.016395 0.0013662                          \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n         0        3     8\n0          0.547453 0.005\n3 0.516927          0.003\n8 0.027061 0.017081      \n\n\n\n\n\n\n\n#################################################### \nBETA DISPERSION significance test jsd distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm Pr(>F)\nGroups     2 0.007284 0.0036418 0.9199   1000 0.4356\nResiduals 12 0.047509 0.0039591                     \n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n        0       3      8\n0         0.84515 0.3876\n3 0.82435         0.3407\n8 0.37620 0.31969       \n\n#################################################### \nBETA DISPERSION significance test unifrac distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm Pr(>F)\nGroups     2 0.016868 0.0084341 1.4936   1000 0.2398\nResiduals 12 0.067761 0.0056468                     \n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n        0       3      8\n0         0.97902 0.2617\n3 0.97444         0.2058\n8 0.24180 0.21934       \n\n#################################################### \nBETA DISPERSION significance test wunifrac distance \n####################################################\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 1000\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm   Pr(>F)   \nGroups     2 0.025802 0.0129010 8.6861   1000 0.008991 **\nResiduals 12 0.017823 0.0014852                          \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n         0        3     8\n0          0.982018 0.006\n3 0.982289          0.004\n8 0.015248 0.014089      \n\n\n\n\n\nRemember, if the beta dispersion p-value is greater than 0.05 we use PERMANOVA, otherwise we use ANOSIM.\n\n\n\n\n\n\n(16S rRNA) Table 1 | Summary of Beta Dispersion Tests for unfiltered & filtered data sets against various distance metrics. The test column shows the method chosen (based on the test results) to assess differences in beta diversity. If p-values are significant (red, p-value < 0.05) we used ANOSIM, otherwise we used PERMANOVA (blue, p-value > 0.05).\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nNow we to create sample data frames for each data set.\n\nCodefor (i in ssu_samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_prop\")))\n     tmp_samp <- data.frame(sample_data(tmp_get))\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_sampledf\"))\n     assign(tmp_name, tmp_samp)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern=\"_sampledf\")"
  },
  {
    "objectID": "beta.html#significance-tests",
    "href": "beta.html#significance-tests",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "Significance Tests",
    "text": "Significance Tests\nDepending on the test, we need different data structures. For adonis we use the sample metadata and for anosim we will start with a phyloseq object. So for each data set we create two variables: adonis_sampledf and anosim_data\n\n\n\nFULL\nArbitrary\nPERfect\nPIME\n\n\n\n\n\n\n\n\n\nIs the p-value of the jsd distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nssu18_ps_work_adonis_jsd <-  adonis(ssu18_ps_work_beta_dist_jsd ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nssu18_ps_work_adonis2_jsd <- adonis2(ssu18_ps_work_beta_dist_jsd ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the unifrac distance metric less than 0.05? TRUE.\nThen we use ANOSIM.\n\nssu18_ps_work_groups <- get_variable(anosim_data, \"TEMP\")\nssu18_ps_work_anosim_unifrac <-\n  anosim(phyloseq::distance(ssu18_ps_work_prop, \"unifrac\"),\n         grouping = ssu18_ps_work_groups)\n\nIs the p-value of the wunifrac distance metric less than 0.05? TRUE.\nThen we use ANOSIM.\n\nssu18_ps_work_groups <- get_variable(anosim_data, \"TEMP\")\nssu18_ps_work_anosim_wunifrac <-\n  anosim(phyloseq::distance(ssu18_ps_work_prop, \"wunifrac\"),\n         grouping = ssu18_ps_work_groups)\n\n\n\n\n\n\n\n\n\nIs the p-value of the jsd distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nssu18_ps_filt_adonis_jsd <-  adonis(ssu18_ps_filt_beta_dist_jsd ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nssu18_ps_filt_adonis2_jsd <- adonis2(ssu18_ps_filt_beta_dist_jsd ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the unifrac distance metric less than 0.05? TRUE.\nThen we use ANOSIM.\n\nssu18_ps_filt_groups <- get_variable(anosim_data, \"TEMP\")\nssu18_ps_filt_anosim_unifrac <-\n  anosim(phyloseq::distance(ssu18_ps_filt_prop, \"unifrac\"),\n         grouping = ssu18_ps_filt_groups)\n\nIs the p-value of the wunifrac distance metric less than 0.05? TRUE.\nThen we use ANOSIM.\n\nssu18_ps_filt_groups <- get_variable(anosim_data, \"TEMP\")\nssu18_ps_filt_anosim_wunifrac <-\n  anosim(phyloseq::distance(ssu18_ps_filt_prop, \"wunifrac\"),\n         grouping = ssu18_ps_filt_groups)\n\n\n\n\n\n\n\n\n\nIs the p-value of the jsd distance metric less than 0.05? TRUE.\nThen we use ANOSIM.\n\nssu18_ps_perfect_groups <- get_variable(anosim_data, \"TEMP\")\nssu18_ps_perfect_anosim_jsd <-\n  anosim(phyloseq::distance(ssu18_ps_perfect_prop, \"jsd\"),\n         grouping = ssu18_ps_perfect_groups)\n\nIs the p-value of the unifrac distance metric less than 0.05? TRUE.\nThen we use ANOSIM.\n\nssu18_ps_perfect_groups <- get_variable(anosim_data, \"TEMP\")\nssu18_ps_perfect_anosim_unifrac <-\n  anosim(phyloseq::distance(ssu18_ps_perfect_prop, \"unifrac\"),\n         grouping = ssu18_ps_perfect_groups)\n\nIs the p-value of the wunifrac distance metric less than 0.05? TRUE.\nThen we use ANOSIM.\n\nssu18_ps_perfect_groups <- get_variable(anosim_data, \"TEMP\")\nssu18_ps_perfect_anosim_wunifrac <-\n  anosim(phyloseq::distance(ssu18_ps_perfect_prop, \"wunifrac\"),\n         grouping = ssu18_ps_perfect_groups)\n\n\n\n\n\n\n\n\n\nIs the p-value of the jsd distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nssu18_ps_pime_adonis_jsd <-  adonis(ssu18_ps_pime_beta_dist_jsd ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nssu18_ps_pime_adonis2_jsd <- adonis2(ssu18_ps_pime_beta_dist_jsd ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the unifrac distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nssu18_ps_pime_adonis_unifrac <-  adonis(ssu18_ps_pime_beta_dist_unifrac ~ TEMP,\n                                     data = adonis_sampledf, permutations = 1000)\nssu18_ps_pime_adonis2_unifrac <- adonis2(ssu18_ps_pime_beta_dist_unifrac ~ TEMP,\n                                      data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the wunifrac distance metric less than 0.05? TRUE.\nThen we use ANOSIM.\n\nssu18_ps_pime_groups <- get_variable(anosim_data, \"TEMP\")\nssu18_ps_pime_anosim_wunifrac <-\n  anosim(phyloseq::distance(ssu18_ps_pime_prop, \"wunifrac\"),\n         grouping = ssu18_ps_pime_groups)\n\n\n\n\n\nDetailed results of Significance tests\n\n\nFULL\nArbitrary\nPERfect\nPIME\n\n\n\n\n\n\n ***PERMANOVA for Jensen-Shannon Divergence, `jsd`*** \n\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = ssu18_ps_work_beta_dist_jsd ~ TEMP, data = adonis_sampledf, permutations = 1000)\n         Df SumOfSqs      R2      F   Pr(>F)   \nTEMP      2  0.21809 0.23826 1.8767 0.003996 **\nResidual 12  0.69727 0.76174                   \nTotal    14  0.91536 1.00000                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n ***ANOSIM for Unweighted UniFrac distance, `unifrac`*** \n\n\n\nCall:\nanosim(x = phyloseq::distance(ssu18_ps_work_prop, \"unifrac\"),      grouping = ssu18_ps_work_groups) \nDissimilarity: \n\nANOSIM statistic R: 0.2356 \n      Significance: 0.003 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n   90%    95%  97.5%    99% \n0.0873 0.1130 0.1396 0.1725 \n\nDissimilarity ranks between and within classes:\n        0%   25%  50%   75% 100%  N\nBetween  1 32.50 58.0 81.00  103 75\n0        2 10.25 17.5 33.25   56 10\n3        4 14.75 26.5 41.00   65 10\n8       39 69.75 85.5 94.25  105 10\n\n\n\n ***ANOSIM for Weighted-UniFrac distance, `wunifrac`*** \n\n\n\nCall:\nanosim(x = phyloseq::distance(ssu18_ps_work_prop, \"wunifrac\"),      grouping = ssu18_ps_work_groups) \nDissimilarity: \n\nANOSIM statistic R: 0.3218 \n      Significance: 0.001 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.090 0.120 0.149 0.176 \n\nDissimilarity ranks between and within classes:\n        0%   25%  50%    75% 100%  N\nBetween  2 36.50 59.0  81.50  102 75\n0        3  9.75 20.5  35.00   50 10\n3        1 14.50 19.0  27.75   48 10\n8       17 66.75 80.5 102.00  105 10\n\n\n\n\n\n\n\n ***PERMANOVA for Jensen-Shannon Divergence, `jsd`*** \n\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = ssu18_ps_filt_beta_dist_jsd ~ TEMP, data = adonis_sampledf, permutations = 1000)\n         Df SumOfSqs      R2      F  Pr(>F)   \nTEMP      2  0.15050 0.29049 2.4566 0.00999 **\nResidual 12  0.36758 0.70951                  \nTotal    14  0.51808 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n ***ANOSIM for Unweighted UniFrac distance, `unifrac`*** \n\n\n\nCall:\nanosim(x = phyloseq::distance(ssu18_ps_filt_prop, \"unifrac\"),      grouping = ssu18_ps_filt_groups) \nDissimilarity: \n\nANOSIM statistic R: 0.2427 \n      Significance: 0.009 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.109 0.136 0.169 0.225 \n\nDissimilarity ranks between and within classes:\n        0%   25%  50%   75% 100%  N\nBetween  2 35.00 58.0 81.50  102 75\n0        1  7.75 17.5 34.75   59 10\n3       13 23.00 28.5 34.50   57 10\n8       11 66.50 81.5 99.50  105 10\n\n\n\n ***ANOSIM for Weighted-UniFrac distance, `wunifrac`*** \n\n\n\nCall:\nanosim(x = phyloseq::distance(ssu18_ps_filt_prop, \"wunifrac\"),      grouping = ssu18_ps_filt_groups) \nDissimilarity: \n\nANOSIM statistic R: 0.3618 \n      Significance: 0.001 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.103 0.140 0.173 0.216 \n\nDissimilarity ranks between and within classes:\n        0%   25% 50%   75% 100%  N\nBetween  3 37.50  61 82.50  104 75\n0        1  7.25  14 27.25   36 10\n3        2 13.00  27 37.75   43 10\n8       23 59.25  76 99.00  105 10\n\n\n\n\n\n\n\n ***ANOSIM for Jensen-Shannon Divergence, `jsd`*** \n\n\n\nCall:\nanosim(x = phyloseq::distance(ssu18_ps_perfect_prop, \"jsd\"),      grouping = ssu18_ps_perfect_groups) \nDissimilarity: \n\nANOSIM statistic R: 0.2996 \n      Significance: 0.001 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n   90%    95%  97.5%    99% \n0.0999 0.1450 0.1770 0.2081 \n\nDissimilarity ranks between and within classes:\n        0%  25%  50%   75% 100%  N\nBetween  2 33.5 61.0 83.50  104 75\n0        5 15.5 19.5 44.75   59 10\n3        1  9.5 23.0 29.75   57 10\n8       39 53.5 75.0 95.50  105 10\n\n\n\n ***ANOSIM for Unweighted UniFrac distance, `unifrac`*** \n\n\n\nCall:\nanosim(x = phyloseq::distance(ssu18_ps_perfect_prop, \"unifrac\"),      grouping = ssu18_ps_perfect_groups) \nDissimilarity: \n\nANOSIM statistic R: 0.2053 \n      Significance: 0.007 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n   90%    95%  97.5%    99% \n0.0908 0.1191 0.1441 0.1814 \n\nDissimilarity ranks between and within classes:\n        0%   25% 50%    75% 100%  N\nBetween  5 31.00  59  81.50  102 75\n0        1  4.75  14  36.25   56 10\n3        4 21.75  34  43.50   53 10\n8       52 70.50  80 100.00  105 10\n\n\n\n ***ANOSIM for Weighted-UniFrac distance, `wunifrac`*** \n\n\n\nCall:\nanosim(x = phyloseq::distance(ssu18_ps_perfect_prop, \"wunifrac\"),      grouping = ssu18_ps_perfect_groups) \nDissimilarity: \n\nANOSIM statistic R: 0.3378 \n      Significance: 0.001 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n   90%    95%  97.5%    99% \n0.0942 0.1213 0.1538 0.1831 \n\nDissimilarity ranks between and within classes:\n        0%   25%  50%   75% 100%  N\nBetween  2 36.00 61.0 80.50  105 75\n0        1 20.00 26.5 35.50   40 10\n3        3  6.25 15.5 20.75   42 10\n8       14 61.00 92.5 99.25  104 10\n\n\n\n\n\n\n\n ***PERMANOVA for Jensen-Shannon Divergence, `jsd`*** \n\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = ssu18_ps_pime_beta_dist_jsd ~ TEMP, data = adonis_sampledf, permutations = 1000)\n         Df SumOfSqs      R2      F   Pr(>F)    \nTEMP      2  0.23994 0.59595 8.8496 0.000999 ***\nResidual 12  0.16268 0.40405                    \nTotal    14  0.40262 1.00000                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n ***PERMANOVA for Unweighted UniFrac distance, `unifrac`*** \n\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = ssu18_ps_pime_beta_dist_unifrac ~ TEMP, data = adonis_sampledf, permutations = 1000)\n         Df SumOfSqs      R2      F   Pr(>F)    \nTEMP      2  0.78789 0.64853 11.071 0.000999 ***\nResidual 12  0.42699 0.35147                    \nTotal    14  1.21488 1.00000                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n ***ANOSIM for Weighted-UniFrac distance, `wunifrac`*** \n\n\n\nCall:\nanosim(x = phyloseq::distance(ssu18_ps_pime_prop, \"wunifrac\"),      grouping = ssu18_ps_pime_groups) \nDissimilarity: \n\nANOSIM statistic R: 0.5316 \n      Significance: 0.001 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.114 0.157 0.207 0.262 \n\nDissimilarity ranks between and within classes:\n        0%   25% 50%  75% 100%  N\nBetween  9 41.00  62 85.5  105 75\n0        1  8.75  17 28.5   37 10\n3        2  5.50  17 26.5   39 10\n8        4 50.50  73 81.0   97 10"
  },
  {
    "objectID": "beta.html#summaries",
    "href": "beta.html#summaries",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "Summaries",
    "text": "Summaries\nHere is a quick summary of significance tests for the data sets against three distance matrices.\n\n\n\n(16S rRNA) Table 2 | Summary of significant tests for the unfiltered & filtered data sets. P-values in red indicate significant differences (p-value < 0.05).\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "beta.html#significance-tests-1",
    "href": "beta.html#significance-tests-1",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "Significance Tests",
    "text": "Significance Tests\nDepending on the test, we need different data structures. For adonis we use the sample metadata and for anosim we will start with a phyloseq object. So for each data set we create two variables: adonis_sampledf and anosim_data\n\n\n\nFULL\nArbitrary\nPERfect\nPIME\n\n\n\n\n\n\n\n\n\nIs the p-value of the jsd distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_work_adonis_jsd <-  adonis(its18_ps_work_beta_dist_jsd ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_work_adonis2_jsd <- adonis2(its18_ps_work_beta_dist_jsd ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the bray distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_work_adonis_bray <-  adonis(its18_ps_work_beta_dist_bray ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_work_adonis2_bray <- adonis2(its18_ps_work_beta_dist_bray ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the gower distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_work_adonis_gower <-  adonis(its18_ps_work_beta_dist_gower ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_work_adonis2_gower <- adonis2(its18_ps_work_beta_dist_gower ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\n\n\n\n\n\n\n\n\nIs the p-value of the jsd distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_filt_adonis_jsd <-  adonis(its18_ps_filt_beta_dist_jsd ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_filt_adonis2_jsd <- adonis2(its18_ps_filt_beta_dist_jsd ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the bray distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_filt_adonis_bray <-  adonis(its18_ps_filt_beta_dist_bray ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_filt_adonis2_bray <- adonis2(its18_ps_filt_beta_dist_bray ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the gower distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_filt_adonis_gower <-  adonis(its18_ps_filt_beta_dist_gower ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_filt_adonis2_gower <- adonis2(its18_ps_filt_beta_dist_gower ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\n\n\n\n\n\n\n\n\nIs the p-value of the jsd distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_perfect_adonis_jsd <-  adonis(its18_ps_perfect_beta_dist_jsd ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_perfect_adonis2_jsd <- adonis2(its18_ps_perfect_beta_dist_jsd ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the bray distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_perfect_adonis_bray <-  adonis(its18_ps_perfect_beta_dist_bray ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_perfect_adonis2_bray <- adonis2(its18_ps_perfect_beta_dist_bray ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the gower distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_perfect_adonis_gower <-  adonis(its18_ps_perfect_beta_dist_gower ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_perfect_adonis2_gower <- adonis2(its18_ps_perfect_beta_dist_gower ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\n\n\n\n\n\n\n\n\nIs the p-value of the jsd distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_pime_adonis_jsd <-  adonis(its18_ps_pime_beta_dist_jsd ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_pime_adonis2_jsd <- adonis2(its18_ps_pime_beta_dist_jsd ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the bray distance metric less than 0.05? FALSE.\nThen we use ADONIS.\n\nits18_ps_pime_adonis_bray <-  adonis(its18_ps_pime_beta_dist_bray ~ TEMP,\n                                 data = adonis_sampledf, permutations = 1000)\nits18_ps_pime_adonis2_bray <- adonis2(its18_ps_pime_beta_dist_bray ~ TEMP,\n                                  data = adonis_sampledf, permutations = 1000)\n\nIs the p-value of the gower distance metric less than 0.05? TRUE.\nThen we use ANOSIM.\n\nits18_ps_pime_groups <- get_variable(anosim_data, \"TEMP\")\nits18_ps_pime_anosim_gower <-\n  anosim(phyloseq::distance(its18_ps_pime_prop, \"gower\"),\n         grouping = its18_ps_pime_groups)\n\n\n\n\n\nDetailed results of Significance tests.\n\n\nFULL\nArbitrary\nPERfect\nPIME\n\n\n\n\n\n\n ***PERMANOVA for Jensen-Shannon Divergence, `jsd`*** \n\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = its18_ps_work_beta_dist_jsd ~ TEMP, data = adonis_sampledf, permutations = 1000)\n         Df SumOfSqs     R2      F  Pr(>F)  \nTEMP      2  0.38087 0.2287 1.4826 0.03796 *\nResidual 10  1.28448 0.7713                 \nTotal    12  1.66534 1.0000                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n ***PERMANOVA for Bray-Curtis dissimilarity, `bray`*** \n\n\n       Df        SumOfSqs            R2               F        \n Min.   : 2   Min.   :0.9003   Min.   :0.2070   Min.   :1.305  \n 1st Qu.: 6   1st Qu.:2.1744   1st Qu.:0.5000   1st Qu.:1.305  \n Median :10   Median :3.4484   Median :0.7930   Median :1.305  \n Mean   : 8   Mean   :2.8992   Mean   :0.6667   Mean   :1.305  \n 3rd Qu.:11   3rd Qu.:3.8986   3rd Qu.:0.8965   3rd Qu.:1.305  \n Max.   :12   Max.   :4.3488   Max.   :1.0000   Max.   :1.305  \n                                                NA's   :2      \n     Pr(>F)       \n Min.   :0.05095  \n 1st Qu.:0.05095  \n Median :0.05095  \n Mean   :0.05095  \n 3rd Qu.:0.05095  \n Max.   :0.05095  \n NA's   :2        \n\n\n\n ***PERMANOVA for Gower distance, `gower`*** \n\n\n       Df        SumOfSqs             R2               F        \n Min.   : 2   Min.   :0.05219   Min.   :0.2016   Min.   :1.263  \n 1st Qu.: 6   1st Qu.:0.12941   1st Qu.:0.5000   1st Qu.:1.263  \n Median :10   Median :0.20664   Median :0.7984   Median :1.263  \n Mean   : 8   Mean   :0.17255   Mean   :0.6667   Mean   :1.263  \n 3rd Qu.:11   3rd Qu.:0.23273   3rd Qu.:0.8992   3rd Qu.:1.263  \n Max.   :12   Max.   :0.25882   Max.   :1.0000   Max.   :1.263  \n                                                 NA's   :2      \n     Pr(>F)       \n Min.   :0.00999  \n 1st Qu.:0.00999  \n Median :0.00999  \n Mean   :0.00999  \n 3rd Qu.:0.00999  \n Max.   :0.00999  \n NA's   :2        \n\n\n\n\n\n\n\n ***PERMANOVA for Jensen-Shannon Divergence, `jsd`*** \n\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = its18_ps_filt_beta_dist_jsd ~ TEMP, data = adonis_sampledf, permutations = 1000)\n         Df SumOfSqs      R2      F  Pr(>F)  \nTEMP      2  0.36598 0.25398 1.7022 0.02498 *\nResidual 10  1.07499 0.74602                 \nTotal    12  1.44097 1.00000                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n ***PERMANOVA for Bray-Curtis dissimilarity, `bray`*** \n\n\n       Df        SumOfSqs            R2               F        \n Min.   : 2   Min.   :0.8899   Min.   :0.2216   Min.   :1.423  \n 1st Qu.: 6   1st Qu.:2.0078   1st Qu.:0.5000   1st Qu.:1.423  \n Median :10   Median :3.1257   Median :0.7784   Median :1.423  \n Mean   : 8   Mean   :2.6770   Mean   :0.6667   Mean   :1.423  \n 3rd Qu.:11   3rd Qu.:3.5706   3rd Qu.:0.8892   3rd Qu.:1.423  \n Max.   :12   Max.   :4.0155   Max.   :1.0000   Max.   :1.423  \n                                                NA's   :2      \n     Pr(>F)       \n Min.   :0.03996  \n 1st Qu.:0.03996  \n Median :0.03996  \n Mean   :0.03996  \n 3rd Qu.:0.03996  \n Max.   :0.03996  \n NA's   :2        \n\n\n\n ***PERMANOVA for Gower distance, `gower`*** \n\n\n       Df        SumOfSqs            R2               F        \n Min.   : 2   Min.   :0.1076   Min.   :0.2416   Min.   :1.593  \n 1st Qu.: 6   1st Qu.:0.2225   1st Qu.:0.5000   1st Qu.:1.593  \n Median :10   Median :0.3375   Median :0.7584   Median :1.593  \n Mean   : 8   Mean   :0.2967   Mean   :0.6667   Mean   :1.593  \n 3rd Qu.:11   3rd Qu.:0.3913   3rd Qu.:0.8792   3rd Qu.:1.593  \n Max.   :12   Max.   :0.4451   Max.   :1.0000   Max.   :1.593  \n                                                NA's   :2      \n     Pr(>F)        \n Min.   :0.006993  \n 1st Qu.:0.006993  \n Median :0.006993  \n Mean   :0.006993  \n 3rd Qu.:0.006993  \n Max.   :0.006993  \n NA's   :2         \n\n\n\n\n\n\n\n ***PERMANOVA for Jensen-Shannon Divergence, `jsd`*** \n\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = its18_ps_perfect_beta_dist_jsd ~ TEMP, data = adonis_sampledf, permutations = 1000)\n         Df SumOfSqs      R2      F  Pr(>F)  \nTEMP      2  0.38311 0.23194 1.5099 0.04296 *\nResidual 10  1.26864 0.76806                 \nTotal    12  1.65176 1.00000                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n ***PERMANOVA for Bray-Curtis dissimilarity, `bray`*** \n\n\n       Df        SumOfSqs            R2               F        \n Min.   : 2   Min.   :0.8938   Min.   :0.2056   Min.   :1.294  \n 1st Qu.: 6   1st Qu.:2.1734   1st Qu.:0.5000   1st Qu.:1.294  \n Median :10   Median :3.4530   Median :0.7944   Median :1.294  \n Mean   : 8   Mean   :2.8978   Mean   :0.6667   Mean   :1.294  \n 3rd Qu.:11   3rd Qu.:3.8999   3rd Qu.:0.8972   3rd Qu.:1.294  \n Max.   :12   Max.   :4.3468   Max.   :1.0000   Max.   :1.294  \n                                                NA's   :2      \n     Pr(>F)       \n Min.   :0.07293  \n 1st Qu.:0.07293  \n Median :0.07293  \n Mean   :0.07293  \n 3rd Qu.:0.07293  \n Max.   :0.07293  \n NA's   :2        \n\n\n\n ***PERMANOVA for Gower distance, `gower`*** \n\n\n       Df        SumOfSqs            R2               F        \n Min.   : 2   Min.   :0.0647   Min.   :0.2319   Min.   :1.509  \n 1st Qu.: 6   1st Qu.:0.1395   1st Qu.:0.5000   1st Qu.:1.509  \n Median :10   Median :0.2143   Median :0.7681   Median :1.509  \n Mean   : 8   Mean   :0.1860   Mean   :0.6667   Mean   :1.509  \n 3rd Qu.:11   3rd Qu.:0.2466   3rd Qu.:0.8841   3rd Qu.:1.509  \n Max.   :12   Max.   :0.2790   Max.   :1.0000   Max.   :1.509  \n                                                NA's   :2      \n     Pr(>F)        \n Min.   :0.001998  \n 1st Qu.:0.001998  \n Median :0.001998  \n Mean   :0.001998  \n 3rd Qu.:0.001998  \n Max.   :0.001998  \n NA's   :2         \n\n\n\n\n\n\n\n ***PERMANOVA for Jensen-Shannon Divergence, `jsd`*** \n\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = its18_ps_pime_beta_dist_jsd ~ TEMP, data = adonis_sampledf, permutations = 1000)\n         Df SumOfSqs     R2      F   Pr(>F)    \nTEMP      2  0.54824 0.4002 3.3361 0.000999 ***\nResidual 10  0.82167 0.5998                    \nTotal    12  1.36991 1.0000                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n ***PERMANOVA for Bray-Curtis dissimilarity, `bray`*** \n\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 1000\n\nadonis2(formula = its18_ps_pime_beta_dist_bray ~ TEMP, data = adonis_sampledf, permutations = 1000)\n         Df SumOfSqs      R2     F   Pr(>F)    \nTEMP      2   1.1998 0.30872 2.233 0.000999 ***\nResidual 10   2.6865 0.69128                   \nTotal    12   3.8862 1.00000                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n ***ANOSIM for Gower distance, `gower`*** \n\n\n\nCall:\nanosim(x = phyloseq::distance(its18_ps_pime_prop, \"gower\"), grouping = its18_ps_pime_groups) \nDissimilarity: gower \n\nANOSIM statistic R: 0.6542 \n      Significance: 0.001 \n\nPermutation: free\nNumber of permutations: 999\n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.151 0.221 0.265 0.346 \n\nDissimilarity ranks between and within classes:\n        0%   25%  50%   75% 100%  N\nBetween  4 31.50 48.5 64.25   78 56\n0       22 25.00 34.5 40.75   57 10\n3        8 12.75 15.5 16.75   19  6\n8        1  2.25  4.0  5.75    7  6"
  },
  {
    "objectID": "beta.html#summaries-1",
    "href": "beta.html#summaries-1",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "Summaries",
    "text": "Summaries\nHere is a quick summary of significance tests for the data sets against three distance matrices.\n\n\n\n(ITS) Table 2 | Summary of significant tests for the unfiltered & filtered data sets. P-values in red indicate significant differences (p-value < 0.05).\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "beta.html#s-rrna-1",
    "href": "beta.html#s-rrna-1",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "16S rRNA",
    "text": "16S rRNA\n\n\n\nFULL\nArbitrary\nPERfect\nPIME\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 1 | PCoA ordination plots of the FULL data set. Top = phyloseq, bottom = microeco; Left = Jensen-Shannon Divergence, middle = UniFrac (unwieghted), right = UniFrac (wieghted).\n\n\n\n\n\n\n\n(16S rRNA) Figure 2 | PCoA ordination plots of the Arbitrary filtered data set. Top = phyloseq, bottom = microeco; Left = Jensen-Shannon Divergence, middle = UniFrac (unwieghted), right = UniFrac (wieghted).\n\n\n\n\n\n\n\n(16S rRNA) Figure 3 | PCoA ordination plots of the PERfect filtered data set. Top = phyloseq, bottom = microeco; Left = Jensen-Shannon Divergence, middle = UniFrac (unwieghted), right = UniFrac (wieghted).\n\n\n\n\n\n\n\n(16S rRNA) Figure 4 | PCoA ordination plots of the PIME filtered data set. Top = phyloseq, bottom = microeco; Left = Jensen-Shannon Divergence, middle = UniFrac (unwieghted), right = UniFrac (wieghted)."
  },
  {
    "objectID": "beta.html#its-1",
    "href": "beta.html#its-1",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "ITS",
    "text": "ITS\n\n\n\nArbitrary\nFILT\nPERfect\nPIME\n\n\n\n\n\n\n\n\n(ITS) Figure 1 | PCoA ordination plots of the FULL data set. Top = phyloseq, bottom = microeco; Left = Jensen-Shannon Divergence, middle = Bray-Curtis dissimilarity, right = Gower distance.\n\n\n\n\n\n\n\n(ITS) Figure 2 | PCoA ordination plots of the Arbitrary filtered data set. Top = phyloseq, bottom = microeco; Left = Jensen-Shannon Divergence, middle = Bray-Curtis dissimilarity, right = Gower distance.\n\n\n\n\n\n\n\n(ITS) Figure 3 | PCoA ordination plots of the PERfect filtered data set. Top = phyloseq, bottom = microeco; Left = Jensen-Shannon Divergence, middle = Bray-Curtis dissimilarity, right = Gower distance.\n\n\n\n\n\n\n\n(ITS) Figure 4 | PCoA ordination plots of the PIME filtered data set. Top = phyloseq, bottom = microeco; Left = Jensen-Shannon Divergence, middle = Bray-Curtis dissimilarity, right = Gower distance."
  },
  {
    "objectID": "beta.html#s-rrna-2",
    "href": "beta.html#s-rrna-2",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "16S rRNA",
    "text": "16S rRNA\n\n\n\n\n\n\n\n\n\n\n\n\nFULL\nArbitrary\nPERfect\nPIME\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 5 | Within (top) and between (bottom) group variation plots of the FULL data set. Left = Jensen-Shannon Divergence, middle = UniFrac (unwieghted), right = UniFrac (wieghted). Significant differences denoted by asterisks (* p < 0.05, ** p < 0.01, *** p < 0.001, **** p < 0.0001). ns = not significant. \n\n\n\n\n\n\n\n(16S rRNA) Figure 6 | Within (top) and between (bottom) group variation plots of the Arbitrary filtered data set. Left = Jensen-Shannon Divergence, middle = UniFrac (unwieghted), right = UniFrac (wieghted). Significant differences denoted by asterisks (* p < 0.05, ** p < 0.01, *** p < 0.001, **** p < 0.0001). ns = not significant. \n\n\n\n\n\n\n\n(16S rRNA) Figure 7 | Within (top) and between (bottom) group variation plots of the PERfect filtered data set. Left = Jensen-Shannon Divergence, middle = UniFrac (unwieghted), right = UniFrac (wieghted). Significant differences denoted by asterisks (* p < 0.05, ** p < 0.01, *** p < 0.001, **** p < 0.0001). ns = not significant. \n\n\n\n\n\n\n\n(16S rRNA) Figure 8 | Within (top) and between (bottom) group variation plots of the PIME filtered data set. Left = Jensen-Shannon Divergence, middle = UniFrac (unwieghted), right = UniFrac (wieghted). Significant differences denoted by asterisks (* p < 0.05, ** p < 0.01, *** p < 0.001, **** p < 0.0001). ns = not significant."
  },
  {
    "objectID": "beta.html#its-2",
    "href": "beta.html#its-2",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "ITS",
    "text": "ITS\n\n\n\n\n\n\n\n\n\n\n\n\nFULL\nArbitrary\nPERfect\nPIME\n\n\n\n\n\n\n\n\n(ITS) Figure 5 | Within (top) and between (bottom) group variation plots of the FULL data set. Left = Jensen-Shannon Divergence, middle = Bray-Curtis dissimilarity, right = Gower distance. Significant differences denoted by asterisks (* p < 0.05, ** p < 0.01, *** p < 0.001, **** p < 0.0001). ns = not significant. \n\n\n\n\n\n\n\n(ITS) Figure 6 | Within (top) and between (bottom) group variation plots of the Arbitrary filtered data set. Left = Jensen-Shannon Divergence, middle = Bray-Curtis dissimilarity, right = Gower distance. Significant differences denoted by asterisks (* p < 0.05, ** p < 0.01, *** p < 0.001, **** p < 0.0001). ns = not significant. \n\n\n\n\n\n\n\n(ITS) Figure 7 | Within (top) and between (bottom) group variation plots of the PERfect filtered data set. Left = Jensen-Shannon Divergence, middle = Bray-Curtis dissimilarity, right = Gower distance. Significant differences denoted by asterisks (* p < 0.05, ** p < 0.01, *** p < 0.001, **** p < 0.0001). ns = not significant. \n\n\n\n\n\n\n\n(ITS) Figure 8 | Within (top) and between (bottom) group variation plots of the PIME filtered data set. Left = Jensen-Shannon Divergence, middle = Bray-Curtis dissimilarity, right = Gower distance. Significant differences denoted by asterisks (* p < 0.05, ** p < 0.01, *** p < 0.001, **** p < 0.0001). ns = not significant."
  },
  {
    "objectID": "beta.html#data-availability",
    "href": "beta.html#data-availability",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "Data Availability",
    "text": "Data Availability\nData generated in this workflow and the Rdata need to run the workflow can be accessed on figshare at 10.25573/data.16828063."
  },
  {
    "objectID": "beta.html#last-updated-on",
    "href": "beta.html#last-updated-on",
    "title": "6. Beta Diversity & Dispersion Estimates",
    "section": "Last updated on",
    "text": "Last updated on\n\n\n[1] \"2022-06-21 13:38:05 EST\""
  },
  {
    "objectID": "da.html",
    "href": "da.html",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "",
    "text": "Click here for setup information.\n\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(119)\n#library(conflicted)\n#pacman::p_depends(microbiomeMarker, local = TRUE)  \n#pacman::p_depends_reverse(microbiomeMarker, local = TRUE)  \nlibrary(phyloseq); packageVersion(\"phyloseq\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, patchwork, ampvis2, \n               agricolae, labdsv, naniar, ape,\n               pairwiseAdonis, microbiome, seqRFLP, microbiomeMarker,\n               microbiomeMarker, reactable, downloadthis, captioner,\n               install = FALSE, update = FALSE)\n\noptions(scipen=999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))"
  },
  {
    "objectID": "da.html#workflow-input",
    "href": "da.html#workflow-input",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Workflow Input",
    "text": "Workflow Input\nFiles needed to run this workflow can be downloaded from figshare.\n\n\n\n\nIn this workflow, we use the labdsv package (Roberts and Roberts 2016) to run Dufrene-Legendre Indicator Species Analysis and the microbiomeMarker package (Cao 2020) to run linear discriminant analysis (LDA) effect size (LEfSe) (Segata et al. 2011)."
  },
  {
    "objectID": "da.html#indicator-analysis",
    "href": "da.html#indicator-analysis",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Indicator Analysis",
    "text": "Indicator Analysis\nIndicator Analysis calculates the indicator value (fidelity and relative abundance) of species in clusters or types.\n\nChoose a data set(s) & set the p-value cutoff.\n\n\nCodesamp_ps <- c(\"ssu18_ps_work\", \"ssu18_ps_filt\", \n             \"ssu18_ps_perfect\", \"ssu18_ps_pime\")\nindval_pval <- 0.05\n\n\n\nFormat Data Frame\n\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(i)\n     tmp_df <- data.frame(otu_table(tmp_get))\n     tmp_df <- tmp_df[, which(colSums(tmp_df) != 0)]\n     tmp_row_names <- row.names(tmp_df)\n     tmp_row_names <- tmp_row_names %>%\n              stringr::str_replace(\"P[0-9]{2}_D[0-9]{2}_[0-9]{3}_\", \"\") %>%\n              stringr::str_replace(\"[A-Z]$\", \"\")\n     tmp_df <- tmp_df %>% tibble::add_column(tmp_row_names, .before = 1)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_seq_tab\"))\n     assign(tmp_name, tmp_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()\n\n\n\nRun Indicator Analysis\n\n\nCodeset.seed(1191)\nfor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_seq_tab\")))\n     tmp_iva <- indval(tmp_get[,-1], tmp_get[,1])\n     tmp_gr <- tmp_iva$maxcls[tmp_iva$pval <= indval_pval]\n     tmp_iv <- tmp_iva$indcls[tmp_iva$pval <= indval_pval]\n     tmp_pv <- tmp_iva$pval[tmp_iva$pval <= indval_pval]\n     tmp_fr <- apply(tmp_get[,-1] > 0, 2, sum)[tmp_iva$pval <= indval_pval]\n     tmp_sum <- data.frame(group = tmp_gr, indval = tmp_iv,\n                                  pval = tmp_pv, freq = tmp_fr)\n     tmp_sum <- tmp_sum[order(tmp_sum$group, -tmp_sum$indval),]\n\n     tmp_tax_df <- data.frame(tax_table(get(i)))\n     tmp_tax_df$ASV_ID <- NULL\n     tmp_sum_tax <- merge(tmp_sum, tmp_tax_df, by = \"row.names\", all = TRUE)\n     tmp_sum_tax <- tmp_sum_tax[!(is.na(tmp_sum_tax$group)),]\n     class(tmp_sum_tax$group) <- \"character\"\n     tmp_sum_tax$group <- stringr::str_replace(tmp_sum_tax$group, \"^1$\", \"C0\")\n     tmp_sum_tax$group <- stringr::str_replace(tmp_sum_tax$group, \"^2$\", \"W3\")\n     tmp_sum_tax$group <- stringr::str_replace(tmp_sum_tax$group, \"^3$\", \"W8\")\n     tmp_sum_tax <- tmp_sum_tax %>% dplyr::rename(\"ASV_ID\" = \"Row.names\")\n     tmp_sum_tax <- tmp_sum_tax[order(as.numeric(gsub(\"[A-Z]{3}\", \"\", tmp_sum_tax$ASV_ID))),]\n     tmp_sum_tax$ASV_ID <-  as.character(tmp_sum_tax$ASV_ID)\n     tmp_sum.prob.corrected <- p.adjust(tmp_sum$pval, \"bonferroni\")\n     tmp_res_name <- purrr::map_chr(i, ~ paste0(., \"_indval_summary\"))\n     assign(tmp_res_name, tmp_sum_tax)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()\n\n\nNow we can save a few files and display the data.\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_seq_tab\")))\n     tmp_get[,1] <- NULL\n     tmp_df <- as.data.frame(t(tmp_get))\n     tmp_col_names <- colnames(tmp_df)\n     tmp_col_names <- tmp_col_names %>%\n              stringr::str_replace(\"P[0-9]{2}_D[0-9]{2}_[0-9]{3}_\", \"\") %>%\n              stringr::str_replace(\"[A-Z]$\", \"\")\n     colnames(tmp_df) <- tmp_col_names\n     #tmp_df$freq_all <- apply(tmp_df > 0, 1, sum)\n     tmp_df$freq_C0 <- apply(tmp_df[ , (names(tmp_df) %in% \"C0\")] > 0, 1, sum)\n     tmp_df$freq_W3 <- apply(tmp_df[ , (names(tmp_df) %in% \"W3\")] > 0, 1, sum)\n     tmp_df$freq_W8 <- apply(tmp_df[ , (names(tmp_df) %in% \"W8\")] > 0, 1, sum)\n     tmp_df$reads_total <- base::rowSums(tmp_df[ , (names(tmp_df) %in% c(\"C0\", \"W3\", \"W8\"))])\n     tmp_df$reads_C0 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"C0\")])\n     tmp_df$reads_W3 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"W3\")])\n     tmp_df$reads_W8 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"W8\")])\n     tmp_df <- tmp_df[,!grepl(\"^[C | W]\", names(tmp_df))]\n     tmp_df <- tmp_df %>% tibble::rownames_to_column(\"ASV_ID\")\n\n     tmp_get_indval <- get(purrr::map_chr(i, ~ paste0(., \"_indval_summary\")))\n     tmp_merge_df <- merge(tmp_df, tmp_get_indval, by = \"ASV_ID\", all = FALSE)\n     tmp_merge_df <- tmp_merge_df[,c(1,9:12,2:8,13:19)]\n     tmp_merge_df <- tmp_merge_df[order(tmp_merge_df$reads_total, decreasing = TRUE), ]\n     tmp_merge_name <- purrr::map_chr(i, ~ paste0(., \"_indval_final\"))\n     assign(tmp_merge_name, tmp_merge_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_indval_summary\")\n\n\n\nSave a new phyloseq object\n\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(i)\n     tmp_tab <- get(purrr::map_chr(i, ~ paste0(., \"_indval_summary\")))\n     tmp_list <- tmp_tab[,1]\n     tmp_ps <- prune_taxa(tmp_list, tmp_get)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_ind\"))\n     assign(tmp_name, tmp_ps)\n     tmp_ps@phy_tree <- NULL\n     tmp_ps <- prune_samples(sample_sums(tmp_ps) > 0, tmp_ps)\n     tmp_ps_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE,\n                            tip.label = taxa_names(tmp_ps))\n     tmp_ps <- merge_phyloseq(tmp_ps, sample_data, tmp_ps_tree)\n     assign(tmp_name, tmp_ps)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()"
  },
  {
    "objectID": "da.html#indicator-summary",
    "href": "da.html#indicator-summary",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Indicator Summary",
    "text": "Indicator Summary\n\n\n\n\nFULL\nArbitrary filtered\nPERfect filtered\nPIME filtered\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 1 | Significant results of Indicator Analysis for FULL data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 2 | Significant results of Indicator Analysis for Arbitrary filtered ASV data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 3 | Significant results of Indicator Analysis for PERfect filtered data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 4 | Significant results of Indicator Analysis for PIME data set."
  },
  {
    "objectID": "da.html#lefse-analysis",
    "href": "da.html#lefse-analysis",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "LEfSe Analysis",
    "text": "LEfSe Analysis\n\n\n\n\n\n\nNote\n\n\n\nFrom (Segata et al. 2011).\n\n\nLinear discriminant analysis (LDA) effect size (LEfSe) is a method to support high-dimensional class comparisons with a particular focus on microbial community analyses. LEfSe determines the features (organisms, clades, operational taxonomic units, genes, or functions) most likely to explain differences between classes by coupling standard tests for statistical significance with additional tests encoding biological consistency and effect relevance. Class comparison methods typically predict biomarkers consisting of features that violate a null hypothesis of no difference between classes. The effect size provides an estimation of the magnitude of the observed phenomenon due to each characterizing feature and it is thus a valuable tool for ranking the relevance of different biological aspects and for addressing further investigations and analyses.\nFirst, a little data tidying and subsetting only ASV data from the taxonomy table.\n\nCodefor (i in samp_ps) {\n  tmp_get <- get(i)\n  tax_table(tmp_get) <- tax_table(tmp_get)[,c(1:6)]\n  tax_table(tmp_get) <- cbind(tax_table(tmp_get),\n                           rownames(tax_table(tmp_get)))\n  colnames(tax_table(tmp_get)) <- c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \n                                    \"Family\", \"Genus\", \"Species\")\n  tmp_name <- purrr::map_chr(i, ~ paste0(., \"_rf_all\"))\n  assign(tmp_name, tmp_get)\n  tax_table(tmp_get) <- tax_table(tmp_get)[,c(7)]\n  tmp_name_asv <- purrr::map_chr(i, ~ paste0(., \"_rf_asv\"))\n  assign(tmp_name_asv, tmp_get)\n  rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_rf\")\n\n\n\nChoose a data set(s) & set the LEfSe cutoff values.\n\n\nsamp_ps <- c(\"ssu18_ps_work\", \"ssu18_ps_filt\", \"ssu18_ps_perfect\", \"ssu18_ps_pime\")\nlefse_lda <- 2\nlefse_kw <- 0.05\nlefse_wc <- 0.05\n\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_rf_asv\")))\n     tmp_lefse <- run_lefse(tmp_get, norm = \"CPM\", class = \"TEMP\", correct = \"2\", \n                        lda_cutoff = lefse_lda, \n                        kw_cutoff = lefse_kw, \n                        wilcoxon_cutoff = lefse_wc, \n                        bootstrap_n = 30, bootstrap_fraction = 2/3, \n                        sample_min = 10, multicls_strat = FALSE, curv = FALSE)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_asv\"))\n     assign(tmp_name, tmp_lefse)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_lefse_asv\")\n\nfor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_rf_all\")))\n     tmp_lefse <- run_lefse(tmp_get, norm = \"CPM\", class = \"TEMP\", correct = \"2\", \n                        lda_cutoff = lefse_lda, \n                        kw_cutoff = lefse_kw, \n                        wilcoxon_cutoff = lefse_wc, \n                        bootstrap_n = 30, bootstrap_fraction = 2/3, \n                        sample_min = 10, multicls_strat = FALSE, curv = FALSE)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_all\"))\n     assign(tmp_name, tmp_lefse)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_lefse_asv\")\nobjects(pattern = \"_lefse_all\")\n\n\n\n\n\n\nCodefor (i in samp_ps) {\n    tmp_o_tax <- data.frame(tax_table(get(i)))\n    tmp_o_tax$ASV_ID <- NULL \n    tmp_o_tax <- tmp_o_tax %>% tibble::rownames_to_column(\"ASV_ID\")\n    \n    tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_asv\")))\n    tmp_mt <- data.frame(marker_table(tmp_get))\n    tmp_mt$feature <- gsub('s__', '', tmp_mt$feature)\n    tmp_mt <- tmp_mt %>% dplyr::rename(c(\"ASV_ID\" = 1, \"group\" = 2, \"pval\" = 4)) %>%\n          tibble::remove_rownames()\n    tmp_sum <- dplyr::left_join(tmp_mt, tmp_o_tax, by = \"ASV_ID\")\n    tmp_sum$group <- stringr::str_replace(tmp_sum$group, \"^0$\", \"C0\")\n    tmp_sum$group <- stringr::str_replace(tmp_sum$group, \"^3$\", \"W3\")\n    tmp_sum$group <- stringr::str_replace(tmp_sum$group, \"^8$\", \"W8\")\n\n    tmp_sum <- tmp_sum[order(as.numeric(gsub(\"[A-Z]{3}\", \"\", tmp_sum$ASV_ID))),]\n    \n    tmp_res_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_summary\"))\n    assign(tmp_res_name, tmp_sum)\n    rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nNow we can save a few files and display the data.\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(i)\n     tmp_df <- data.frame(t(otu_table(tmp_get)))\n     #tmp_get[,1] <- NULL\n     #tmp_df <- as.data.frame(t(tmp_otu))\n     tmp_col_names <- colnames(tmp_df)\n     tmp_col_names <- tmp_col_names %>%\n              stringr::str_replace(\"P[0-9]{2}_D[0-9]{2}_[0-9]{3}_\", \"\") %>%\n              stringr::str_replace(\"[A-Z]$\", \"\")\n     colnames(tmp_df) <- tmp_col_names\n     tmp_df$freq <- apply(tmp_df > 0, 1, sum)\n     tmp_df$freq_C0 <- apply(tmp_df[ , (names(tmp_df) %in% \"C0\")] > 0, 1, sum)\n     tmp_df$freq_W3 <- apply(tmp_df[ , (names(tmp_df) %in% \"W3\")] > 0, 1, sum)\n     tmp_df$freq_W8 <- apply(tmp_df[ , (names(tmp_df) %in% \"W8\")] > 0, 1, sum)\n     tmp_df$reads_total <- base::rowSums(tmp_df[ , (names(tmp_df) %in% c(\"C0\", \"W3\", \"W8\"))])\n     tmp_df$reads_C0 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"C0\")])\n     tmp_df$reads_W3 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"W3\")])\n     tmp_df$reads_W8 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"W8\")])\n     tmp_df <- tmp_df[,!grepl(\"^[C | W]\", names(tmp_df))]\n     tmp_df <- tmp_df %>% tibble::rownames_to_column(\"ASV_ID\")\n\n     tmp_get_lefse <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_summary\")))\n     tmp_merge_df <- merge(tmp_df, tmp_get_lefse, by = \"ASV_ID\", all = FALSE)\n\n     tmp_merge_df <- tmp_merge_df[,c(1,10:12,2:9,14:20)]\n     tmp_merge_df <- tmp_merge_df[order(tmp_merge_df$reads_total, decreasing = TRUE), ]\n     tmp_merge_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_final\"))\n     assign(tmp_merge_name, tmp_merge_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nSave a new phyloseq object\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(i)\n     tmp_tab <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_summary\")))\n     tmp_list <- tmp_tab[,1]\n     tmp_ps <- prune_taxa(tmp_list, tmp_get)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_lefse\"))\n     assign(tmp_name, tmp_ps)\n     tmp_ps@phy_tree <- NULL\n     tmp_ps <- prune_samples(sample_sums(tmp_ps) > 0, tmp_ps)\n     tmp_ps_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE,\n                            tip.label = taxa_names(tmp_ps))\n     tmp_ps <- merge_phyloseq(tmp_ps, sample_data, tmp_ps_tree)\n     assign(tmp_name, tmp_ps)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()"
  },
  {
    "objectID": "da.html#lefse-summaries-asvs-markers",
    "href": "da.html#lefse-summaries-asvs-markers",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "LEfSe Summaries (ASVs & Markers)",
    "text": "LEfSe Summaries (ASVs & Markers)\nASV summary data\n\n\n\n\nFULL\nArbitrary filtered\nPERfect filtered\nPIME filtered\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 5 | Significant results of LEfSe Analysis for FULL data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 6 | Significant results of LEfSe Analysis for Arbitrary filtered ASV data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 7 | Significant results of LEfSe Analysis for PERfect filtered data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 8 | Significant results of LEfSe Analysis for PIME data set.\n\n\n\n\n\n\n\n\n\n\n\nASV Visualizations\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_asv\")))\n     tmp_mt <- data.frame(marker_table(tmp_get))\n     tmp_mt  <-  tmp_mt %>% filter(ef_lda >= 3)\n     tmp_mt <- marker_table(tmp_mt)\n     tmp_res_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_asv_viz_trim\"))\n     assign(tmp_res_name, tmp_mt)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\nobjects(pattern = \"_lefse_asv_viz_trim\")\n\n\n\n\n\n\nFULL\nArbitrary filtered\nPERfect filtered\nPIME filtered\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 1 | Differentially abundant ASVs from LEfSe analysis for the FULL data set, trimmed to show those with LDA score ≥ 3.0.\n\n\n\n\n\n\n\n(16S rRNA) Figure 2 | Differentially abundant ASVs from LEfSe analysis for the Arbitrary filtered data set, trimmed to show those with LDA score ≥ 3.0.\n\n\n\n\n\n\n\n(16S rRNA) Figure 3 | Differentially abundant ASVs from LEfSe analysis for the PERfect filtered data set, trimmed to show those with LDA score ≥ 3.0.\n\n\n\n\n\n\n\n(16S rRNA) Figure 4 | Differentially abundant ASVs from LEfSe analysis for the PIME filtered data set, trimmed to show those with LDA score ≥ 3.0.\n\n\n\n\nMarker summary data\n\nCodefor (i in samp_ps) {\n    tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_all\")))\n    tmp_mt <- data.frame(marker_table(tmp_get))\n    tmp_mt <- tmp_mt[, c(2:4,1)]\n    tmp_mt <- tmp_mt %>% dplyr::rename(c(\"group\" = 1, \"pval\" = 3))\n    tmp_mt$group <- stringr::str_replace(tmp_mt$group, \"^0$\", \"C0\")\n    tmp_mt$group <- stringr::str_replace(tmp_mt$group, \"^3$\", \"W3\")\n    tmp_mt$group <- stringr::str_replace(tmp_mt$group, \"^8$\", \"W8\")\n    tmp_mt <- tmp_mt %>% tibble::rownames_to_column(\"marker\")\n    tmp_mt <- tmp_mt %>% tidyr::separate(col = \"feature\", \n                                         into = c(\"Kingdom\", \"Phylum\", \"Class\", \n                                                  \"Order\", \"Family\", \"Genus\", \"ASV_ID\"), \n                                         sep = \"\\\\|\\\\w__\")\n    \n    tmp_mt$Kingdom <- gsub('k__', '', tmp_mt$Kingdom)\n    tmp_res_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_marker_final\"))\n    assign(tmp_res_name, tmp_mt)\n    rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern =  \"_lefse_marker_final\")\n\n\n\n\n\n\nFULL\nArbitrary filtered\nPERfect filtered\nPIME filtered\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 9 | Significant results of LEfSe MARKER Analysis for FULL data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 10 | Significant results of LEfSe MARKER Analysis for Arbitrary filtered ASV data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 11 | Significant results of LEfSe MARKER Analysis for PERfect filtered data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 12 | Significant results of LEfSe MARKER Analysis for PIME data set.\n\n\n\n\n\n\n\n\n\n\n\nMarker visualizations\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_all\")))\n     tmp_mt <- data.frame(marker_table(tmp_get))\n     tmp_mt  <-  tmp_mt %>% filter(ef_lda >= 4)\n     tmp_mt <- marker_table(tmp_mt)\n     tmp_res_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_all_viz_trim\"))\n     assign(tmp_res_name, tmp_mt)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_lefse_all_viz_trim\")\n\n\n\n\n\n\nFULL\nArbitrary filtered\nPERfect filtered\nPIME filtered\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 5 | Differentially abundant MARKER from LEfSe analysis for the FULL data set, trimmed to show those with LDA score ≥ 4.0.\n\n\n\n\n\n\n\n(16S rRNA) Figure 6 | Differentially abundant MARKER from LEfSe analysis for the Arbitrary filtered data set, trimmed to show those with LDA score ≥ 4.0.\n\n\n\n\n\n\n\n(16S rRNA) Figure 7 | Differentially abundant MARKER from LEfSe analysis for the PERfect filtered data set, trimmed to show those with LDA score ≥ 4.0.\n\n\n\n\n\n\n\n(16S rRNA) Figure 8 | Differentially abundant MARKER from LEfSe analysis for the PIME filtered data set, trimmed to show those with LDA score ≥ 4.0."
  },
  {
    "objectID": "da.html#response-of-select-taxa",
    "href": "da.html#response-of-select-taxa",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Response of select taxa",
    "text": "Response of select taxa\nIn this section of the workflow we use the microbiomeMarker package to assess the response of taxonomic lineages to soil warming. In the first step we need to fix the selected data set to make it compatible with the various functions. For this analysis we use the PERfect filtered data set.\n\n\n\n\nCode## FIX ps object\nssu_ps <- ssu18_ps_perfect_rf_all\ntmp_tax1 <- data.frame(tax_table(ssu_ps))\n#tmp_tax1$ASV_SEQ <- NULL\ntmp_rn <- row.names(tmp_tax1)\ntmp_tax <- data.frame(lapply(tmp_tax1, function(x) {gsub(\"\\\\(|)\", \"\", x)}))\nrow.names(tmp_tax) <- tmp_rn\nidentical(row.names(tmp_tax), row.names(tmp_tax1))\n#dplyr::filter(tmp_tax, Phylum == \"Acidobacteriota\")\nps_tax_new <- as.matrix(tmp_tax)\ntmp_ps <- phyloseq(otu_table(ssu_ps),\n                     phy_tree(ssu_ps),\n                     tax_table(ps_tax_new),\n                     sample_data(ssu_ps))\nssu_ps <- tmp_ps\nphyloseq::rank_names(ssu_ps)\ndata.frame(tax_table(ssu_ps))\n\n\nNext we run a statistical test for multiple groups using the run_test_multiple_groups function.\n\nCodessu_group_anova <-  run_test_multiple_groups(ssu_ps, group = \"TEMP\", \n                                             taxa_rank = \"all\", \n                                             method = \"anova\")\nssu_group_anova@marker_table\nmarker_table(ssu_group_anova)\n\n\n\n\n\nAnd then conduct ost hoc pairwise comparisons for multiple groups test using the run_posthoc_test function.\n\nCodessu_default_pht <- run_posthoc_test(ssu_ps, group = \"TEMP\", \n                                    method = \"tukey\", transform = \"log10\")\n\n\nWe can filter out a select taxa and plot the results.\n\nfilter(data.frame(ssu_default_pht@result), group_name == \"k__Bacteria|p__Actinobacteriota|c__Thermoleophilia|o__Gaiellales\")\n\n        group                                                       group_name\n3-0.140   141 k__Bacteria|p__Actinobacteriota|c__Thermoleophilia|o__Gaiellales\n8-0.140   141 k__Bacteria|p__Actinobacteriota|c__Thermoleophilia|o__Gaiellales\n8-3.140   141 k__Bacteria|p__Actinobacteriota|c__Thermoleophilia|o__Gaiellales\n        comparions   diff_mean      pvalue      ci_lower   ci_upper\n3-0.140        3-0 0.007492377 0.645933374 -0.0145296109 0.02951437\n8-0.140        8-0 0.030358682 0.008215796  0.0083366941 0.05238067\n8-3.140        8-3 0.022866305 0.041743700  0.0008443168 0.04488829\n\nplot_postHocTest(ssu_default_pht, feature = \"k__Bacteria|p__Actinobacteriota|c__Thermoleophilia|o__Gaiellales\") & theme_bw()\n\n\n\n\nBut what we really want to do is get all of the markers that are significant from the analysis, excluding any significant ASVs so we can look at high taxa ranks.\n\nCodessu_pht <- ssu_default_pht\nssu18_pht_filt <- filter(data.frame(ssu_pht@result), pvalue <= \"0.05\")[!grepl(\"ASV\", filter(data.frame(ssu_pht@result), pvalue <= \"0.05\")$group_name),]\nssu18_pht_filt <- ssu18_pht_filt[!grepl(\"[a-z]__$\", ssu18_pht_filt$group_name),]\nssu18_pht_filt <- unique(ssu18_pht_filt$group_name)\nlength(ssu18_pht_filt)\nsave.image(\"page_build/mmarker_wf.rdata\")\n\n\nThere are 47 significantly different lineage markers.\nClick here to see a list of significantly different lineage markers.\n\n\n [1] \"k__Archaea|p__Thermoplasmatota\"                                                                           \n [2] \"k__Bacteria|p__Actinobacteriota\"                                                                          \n [3] \"k__Bacteria|p__Bacteroidota\"                                                                              \n [4] \"k__Bacteria|p__Firmicutes\"                                                                                \n [5] \"k__Bacteria|p__MBNT15\"                                                                                    \n [6] \"k__Bacteria|p__Myxococcota\"                                                                               \n [7] \"k__Archaea|p__Thermoplasmatota|c__Thermoplasmata\"                                                         \n [8] \"k__Bacteria|p__Acidobacteriota|c__Subgroup_11\"                                                            \n [9] \"k__Bacteria|p__Acidobacteriota|c__Subgroup_22\"                                                            \n[10] \"k__Bacteria|p__Actinobacteriota|c__MB-A2-108\"                                                             \n[11] \"k__Bacteria|p__Actinobacteriota|c__Thermoleophilia\"                                                       \n[12] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia\"                                                               \n[13] \"k__Bacteria|p__Firmicutes|c__Bacilli\"                                                                     \n[14] \"k__Bacteria|p__Myxococcota|c__bacteriap25\"                                                                \n[15] \"k__Bacteria|p__Planctomycetota|c__Pla4_lineage\"                                                           \n[16] \"k__Bacteria|p__Acidobacteriota|c__Acidobacteriae|o__Subgroup_2\"                                           \n[17] \"k__Bacteria|p__Actinobacteriota|c__Acidimicrobiia|o__Microtrichales\"                                      \n[18] \"k__Bacteria|p__Actinobacteriota|c__Thermoleophilia|o__Gaiellales\"                                         \n[19] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Cytophagales\"                                               \n[20] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Flavobacteriales\"                                           \n[21] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Sphingobacteriales\"                                         \n[22] \"k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales\"                                                       \n[23] \"k__Bacteria|p__Myxococcota|c__Polyangia|o__mle1-27\"                                                       \n[24] \"k__Bacteria|p__Myxococcota|c__Polyangia|o__Polyangiales\"                                                  \n[25] \"k__Bacteria|p__Actinobacteriota|c__Thermoleophilia|o__Gaiellales|f__Gaiellaceae\"                          \n[26] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Chitinophagales|f__Saprospiraceae\"                          \n[27] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Cytophagales|f__Hymenobacteraceae\"                          \n[28] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Flavobacteriales|f__Flavobacteriaceae\"                      \n[29] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Sphingobacteriales|f__AKYH767\"                              \n[30] \"k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales|f__Planococcaceae\"                                     \n[31] \"k__Bacteria|p__Myxococcota|c__Polyangia|o__Polyangiales|f__BIrii41\"                                       \n[32] \"k__Bacteria|p__Myxococcota|c__Polyangia|o__Polyangiales|f__Polyangiaceae\"                                 \n[33] \"k__Bacteria|p__Proteobacteria|c__Alphaproteobacteria|o__Caulobacterales|f__Hyphomonadaceae\"               \n[34] \"k__Bacteria|p__Actinobacteriota|c__Thermoleophilia|o__Gaiellales|f__Gaiellaceae|g__Gaiella\"               \n[35] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Chitinophagales|f__Chitinophagaceae|g__Edaphobaculum\"       \n[36] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Chitinophagales|f__Chitinophagaceae|g__UTBCD1\"              \n[37] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Cytophagales|f__Hymenobacteraceae|g__Adhaeribacter\"         \n[38] \"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Flavobacteriales|f__Flavobacteriaceae|g__Flavobacterium\"    \n[39] \"k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales|f__Planococcaceae|g__Chungangia\"                       \n[40] \"k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales|f__Planococcaceae|g__Paenisporosarcina\"                \n[41] \"k__Bacteria|p__Myxococcota|c__Myxococcia|o__Myxococcales|f__Myxococcaceae|g__Corallococcus\"               \n[42] \"k__Bacteria|p__Proteobacteria|c__Alphaproteobacteria|o__Caulobacterales|f__Hyphomonadaceae|g__SWB02\"      \n[43] \"k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Burkholderiales|f__Burkholderiaceae|g__Ralstonia\" \n[44] \"k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Burkholderiales|f__Comamonadaceae|g__Ramlibacter\" \n[45] \"k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Burkholderiales|f__Comamonadaceae|g__Rubrivivax\"  \n[46] \"k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Burkholderiales|f__Nitrosomonadaceae|g__mle1-7\"   \n[47] \"k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Xanthomonadales|f__Xanthomonadaceae|g__Arenimonas\"\n\n\n\n\n\nNow we select a subset of the 47 markers to plot and visualize the results.\n\nCodessu_select <- c(\n\"k__Bacteria|p__Acidobacteriota|c__Acidobacteriae|o__Subgroup_2\", \n\"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Chitinophagales|f__Saprospiraceae\", \n\"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Cytophagales\", \n\"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Flavobacteriales\", \n\"k__Bacteria|p__Bacteroidota|c__Bacteroidia|o__Sphingobacteriales\", \n\"k__Bacteria|p__Myxococcota|c__Polyangia|o__mle1-27\", \n\"k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Burkholderiales|f__Comamonadaceae|g__Rubrivivax\", \n\"k__Bacteria|p__Actinobacteriota|c__Acidimicrobiia|o__Microtrichales\", \n\"k__Bacteria|p__Actinobacteriota|c__Thermoleophilia|o__Gaiellales\", \n\"k__Bacteria|p__Firmicutes|c__Bacilli|o__Bacillales\", \n\"k__Bacteria|p__Myxococcota|c__Myxococcia|o__Myxococcales|f__Myxococcaceae|g__Corallococcus\", \n\"k__Bacteria|p__Proteobacteria|c__Gammaproteobacteria|o__Burkholderiales|f__Burkholderiaceae|g__Ralstonia\" \n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 9 | The response of select taxa to two years of warming by +3°C and +8°C. Differences assessed for multiple-group pair-wise comparisons using ANOVA followed by Tukey HSD post hoc tests. PERfect filtered read count data was log10 transformed and normalized using total sum scaling (TSS). The centre line of each box plot represents the median, the lower and upper hinges represent the first and third quartiles and whiskers represent + 1.5 the interquartile range. Significant differences denoted by asterisks (* p ≤ 0.05, ** p ≤ 0.01, *** p ≤ 0.001, **** p ≤ 0.0001; ns = not significant)."
  },
  {
    "objectID": "da.html#indicator-analysis-1",
    "href": "da.html#indicator-analysis-1",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Indicator Analysis",
    "text": "Indicator Analysis\n\nChoose a data set(s) & set the p-value cutoff.\n\n\nsamp_ps <- c(\"its18_ps_work\", \"its18_ps_filt\", \"its18_ps_perfect\", \"its18_ps_pime\") \nindval_pval <- 0.05\n\n\nFormat Data Frame\n\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(i)\n     tmp_df <- data.frame(otu_table(tmp_get))\n     tmp_df <- tmp_df[, which(colSums(tmp_df) != 0)]\n     tmp_row_names <- row.names(tmp_df)\n     tmp_row_names <- tmp_row_names %>%\n              stringr::str_replace(\"P[0-9]{2}_D[0-9]{2}_[0-9]{3}_\", \"\") %>%\n              stringr::str_replace(\"[A-Z]$\", \"\")\n     tmp_df <- tmp_df %>% tibble::add_column(tmp_row_names, .before = 1)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_seq_tab\"))\n     assign(tmp_name, tmp_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()\n\n\n\nRun Indicator Analysis\n\n\nCodeset.seed(1191)\nfor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_seq_tab\")))\n     tmp_iva <- indval(tmp_get[,-1], tmp_get[,1])\n     tmp_gr <- tmp_iva$maxcls[tmp_iva$pval <= indval_pval]\n     tmp_iv <- tmp_iva$indcls[tmp_iva$pval <= indval_pval]\n     tmp_pv <- tmp_iva$pval[tmp_iva$pval <= indval_pval]\n     tmp_fr <- apply(tmp_get[,-1] > 0, 2, sum)[tmp_iva$pval <= indval_pval]\n     tmp_sum <- data.frame(group = tmp_gr, indval = tmp_iv,\n                                  pval = tmp_pv, freq = tmp_fr)\n     tmp_sum <- tmp_sum[order(tmp_sum$group, -tmp_sum$indval),]\n\n     tmp_tax_df <- data.frame(tax_table(get(i)))\n     tmp_tax_df$ASV_ID <- NULL\n     tmp_sum_tax <- merge(tmp_sum, tmp_tax_df, by = \"row.names\", all = TRUE)\n     tmp_sum_tax <- tmp_sum_tax[!(is.na(tmp_sum_tax$group)),]\n     class(tmp_sum_tax$group) <- \"character\"\n     tmp_sum_tax$group <- stringr::str_replace(tmp_sum_tax$group, \"^1$\", \"C0\")\n     tmp_sum_tax$group <- stringr::str_replace(tmp_sum_tax$group, \"^2$\", \"W3\")\n     tmp_sum_tax$group <- stringr::str_replace(tmp_sum_tax$group, \"^3$\", \"W8\")\n     tmp_sum_tax <- tmp_sum_tax %>% dplyr::rename(\"ASV_ID\" = \"Row.names\")\n     tmp_sum_tax <- tmp_sum_tax[order(as.numeric(gsub(\"[A-Z]{3}\", \"\", tmp_sum_tax$ASV_ID))),]\n     tmp_sum_tax$ASV_ID <-  as.character(tmp_sum_tax$ASV_ID)\n     tmp_sum.prob.corrected <- p.adjust(tmp_sum$pval, \"bonferroni\")\n     tmp_res_name <- purrr::map_chr(i, ~ paste0(., \"_indval_summary\"))\n     assign(tmp_res_name, tmp_sum_tax)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nNow we can save a few files and display the data.\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_seq_tab\")))\n     tmp_get[,1] <- NULL\n     tmp_df <- as.data.frame(t(tmp_get))\n     tmp_col_names <- colnames(tmp_df)\n     tmp_col_names <- tmp_col_names %>%\n              stringr::str_replace(\"P[0-9]{2}_D[0-9]{2}_[0-9]{3}_\", \"\") %>%\n              stringr::str_replace(\"[A-Z]$\", \"\")\n     colnames(tmp_df) <- tmp_col_names\n     #tmp_df$freq_all <- apply(tmp_df > 0, 1, sum)\n     tmp_df$freq_C0 <- apply(tmp_df[ , (names(tmp_df) %in% \"C0\")] > 0, 1, sum)\n     tmp_df$freq_W3 <- apply(tmp_df[ , (names(tmp_df) %in% \"W3\")] > 0, 1, sum)\n     tmp_df$freq_W8 <- apply(tmp_df[ , (names(tmp_df) %in% \"W8\")] > 0, 1, sum)\n     tmp_df$reads_total <- base::rowSums(tmp_df[ , (names(tmp_df) %in% c(\"C0\", \n                                                                         \"W3\", \n                                                                         \"W8\"))])\n     tmp_df$reads_C0 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"C0\")])\n     tmp_df$reads_W3 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"W3\")])\n     tmp_df$reads_W8 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"W8\")])\n     tmp_df <- tmp_df[,!grepl(\"^[C | W]\", names(tmp_df))]\n     tmp_df <- tmp_df %>% tibble::rownames_to_column(\"ASV_ID\")\n\n     tmp_get_indval <- get(purrr::map_chr(i, ~ paste0(., \"_indval_summary\")))\n     tmp_merge_df <- merge(tmp_df, tmp_get_indval, by = \"ASV_ID\", all = FALSE)\n     tmp_merge_df <- tmp_merge_df[,c(1,9:12,2:8,13:19)]\n     tmp_merge_df <- tmp_merge_df[order(tmp_merge_df$reads_total, decreasing = TRUE), ]\n     tmp_merge_name <- purrr::map_chr(i, ~ paste0(., \"_indval_final\"))\n     assign(tmp_merge_name, tmp_merge_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nSave a new phyloseq object\n\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(i)\n     tmp_tab <- get(purrr::map_chr(i, ~ paste0(., \"_indval_summary\")))\n     tmp_list <- tmp_tab[,1]\n     tmp_ps <- prune_taxa(tmp_list, tmp_get)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_ind\"))\n     assign(tmp_name, tmp_ps)\n     tmp_ps@phy_tree <- NULL\n     tmp_ps <- prune_samples(sample_sums(tmp_ps) > 0, tmp_ps)\n     tmp_ps_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE,\n                            tip.label = taxa_names(tmp_ps))\n     tmp_ps <- merge_phyloseq(tmp_ps, sample_data, tmp_ps_tree)\n     assign(tmp_name, tmp_ps)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()"
  },
  {
    "objectID": "da.html#indicator-summary-1",
    "href": "da.html#indicator-summary-1",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Indicator Summary",
    "text": "Indicator Summary\n\n\n\n\nFULL\nArbitrary filtered\nPERfect filtered\nPIME filtered\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 1 | Significant results of Indicator Analysis for FULL data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 2 | Significant results of Indicator Analysis for Arbitrary filtered ASV data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 3 | Significant results of Indicator Analysis for PERfect filtered data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 4 | Significant results of Indicator Analysis for PIME data set."
  },
  {
    "objectID": "da.html#lefse-analysis-1",
    "href": "da.html#lefse-analysis-1",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "LEfSe Analysis",
    "text": "LEfSe Analysis\nDetails about LEFSE\nmicrobiomeMarker\nRemove ASV seq, subset only ASVs\n\nCodefor (i in samp_ps) {\n  tmp_get <- get(i)\n  tax_table(tmp_get) <- tax_table(tmp_get)[,c(1:6)]\n  tax_table(tmp_get) <- cbind(tax_table(tmp_get),\n                           rownames(tax_table(tmp_get)))\n  colnames(tax_table(tmp_get)) <- c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \n                                    \"Family\", \"Genus\", \"Species\")\n  tmp_name <- purrr::map_chr(i, ~ paste0(., \"_rf_all\"))\n  assign(tmp_name, tmp_get)\n  tax_table(tmp_get) <- tax_table(tmp_get)[,c(7)]\n  tmp_name_asv <- purrr::map_chr(i, ~ paste0(., \"_rf_asv\"))\n  assign(tmp_name_asv, tmp_get)\n  rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_rf\")\n\n\n\nChoose a data set(s) & set the LEfSe cutoff values.\n\n\nsamp_ps <- c(\"its18_ps_work\", \"its18_ps_filt\", \"its18_ps_perfect\", \"its18_ps_pime\") \nlefse_lda <- 2  \nlefse_kw <- 0.05    \nlefse_wc <- 0.05    \n\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_rf_asv\")))\n     tmp_lefse <- run_lefse(tmp_get, norm = \"CPM\", class = \"TEMP\", correct = \"2\", \n                        lda_cutoff = lefse_lda, kw_cutoff = lefse_kw, wilcoxon_cutoff = lefse_wc, \n                        bootstrap_n = 30, bootstrap_fraction = 2/3, \n                        sample_min = 10, multicls_strat = FALSE, curv = FALSE)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_asv\"))\n     assign(tmp_name, tmp_lefse)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_lefse_asv\")\n\nfor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_rf_all\")))\n     tmp_lefse <- run_lefse(tmp_get, norm = \"CPM\", class = \"TEMP\", correct = \"2\", \n                        lda_cutoff = lefse_lda, kw_cutoff = lefse_kw, wilcoxon_cutoff = lefse_wc, \n                        bootstrap_n = 30, bootstrap_fraction = 2/3, \n                        sample_min = 10, multicls_strat = FALSE, curv = FALSE)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_all\"))\n     assign(tmp_name, tmp_lefse)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_lefse_asv\")\nobjects(pattern = \"_lefse_all\")\n\n\n\n\n\n\nCodefor (i in samp_ps) {\n    tmp_o_tax <- data.frame(tax_table(get(i)))\n    tmp_o_tax$ASV_ID <- NULL \n    tmp_o_tax <- tmp_o_tax %>% tibble::rownames_to_column(\"ASV_ID\")\n    \n    tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_asv\")))\n    tmp_mt <- data.frame(marker_table(tmp_get))\n    tmp_mt$feature <- gsub('s__', '', tmp_mt$feature)\n    tmp_mt <- tmp_mt %>% dplyr::rename(c(\"ASV_ID\" = 1, \"group\" = 2, \"pval\" = 4)) %>%\n          tibble::remove_rownames()\n    tmp_sum <- dplyr::left_join(tmp_mt, tmp_o_tax, by = \"ASV_ID\")\n    tmp_sum$group <- stringr::str_replace(tmp_sum$group, \"^0$\", \"C0\")\n    tmp_sum$group <- stringr::str_replace(tmp_sum$group, \"^3$\", \"W3\")\n    tmp_sum$group <- stringr::str_replace(tmp_sum$group, \"^8$\", \"W8\")\n\n    tmp_sum <- tmp_sum[order(as.numeric(gsub(\"[A-Z]{3}\", \"\", tmp_sum$ASV_ID))),]\n    \n    tmp_res_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_summary\"))\n    assign(tmp_res_name, tmp_sum)\n    rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nNow we can save a few files and display the data.\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(i)\n     tmp_df <- data.frame(t(otu_table(tmp_get)))\n     #tmp_get[,1] <- NULL\n     #tmp_df <- as.data.frame(t(tmp_otu))\n     tmp_col_names <- colnames(tmp_df)\n     tmp_col_names <- tmp_col_names %>%\n              stringr::str_replace(\"P[0-9]{2}_D[0-9]{2}_[0-9]{3}_\", \"\") %>%\n              stringr::str_replace(\"[A-Z]$\", \"\")\n     colnames(tmp_df) <- tmp_col_names\n     tmp_df$freq <- apply(tmp_df > 0, 1, sum)\n     tmp_df$freq_C0 <- apply(tmp_df[ , (names(tmp_df) %in% \"C0\")] > 0, 1, sum)\n     tmp_df$freq_W3 <- apply(tmp_df[ , (names(tmp_df) %in% \"W3\")] > 0, 1, sum)\n     tmp_df$freq_W8 <- apply(tmp_df[ , (names(tmp_df) %in% \"W8\")] > 0, 1, sum)\n     tmp_df$reads_total <- base::rowSums(tmp_df[ , (names(tmp_df) %in% c(\"C0\", \"W3\", \"W8\"))])\n     tmp_df$reads_C0 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"C0\")])\n     tmp_df$reads_W3 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"W3\")])\n     tmp_df$reads_W8 <- base::rowSums(tmp_df[ , (names(tmp_df) %in% \"W8\")])\n     tmp_df <- tmp_df[,!grepl(\"^[C | W]\", names(tmp_df))]\n     tmp_df <- tmp_df %>% tibble::rownames_to_column(\"ASV_ID\")\n\n     tmp_get_lefse <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_summary\")))\n     tmp_merge_df <- merge(tmp_df, tmp_get_lefse, by = \"ASV_ID\", all = FALSE)\n\n     tmp_merge_df <- tmp_merge_df[,c(1,10:12,2:9,14:20)]\n     tmp_merge_df <- tmp_merge_df[order(tmp_merge_df$reads_total, decreasing = TRUE), ]\n     tmp_merge_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_final\"))\n     assign(tmp_merge_name, tmp_merge_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nSave a new phyloseq object\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(i)\n     tmp_tab <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_summary\")))\n     tmp_list <- tmp_tab[,1]\n     tmp_ps <- prune_taxa(tmp_list, tmp_get)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_lefse\"))\n     assign(tmp_name, tmp_ps)\n     tmp_ps@phy_tree <- NULL\n     tmp_ps <- prune_samples(sample_sums(tmp_ps) > 0, tmp_ps)\n     tmp_ps_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE,\n                            tip.label = taxa_names(tmp_ps))\n     tmp_ps <- merge_phyloseq(tmp_ps, sample_data, tmp_ps_tree)\n     assign(tmp_name, tmp_ps)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()"
  },
  {
    "objectID": "da.html#lefse-summaries-asvs-markers-1",
    "href": "da.html#lefse-summaries-asvs-markers-1",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "LEfSe Summaries (ASVs & Markers)",
    "text": "LEfSe Summaries (ASVs & Markers)\nASV summary data\n\n\n\n\nFULL\nArbitrary filtered\nPERfect filtered\nPIME filtered\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 5 | Significant results of LEfSe Analysis for FULL data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 6 | Significant results of LEfSe Analysis for Arbitrary filtered ASV data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 7 | Significant results of LEfSe Analysis for PERfect filtered data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 8 | Significant results of LEfSe Analysis for PIME data set.\n\n\n\n\n\n\n\n\n\n\n\nASV Visualizations\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_asv\")))\n     tmp_mt <- data.frame(marker_table(tmp_get))\n     tmp_mt  <-  tmp_mt %>% filter(ef_lda >= 3)\n     tmp_mt <- marker_table(tmp_mt)\n     tmp_res_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_asv_viz_trim\"))\n     assign(tmp_res_name, tmp_mt)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\nobjects(pattern = \"_lefse_asv_viz_trim\")\n\n\n\n\n\n\nFULL\nArbitrary filtered\nPERfect filtered\nPIME filtered\n\n\n\n\n\n\n\n\n(ITS) Figure 1 | Differentially abundant ASVs from LEfSe analysis for the FULL data set, trimmed to show those with LDA score ≥ 3.0.\n\n\n\n\n\n\n\n(ITS) Figure 2 | Differentially abundant ASVs from LEfSe analysis for the Arbitrary filtered data set, trimmed to show those with LDA score ≥ 3.0.\n\n\n\n\n\n\n\n(ITS) Figure 3 | Differentially abundant ASVs from LEfSe analysis for the PERfect filtered data set, trimmed to show those with LDA score ≥ 3.0.\n\n\n\n\n\n\n\n(ITS) Figure 4 | Differentially abundant ASVs from LEfSe analysis for the PIME filtered data set, trimmed to show those with LDA score ≥ 3.0.\n\n\n\n\nMarker summary data\n\nCodefor (i in samp_ps) {\n    tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_all\")))\n    tmp_mt <- data.frame(marker_table(tmp_get))\n    tmp_mt <- tmp_mt[, c(2:4,1)]\n    tmp_mt <- tmp_mt %>% dplyr::rename(c(\"group\" = 1, \"pval\" = 3))\n    tmp_mt$group <- stringr::str_replace(tmp_mt$group, \"^0$\", \"C0\")\n    tmp_mt$group <- stringr::str_replace(tmp_mt$group, \"^3$\", \"W3\")\n    tmp_mt$group <- stringr::str_replace(tmp_mt$group, \"^8$\", \"W8\")\n    tmp_mt <- tmp_mt %>% tibble::rownames_to_column(\"marker\")\n    tmp_mt <- tmp_mt %>% tidyr::separate(col = \"feature\", \n                                         into = c(\"Kingdom\", \"Phylum\", \"Class\", \n                                                  \"Order\", \"Family\", \"Genus\", \"ASV_ID\"), \n                                         sep = \"\\\\|\\\\w__\")\n    \n    tmp_mt$Kingdom <- gsub('k__', '', tmp_mt$Kingdom)\n    tmp_res_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_marker_final\"))\n    assign(tmp_res_name, tmp_mt)\n    rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\n\n\n\nFULL\nArbitrary filtered\nPERfect filtered\nPIME filtered\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 9 | Significant results of LEfSe MARKER Analysis for FULL data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 10 | Significant results of LEfSe MARKER Analysis for Arbitrary filtered ASV data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 11 | Significant results of LEfSe MARKER Analysis for PERfect filtered data set.\n\n\n\n\n\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 12 | Significant results of LEfSe MARKER Analysis for PIME data set.\n\n\n\n\n\n\n\n\n\n\n\nMarker visualizations\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_all\")))\n     tmp_mt <- data.frame(marker_table(tmp_get))\n     tmp_mt  <-  tmp_mt %>% filter(ef_lda >= 3.5)\n     tmp_mt <- marker_table(tmp_mt)\n     tmp_res_name <- purrr::map_chr(i, ~ paste0(., \"_lefse_all_viz_trim\"))\n     assign(tmp_res_name, tmp_mt)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\nobjects(pattern = \"_lefse_all_viz_trim\")\n\n\n\n\n\nFULL\nArbitrary filtered\nPERfect filtered\nPIME filtered\n\n\n\n\n\n\n\n\n(ITS) Figure 5 | Differentially abundant MARKER from LEfSe analysis for the FULL data set, trimmed to show those with LDA score ≥ 4.0.\n\n\n\n\n\n\n\n(ITS) Figure 6 | Differentially abundant MARKER from LEfSe analysis for the Arbitrary filtered data set, trimmed to show those with LDA score ≥ 4.0.\n\n\n\n\n\n\n\n(ITS) Figure 7 | Differentially abundant MARKER from LEfSe analysis for the PERfect filtered data set, trimmed to show those with LDA score ≥ 4.0.\n\n\n\n\n\n\n\n(ITS) Figure 8 | Differentially abundant MARKER from LEfSe analysis for the PIME filtered data set, trimmed to show those with LDA score ≥ 4.0."
  },
  {
    "objectID": "da.html#response-of-select-taxa-1",
    "href": "da.html#response-of-select-taxa-1",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Response of select taxa",
    "text": "Response of select taxa\nThe workflow here is basically a carbon copy of the workflow describe above. Presented here for posterity.\nClick here to see the ITS version of this workflow.\n\n\n\n\n## FIX ps object\nps <- its18_ps_perfect_rf_all\ntmp_tax1 <- data.frame(tax_table(ps))\ntmp_tax1$ASV_SEQ <- NULL\ntmp_rn <- row.names(tmp_tax1)\ntmp_tax <- data.frame(lapply(tmp_tax1, function(x) {gsub(\"\\\\(|)\", \"\", x)}))\nrow.names(tmp_tax) <- tmp_rn\nidentical(row.names(tmp_tax), row.names(tmp_tax1))\n#dplyr::filter(tmp_tax, Phylum == \"Acidobacteriota\")\nps_tax_new <- as.matrix(tmp_tax)\ntmp_ps <- phyloseq(otu_table(ps),\n                     tax_table(ps_tax_new),\n                     sample_data(ps))\nits_ps <- tmp_ps\nphyloseq::rank_names(its_ps)\ndata.frame(tax_table(its_ps))\n\n\nits_group_anova <-  run_test_multiple_groups(its_ps, group = \"TEMP\", \n                                             taxa_rank = \"all\", method = \"anova\")\nits_group_anova@marker_table\nmarker_table(its_group_anova)\n\n\nits_default_pht <- run_posthoc_test(its_ps, \n                                    group = \"TEMP\", method = \"tukey\", \n                                    transform = \"log10\")\n\n\n\n\n\nits_pht <- its_default_pht\nits18_pht_filt <- filter(data.frame(its_pht@result), pvalue <= \"0.05\")[!grepl(\"ASV\", filter(data.frame(its_pht@result), pvalue <= \"0.05\")$group_name),]\nits18_pht_filt <- its18_pht_filt[!grepl(\"[a-z]__$\", its18_pht_filt$group_name),]\nits18_pht_filt <- unique(its18_pht_filt$group_name)\nlength(its18_pht_filt)\n\nThere are 32 significantly different lineage markers.\nClick here to see a list of significantly different lineage markers.\n\n\n [1] \"k__Fungi\"                                                                                               \n [2] \"k__Fungi|p__Basidiomycota\"                                                                              \n [3] \"k__Fungi|p__Chytridiomycota\"                                                                            \n [4] \"k__Fungi|p__Glomeromycota\"                                                                              \n [5] \"k__Fungi|p__Mortierellomycota\"                                                                          \n [6] \"k__Fungi|p__Basidiomycota|c__Agaricomycetes\"                                                            \n [7] \"k__Fungi|p__Basidiomycota|c__Microbotryomycetes\"                                                        \n [8] \"k__Fungi|p__Glomeromycota|c__Glomeromycetes\"                                                            \n [9] \"k__Fungi|p__Mortierellomycota|c__Mortierellomycetes\"                                                    \n[10] \"k__Fungi|p__Rozellomycota|c__Rozellomycotina_cls_Incertae_sedis\"                                        \n[11] \"k__Fungi|p__Basidiomycota|c__Agaricomycetes|o__Agaricales\"                                              \n[12] \"k__Fungi|p__Basidiomycota|c__Microbotryomycetes|o__Sporidiobolales\"                                     \n[13] \"k__Fungi|p__Glomeromycota|c__Glomeromycetes|o__Glomerales\"                                              \n[14] \"k__Fungi|p__Mortierellomycota|c__Mortierellomycetes|o__Mortierellales\"                                  \n[15] \"k__Fungi|p__Rozellomycota|c__Rozellomycotina_cls_Incertae_sedis|o__GS11\"                                \n[16] \"k__Fungi|p__Ascomycota|c__Eurotiomycetes|o__Eurotiales|f__Trichocomaceae\"                               \n[17] \"k__Fungi|p__Ascomycota|c__Pezizomycetes|o__Pezizales|f__Pyronemataceae\"                                 \n[18] \"k__Fungi|p__Ascomycota|c__Saccharomycetes|o__Saccharomycetales|f__Metschnikowiaceae\"                    \n[19] \"k__Fungi|p__Ascomycota|c__Sordariomycetes|o__Xylariales|f__Microdochiaceae\"                             \n[20] \"k__Fungi|p__Basidiomycota|c__Agaricomycetes|o__Agaricales|f__Clavariaceae\"                              \n[21] \"k__Fungi|p__Basidiomycota|c__Agaricomycetes|o__Agaricales|f__Entolomataceae\"                            \n[22] \"k__Fungi|p__Basidiomycota|c__Microbotryomycetes|o__Sporidiobolales|f__Sporidiobolaceae\"                 \n[23] \"k__Fungi|p__Glomeromycota|c__Glomeromycetes|o__Glomerales|f__Glomeraceae\"                               \n[24] \"k__Fungi|p__Mortierellomycota|c__Mortierellomycetes|o__Mortierellales|f__Mortierellaceae\"               \n[25] \"k__Fungi|p__Ascomycota|c__Eurotiomycetes|o__Eurotiales|f__Trichocomaceae|g__Talaromyces\"                \n[26] \"k__Fungi|p__Ascomycota|c__Pezizomycetes|o__Pezizales|f__Pyronemataceae|g__Scutellinia\"                  \n[27] \"k__Fungi|p__Ascomycota|c__Saccharomycetes|o__Saccharomycetales|f__Metschnikowiaceae|g__Metschnikowia\"   \n[28] \"k__Fungi|p__Ascomycota|c__Sordariomycetes|o__Hypocreales|f__Nectriaceae|g__Fusarium\"                    \n[29] \"k__Fungi|p__Ascomycota|c__Sordariomycetes|o__Xylariales|f__Microdochiaceae|g__Idriella\"                 \n[30] \"k__Fungi|p__Basidiomycota|c__Microbotryomycetes|o__Sporidiobolales|f__Sporidiobolaceae|g__Rhodotorula\"  \n[31] \"k__Fungi|p__Glomeromycota|c__Glomeromycetes|o__Glomerales|f__Glomeraceae|g__Kamienskia\"                 \n[32] \"k__Fungi|p__Mortierellomycota|c__Mortierellomycetes|o__Mortierellales|f__Mortierellaceae|g__Mortierella\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(ITS) Figure 9 | The response of select taxa to two years of warming by +3°C and +8°C. Differences assessed for multiple-group pair-wise comparisons using ANOVA followed by Tukey HSD post hoc tests. PERfect filtered read count data was log10 transformed and normalized using total sum scaling (TSS). The centre line of each box plot represents the median, the lower and upper hinges represent the first and third quartiles and whiskers represent + 1.5 the interquartile range. Significant differences denoted by asterisks (* p ≤ 0.05, ** p ≤ 0.01, *** p ≤ 0.001, **** p ≤ 0.0001; ns = not significant)."
  },
  {
    "objectID": "da.html#main-steps",
    "href": "da.html#main-steps",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Main Steps",
    "text": "Main Steps\n1. View data\nLet’s start with the -d or --view-data file. This file needs to be an ASV by sample matrix of read counts. To simplify the visualization, we will use all ASVs represented by 100 or more total reads, including those identified as differentially abundant by the ISA and/or LEfSe.\n\nCodesamp_ps_all <- c(\"ssu18_ps_filt\", \"ssu18_ps_pime\", \"ssu18_ps_perfect\")\nsamp_ps <- c(\"ssu18_ps_filt\", \"ssu18_ps_pime\", \"ssu18_ps_perfect\")\nsamp_ps_org <- c(\"ssu18_ps_work\")\nsamp_ps_pime <- c(\"ssu18_ps_pime\")\nsamp_ps_other <- c(\"ssu18_ps_filt\", \"ssu18_ps_perfect\")\n\ntrim_val <- 100\nfor (i in samp_ps_pime) {\n     tmp_get <- get(i)\n     tmp_df <- prune_taxa(taxa_sums(tmp_get) > trim_val, tmp_get)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_trim\"))\n     assign(tmp_name, tmp_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\ntrim_val <- 300\nfor (i in samp_ps_other) {\n     tmp_get <- get(i)\n     tmp_df <- prune_taxa(taxa_sums(tmp_get) > trim_val, tmp_get)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_trim\"))\n     assign(tmp_name, tmp_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_trim\")\n\n\n\n\n\n\nfor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_trim\")))\n     tmp_df <- as.data.frame(t(otu_table(tmp_get)))\n     tmp_df <- tmp_df %>% rownames_to_column(\"Group\")\n     tmp_path <- file.path(\"files/anvio/ssu/\")\n     write.table(tmp_df, paste(tmp_path, i, \"/\", \"data.txt\", sep = \"\"),\n            quote = FALSE, sep = \"\\t\", row.names = FALSE)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\nOr export a table of transformed data.\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_trim\")))\n     tmp_trans <- transform_sample_counts(tmp_get, function(x) 1e5 * {x/sum(x)})\n     tmp_df <- as.data.frame(t(otu_table(tmp_trans)))\n     tmp_df <- tmp_df %>% rownames_to_column(\"Group\")\n     #tmp_name <- purrr::map_chr(i, ~ paste0(., \"_trim_tab\"))\n     #assign(tmp_name, tmp_df)\n     tmp_path <- file.path(\"files/anvio/ssu/\")\n     write.table(tmp_df, paste(tmp_path, i, \"/\", \"data_trans.txt\", sep = \"\"),\n            quote = FALSE, sep = \"\\t\", row.names = FALSE)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n2. Additional Layers for ASVs\nNext, we need some additional data about the ASVs to overlay on the visual. This can be anything however what I specifically want are the details of the ISA analysis, total reads, and lineage info. I warn you; this code will get ugly and I urge you to find a better way.\nStart with an ASV + lineage table for the ASVs in the new phyloseq object.\n\nCodefor (i in samp_ps) {\n     tmp_get_indval <- get(purrr::map_chr(i, ~ paste0(., \"_indval_final\")))\n     tmp_get_indval <- tmp_get_indval %>% dplyr::rename(\"Group\" = \"ASV_ID\") %>%\n                                          dplyr::rename(\"enrich_indval\" = \"group\") %>%\n                                          dplyr::rename(\"test_indval\" = \"indval\") %>%\n                                          dplyr::rename(\"pval_indval\" = \"pval\")\n     tmp_get_indval <- tmp_get_indval[,1:5]\n\n     tmp_get_lefse <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_final\")))\n     tmp_get_lefse <- tmp_get_lefse %>% dplyr::rename(\"Group\" = \"ASV_ID\") %>%\n                                          dplyr::rename(\"enrich_lefse\" = \"group\") %>%\n                                          dplyr::rename(\"test_lefse\" = \"lda\") %>%\n                                          dplyr::rename(\"pval_lefse\" = \"pval\")\n     tmp_get_lefse <- tmp_get_lefse[,1:4]\n     \n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_trim\")))\n     tmp_otu_df <- as.data.frame(t(otu_table(tmp_get)))\n     tmp_total <- cbind(tmp_otu_df, total_reads = rowSums(tmp_otu_df))\n     tmp_total <- rev(tmp_total)[1]\n     tmp_total <- tmp_total %>% tibble::rownames_to_column(\"Group\")\n     tmp_tax_df <- as.data.frame(tax_table(tmp_get))\n     tmp_tax_df$ASV_SEQ <- NULL\n     tmp_tax_df$ASV_ID <- NULL\n     \n     tmp_tax_df <- tmp_tax_df %>% tibble::rownames_to_column(\"Group\")\n     tmp_add_lay <- dplyr::left_join(tmp_tax_df, tmp_total, by = \"Group\") %>%\n                   dplyr::left_join(., tmp_get_indval, by = \"Group\") %>%\n                   dplyr::left_join(., tmp_get_lefse, by = \"Group\")\n     tmp_add_lay$ASV_ID <- tmp_add_lay$Group\n     tmp_add_lay <- tmp_add_lay[, c(1,16,8,12,9:11,13:15,2:7)]\n     tmp_path <- file.path(\"files/anvio/ssu/\")\n     write.table(tmp_add_lay, paste(tmp_path, i, \"/\", \"additional_layers.txt\", sep = \"\"),\n            quote = FALSE, sep = \"\\t\", row.names = FALSE, na = \"\")\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n3. Additional Views for Samples\nNow we want some general data about the samples to overlay on the visual. Again, this can be anything. How about a table of alpha diversity metrics? We actually have such a table that was generated way back up the road. Just need to fix the column names.\n\n\n\n\n\n\nNote\n\n\n\nAdded to profile.db with anvi-import-misc-data command & --target-data-table layers flag.\n\n\n\nCodemetadata_tab <- read.table(\"files/metadata/tables/metadata.txt\",\n                           header = TRUE)\n\ntmp_x <- readRDS(\"files/alpha/rdata/ssu18_ps_pime.rds\")\ndata.frame(sample_data(tmp_x))\nmetadata_tab[,c(2:5)] <- list(NULL)\nfor (i in samp_ps_all) {\n     tmp_get <- get(i)\n     tmp_df <- data.frame(sample_data(tmp_get))\n     tmp_df <- tmp_df[,c(2:9)]\n     tmp_df <- tmp_df %>% tibble::rownames_to_column(\"id\")\n     tmp_df <- tmp_df %>% dplyr::rename(\"no_asvs\" = \"Observed\")\n     tmp_rc <- data.frame(readcount(tmp_get))\n     tmp_rc <- tmp_rc %>% tibble::rownames_to_column(\"id\")\n     tmp_rc <- tmp_rc %>% dplyr::rename(\"no_reads\" = 2)\n     #identical(tmp_df$id, tmp_rc$id)\n     tmp_merge <- dplyr::left_join(tmp_df, tmp_rc)\n     tmp_merge <- tmp_merge[, c(1:6,10,7:9)]\n     tmp_final <- dplyr::left_join(tmp_merge, metadata_tab)\n     tmp_path <- file.path(\"files/anvio/ssu/\")\n     write.table(tmp_final, paste(tmp_path, i, \"/\", \"additional_views.txt\", sep = \"\"),\n            quote = FALSE, sep = \"\\t\", row.names = FALSE)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nrm(metadata_tab)\n\n\n4. Taxon rank abundance by sample\nTurned out this was a little tricky to figure out, but thanks to a little nifty block of code written by guoyanzhao on the phyloseq Issues forum, it was a piece of cake. The code can be altered to take any rank. See the post for an explanation.\nAnyway, the goal is to sum each taxon at some rank and present that as a bar chart for each sample in the visualization. Anvi’o has a specific format it needs where each row is a sample and each column is a taxon. Taxa names need the prefix t_<RANK>!. For example, t_class! should be added for Class rank.\n\n\n\n\n\n\nNote\n\n\n\nAdded to profile.db with anvi-import-misc-data command & --target-data-table layers flag.\n\n\n\nCodepick_rank <- \"Phylum\"\npick_rank_l <- \"phylum\"\nfor (i in samp_ps_all) {\n# Make the table\n    tmp_get <- get(i)\n    tmp_glom <- tax_glom(tmp_get, taxrank = pick_rank)\n    tmp_melt <- psmelt(tmp_glom)\n    tmp_melt[[pick_rank]] <- as.character(tmp_melt[[pick_rank]])\n    tmp_abund <- aggregate(Abundance ~ Sample + tmp_melt[[pick_rank]], tmp_melt, FUN = sum)\n    colnames(tmp_abund)[2] <- \"tax_rank\"\n    library(reshape2)\n    tmp_abund <- as.data.frame(reshape::cast(tmp_abund, Sample ~ tax_rank))\n    tmp_abund <- tibble::remove_rownames(tmp_abund)\n    tmp_abund <- tibble::column_to_rownames(tmp_abund, \"Sample\")\n# Reorder table column by sum\n    tmp_layers <- tmp_abund[,names(sort(colSums(tmp_abund), decreasing = TRUE))]\n# Add the prefix\n    tmp_layers <- tmp_layers %>% dplyr::rename_all(function(x) paste0(\"t_\", pick_rank_l,\"!\", x))\n    tmp_layers <- tibble::rownames_to_column (tmp_layers, \"taxon\")\n# save the table\n    tmp_path <- file.path(\"files/anvio/ssu/\")\n    write.table(tmp_layers, paste(tmp_path, i, \"/\", \"tax_layers.txt\", sep = \"\"),\n            quote = FALSE, sep = \"\\t\", row.names = FALSE)\n    rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n5. Construct Dendrograms\nThe last piece we need is to generate dendrograms that order the ASVs by their distribution in the samples and the samples by their ASV composition. For this task we will use anvi’o.\nanvi-matrix-to-newick data.txt --distance euclidean \\\n                               --linkage ward \\\n                               -o asv.tre\nanvi-matrix-to-newick data.txt --distance euclidean \\\n                               --linkage ward \\\n                               -o sample.tre \\\n                               --transpose\nThe first command reads the view data we generated above and uses Euclidean distance and Ward linkage for hierarchical clustering of the ASVs. The second command transposes the view data table and then does the same for the samples. There are several distance metrics and linkage methods available. See the help menu for the command by typing anvi-matrix-to-newick -h. Boom.\n\n\n\n\n\n\n\n\n\nAlternatively, we can generate dendrograms using phyloseq::distance and hclust.\n\nCodepick_dist <- \"bray\"\npick_clust <- \"complete\"\n\nfor (i in samp_ps) {\n# Make the table\n    tmp_get <- get(i)\n    tmp_dist <- phyloseq::distance(physeq = tmp_get, method = pick_dist, type = \"sample\")\n    tmp_dend <- hclust(tmp_dist, method = pick_clust)\n    plot(tmp_dend, hang = -1)\n    tmp_tree <- as.phylo(tmp_dend) \n    \n    tmp_path <- file.path(\"files/anvio/ssu/\")\n    write.tree(phy = tmp_tree, file = paste(tmp_path, i, \"/\", \"sample_\", pick_dist, \"_\", pick_clust, \".tre\", sep = \"\"))\n    rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nCodepick_dist_asv <- \"euclidean\"\npick_clust_asv <- \"ward\"\n\nfor (i in samp_ps) {\n# Make the table\n    tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_trim\")))\n    tmp_dist <- phyloseq::distance(physeq = tmp_get, method = pick_dist_asv, type = \"taxa\")\n    tmp_dend <- hclust(tmp_dist, method = pick_clust_asv)\n    plot(tmp_dend, hang = -1)\n    tmp_tree <- as.phylo(tmp_dend) \n    \n    tmp_path <- file.path(\"files/anvio/ssu/\")\n    write.tree(phy = tmp_tree, file = paste(tmp_path, i, \"/\", \"asv_\", pick_dist_asv, \"_\", pick_clust_asv, \".tre\", sep = \"\"))\n    rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe file asv.tre is loaded with anvi-interactive command & the --tree flag.\n\n\nThe ASV tree is fine as is, but the sample tree needs a special format. Specifically, the tree needs to be in a three column, tab delimited, table. This way you can add multiple orderings to the same file and view them all in the interactive. The table needs to be in this format:\n\n\n\n\n\n\n\nitem_name\ndata_type\ndata_value\n\n\n\ntree_1\nnewick\n((P01_D00_010_W8A:0.0250122,P05_D00_010_W8C:0.02,..\n\n\ntree_2\nnewick\n((((((((OTU14195:0.0712585,OTU13230:0.0712585)0:,…\n\n\n(…)\n(…)\n(…)\n\n\n\nThis is easy to do by hand, but I really need the practice so I will do it in R. Anvi’o is very particular about formatting. For example, if this file ends with a blank line, which it will because when anvi’o made the initial dendrogram it add a new line. We need to get rid of that or we get an error when trying to import the table.\n\n\n\n\n\n\nNote\n\n\n\nThe file sample.tre is added to the profile.db with anvi-import-misc-data command & --target-data-table layer_orders flag.\n\n\n\nCodefor (i in samp_ps) {\n      tmp_path <- file.path(\"files/anvio/ssu/\")\n      tmp_tree <- read_file(paste(tmp_path, i, \"/\", \"sample.tre\", sep = \"\"))\n      tmp_tree <- gsub(\"[\\r\\n]\", \"\", tmp_tree)\n      tmp_item <- c(\"bray_complete\")\n      tmp_type <- c(\"newick\")\n      tmp_df <- c(tmp_tree)\n      tmp_tab <- data.frame(tmp_item, tmp_type, tmp_df)\n      library(janitor)\n      tmp_tab %>% remove_empty(\"rows\")\n      colnames(tmp_tab) <- c(\"item_name\",   \"data_type\",    \"data_value\")\n      write.table(tmp_tab, paste(tmp_path, i, \"/\", \"sample.tre\", sep = \"\"),\n            sep = \"\\t\", quote = FALSE, row.names = FALSE, na = \"\")\n      rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nCode# FOR TRANSFORMED DATA\nfor (i in samp_ps) {\n      tmp_path <- file.path(\"files/anvio/ssu/\")\n      tmp_tree <- read_file(paste(tmp_path, i, \"/\",\"sample_trans.tre\", sep = \"\"))\n      tmp_tree <- gsub(\"[\\r\\n]\", \"\", tmp_tree)\n      tmp_item <- c(\"bray_complete\")\n      tmp_type <- c(\"newick\")\n      tmp_df <- c(tmp_tree)\n      tmp_tab <- data.frame(tmp_item, tmp_type, tmp_df)\n      library(janitor)\n      tmp_tab %>% remove_empty(\"rows\")\n      colnames(tmp_tab) <- c(\"item_name\",   \"data_type\",    \"data_value\")\n      write.table(tmp_tab, paste(tmp_path, i, \"/\", \"sample_trans.tre\", sep = \"\"),\n            sep = \"\\t\", quote = FALSE, row.names = FALSE, na = \"\")\n      rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()\n\n\n\nCode# FOR HCLUST TREE\nfor (i in samp_ps) {\n      tmp_path <- file.path(\"files/anvio/ssu/\")\n      tmp_tree <- read_file(paste(tmp_path, i, \"/\", \"sample_\", pick_dist, \"_\", pick_clust, \".tre\", sep = \"\"))\n      tmp_tree <- gsub(\"[\\r\\n]\", \"\", tmp_tree)\n      tmp_item <- c(paste(pick_dist, \"_\", pick_clust, \"_hclust\", sep = \"\"))\n      tmp_type <- c(\"newick\")\n      tmp_df <- c(tmp_tree)\n      tmp_tab <- data.frame(tmp_item, tmp_type, tmp_df)\n      library(janitor)\n      tmp_tab %>% remove_empty(\"rows\")\n      colnames(tmp_tab) <- c(\"item_name\",   \"data_type\",    \"data_value\")\n      write.table(tmp_tab, paste(tmp_path, i, \"/\", \"sample_\", pick_dist, \"_\", pick_clust, \".tre\", sep = \"\"),\n            sep = \"\\t\", quote = FALSE, row.names = FALSE, na = \"\")\n      rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n6. Make a fasta file\nWe don’t need to add a fasta file, but it is a nice way to keep everything in one place. Plus, you can do BLAST searches directly in the interface by right clicking on the ASV of interest, so it is nice to have the sequences.\n\n\n\n\n\n\nNote\n\n\n\nLoaded with anvi-interactive command & --fasta-file flag.\n\n\n\nCodefor (i in samp_ps) {\n       tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_trim\")))\n       tmp_tab <- tax_table(tmp_get)\n       tmp_tab <- tmp_tab[, 7]\n       tmp_df <- data.frame(row.names(tmp_tab), tmp_tab)\n       colnames(tmp_df) <- c(\"ASV_ID\", \"ASV_SEQ\")\n       tmp_df$ASV_ID <- sub(\"^\", \">\", tmp_df$ASV_ID)\n       tmp_path <- file.path(\"files/anvio/ssu/\")\n\n       write.table(tmp_df, paste(tmp_path, i, \"/\", i, \".fasta\", sep = \"\"),\n            sep = \"\\n\", col.names = FALSE, row.names = FALSE,\n            quote = FALSE, fileEncoding = \"UTF-8\")\n       rm(list = ls(pattern = \"tmp_\"))\n}"
  },
  {
    "objectID": "da.html#building-the-profile-database",
    "href": "da.html#building-the-profile-database",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Building the Profile Database",
    "text": "Building the Profile Database\nTime to put all of these pieces together. This gets a little tricky since we do not have a database to start with because some of these files can be loaded directly in the interface but some need to be added to a database. When we fire up the interactive in --manual mode, we must give anvi’o the name of a database and it will create that database for us. Then we can shut down the interactive, add the necessary data files, and start back up.\nanvi-interactive --view-data data.txt \\\n                 --tree asv.tre \\\n                 --additional-layers additional_layers.txt \\\n                 --profile-db profile.db \\\n                 --manual\nNow we have a new profile database that we can add the sample metadata (additional_layers.txt) and the sample dendrogram (sample.tre) using the command anvi-import-misc-data. These commands add the table to the new profile.db. First, kill the interactive.\nanvi-import-misc-data additional_views.txt \\\n                      --pan-or-profile-db profile.db \\\n                      --target-data-table layers\nanvi-import-misc-data sample.tre \\\n                      --pan-or-profile-db profile.db \\\n                      --target-data-table layer_orders\nOne last this is to get the table with the taxonomy total by sample (tax_layers.txt) into the profile database. We will run the same command we just used.\nanvi-import-misc-data tax_layers.txt \\\n                      --pan-or-profile-db profile.db \\\n                      --target-data-table layers\nIn fact, we could just as easily append the taxonomy total data onto the additional_layers.txt and import in one command. But we didn’t."
  },
  {
    "objectID": "da.html#interactive-interface",
    "href": "da.html#interactive-interface",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Interactive Interface",
    "text": "Interactive Interface\nWith a populated database in hand, we can now begin modifying the visual by running the interactive command again.\nanvi-interactive --view-data data.txt \\\n                 --tree asv.tre \\\n                 --additional-layers additional_layers.txt \\\n                 --profile-db profile.db\n                 --fasta-file anvio.fasta \\\n                 --manual\n\n\n\n\n\n\nClick here for the ITS version of this workflow.\n\nThe ITS version of the anvi’o workflow is basically a carbon copy of the workflow presented above. It is included here for posterity.\nVisualizing DA ASVs in Anvi’o\nNext, we will combine the results of the ISA and LEfSe analyses with the distribution of ASVs across each sample. We are going to do the analysis in anvi’o—an advanced analysis and visualization platform for ’omics data (Eren et al. 2015)—using the anvi-interactive command. Anvi’o likes databases but it also understands that sometimes you do not have a database. So it offers a manual mode. If you type this command you can have a look at the relevant pieces we need for the visualization, specifically those under the headings MANUAL INPUTS and ADDITIONAL STUFF.\nanvi-interactive -h\nMANUAL INPUTS:\n  Mandatory input parameters to start the interactive interface without\n  anvi'o databases.\n\n--manual-mode           We need this flag to run anvi'o in an ad hoc\n                        manner, i.e., no database.\n-f FASTA, --fasta-file FASTA\n                        A FASTA-formatted input file. This is sort of\n                        optional\n-d VIEW_DATA, --view-data VIEW_DATA\n                        A TAB-delimited file for view data. This is the ASV\n                        by sample matrix. We need this\n-t NEWICK, --tree NEWICK\n                        NEWICK formatted tree structure. How the ASVs are\n                        ordered in our case.\nADDITIONAL STUFF:\n  Parameters to provide additional layers, views, or layer data.\n\n-V ADDITIONAL_VIEW, --additional-view ADDITIONAL_VIEW\n                        A TAB-delimited file for an additional view to be used\n                        in the interface. This file should contain all split\n                        names, and values for each of them in all samples.\n                        Each column in this file must correspond to a sample\n                        name. Content of this file will be called 'user_view',\n                        which will be available as a new item in the 'views'\n                        combo box in the interface\n-A ADDITIONAL_LAYERS, --additional-layers ADDITIONAL_LAYERS\n                        A TAB-delimited file for additional layer info. In\n                        our case this is info about each ASV. The first column\n                        of the file must be the ASV names, and\n                        the remaining columns should be unique attributes.\nThere are also a few files we generate that cannot be loaded directly. So, in addition to the files that can be loaded when running the interactive, we also have files that must be added to the database created by anvi’o.\nHere is a nice tutorial on Working with anvi’o additional data tables. A lot of what we need is covered in this tutorial. To get the most out the visualization we need to create a few files to give anvi’o when we fire up the interactive interface.\n\nView data: in our case, a sample by ASV abundance matrix.\nAdditional info about each ASV.\nAdditional info about each sample.\nTaxa abundance data for each sample at some rank.\nDendrograms ordering the ASVs and samples (based on view data).\nFasta file of all ASVs in the analysis.\nMain steps\n1. View data\nLet’s start with the -d or --view-data file. This file needs to be an ASV by sample matrix of read counts. To simplify the visualization, we will use all ASVs represented by 100 or more total reads, including those identified as differentially abundant by the ISA and/or LEfSe.\n\nCodesamp_ps_all <- c(\"its18_ps_filt\", \"its18_ps_pime\", \"its18_ps_perfect\", \n                 \"its18_ps_filt_otu\", \"its18_ps_pime_otu\", \"its18_ps_perfect_otu\", \n                 \"its18_ps_work\", \"its18_ps_work_otu\")\nsamp_ps <- c(\"its18_ps_filt\", \"its18_ps_pime\", \"its18_ps_perfect\", \n             \"its18_ps_filt_otu\", \"its18_ps_pime_otu\", \"its18_ps_perfect_otu\")\nsamp_ps_org <- c(\"its18_ps_work\", \"its18_ps_work_otu\")\nsamp_ps_pime <- c(\"its18_ps_pime\", \"its18_ps_pime_otu\")\nsamp_ps_other <- c(\"its18_ps_filt\", \"its18_ps_perfect\", \n                   \"its18_ps_filt_otu\", \"its18_ps_perfect_otu\")\n\ntrim_val <- 50\nfor (i in samp_ps_pime) {\n     tmp_get <- get(i)\n     tmp_df <- prune_taxa(taxa_sums(tmp_get) > trim_val, tmp_get)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_trim\"))\n     assign(tmp_name, tmp_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\ntrim_val <- 300\nfor (i in samp_ps_other) {\n     tmp_get <- get(i)\n     tmp_df <- prune_taxa(taxa_sums(tmp_get) > trim_val, tmp_get)\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_trim\"))\n     assign(tmp_name, tmp_df)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_trim\")\nits18_ps_filt_trim\nits18_ps_pime_trim\nits18_ps_perfect_trim\nits18_ps_filt_otu_trim\nits18_ps_pime_otu_trim\nits18_ps_perfect_otu_trim\n\n\n\n\n\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_trim\")))\n     tmp_df <- as.data.frame(t(otu_table(tmp_get)))\n     tmp_df <- tmp_df %>% rownames_to_column(\"Group\")\n     tmp_path <- file.path(\"files/anvio/its/\")\n     write.table(tmp_df, paste(tmp_path, i, \"/\", \"data.txt\", sep = \"\"),\n            quote = FALSE, sep = \"\\t\", row.names = FALSE)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()\n\n\nOr export a table of transformed data.\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_trim\")))\n     tmp_trans <- transform_sample_counts(tmp_get, function(x) 1e5 * {x/sum(x)})\n     tmp_df <- as.data.frame(t(otu_table(tmp_trans)))\n     tmp_df <- tmp_df %>% rownames_to_column(\"Group\")\n     #tmp_name <- purrr::map_chr(i, ~ paste0(., \"_trim_tab\"))\n     #assign(tmp_name, tmp_df)\n     tmp_path <- file.path(\"files/anvio/its/\")\n     write.table(tmp_df, paste(tmp_path, i, \"/\", \"data_trans.txt\", sep = \"\"),\n            quote = FALSE, sep = \"\\t\", row.names = FALSE)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()\n\n\n2. Additional Layers for ASVs\nNext, we need some additional data about the ASVs to overlay on the visual. This can be anything however what I specifically want are the details of the ISA analysis, total reads, and lineage info. I warn you; this code will get ugly and I urge you to find a better way.\nStart with an ASV + lineage table for the ASVs in the new phyloseq object.\n\nCodefor (i in samp_ps) {\n     tmp_get_indval <- get(purrr::map_chr(i, ~ paste0(., \"_indval_final\")))\n     tmp_get_indval <- tmp_get_indval %>% dplyr::rename(\"Group\" = \"ASV_ID\") %>%\n                                          dplyr::rename(\"enrich_indval\" = \"group\") %>%\n                                          dplyr::rename(\"test_indval\" = \"indval\") %>%\n                                          dplyr::rename(\"pval_indval\" = \"pval\")\n     tmp_get_indval <- tmp_get_indval[,1:5]\n\n     tmp_get_lefse <- get(purrr::map_chr(i, ~ paste0(., \"_lefse_final\")))\n     tmp_get_lefse <- tmp_get_lefse %>% dplyr::rename(\"Group\" = \"ASV_ID\") %>%\n                                          dplyr::rename(\"enrich_lefse\" = \"group\") %>%\n                                          dplyr::rename(\"test_lefse\" = \"lda\") %>%\n                                          dplyr::rename(\"pval_lefse\" = \"pval\")\n     tmp_get_lefse <- tmp_get_lefse[,1:4]\n     \n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_trim\")))\n     tmp_otu_df <- as.data.frame(t(otu_table(tmp_get)))\n     tmp_total <- cbind(tmp_otu_df, total_reads = rowSums(tmp_otu_df))\n     tmp_total <- rev(tmp_total)[1]\n     tmp_total <- tmp_total %>% tibble::rownames_to_column(\"Group\")\n     tmp_tax_df <- as.data.frame(tax_table(tmp_get))\n     tmp_tax_df$ASV_SEQ <- NULL\n     tmp_tax_df$ASV_ID <- NULL\n     \n     tmp_tax_df <- tmp_tax_df %>% tibble::rownames_to_column(\"Group\")\n     tmp_add_lay <- dplyr::left_join(tmp_tax_df, tmp_total, by = \"Group\") %>%\n                   dplyr::left_join(., tmp_get_indval, by = \"Group\") %>%\n                   dplyr::left_join(., tmp_get_lefse, by = \"Group\")\n     tmp_add_lay$ASV_ID <- tmp_add_lay$Group\n     tmp_add_lay <- tmp_add_lay[, c(1,16,8,12,9:11,13:15,2:7)]\n     tmp_path <- file.path(\"files/anvio/its/\")\n     write.table(tmp_add_lay, paste(tmp_path, i, \"/\", \"additional_layers.txt\", sep = \"\"),\n            quote = FALSE, sep = \"\\t\", row.names = FALSE, na = \"\")\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n3. Additional Views for Samples\nNow we want some general data about the samples to overlay on the visual. Again, this can be anything. How about a table of alpha diversity metrics? We actually have such a table that was generated way back up the road. Just need to fix the column names.\n\n\n\n\n\n\nNote\n\n\n\nAdded to profile.db with anvi-import-misc-data command & --target-data-table layers flag.\n\n\n\nCodemetadata_tab <- read.table(\"files/metadata/tables/metadata.txt\",\n                           header = TRUE)\n\ntmp_x <- readRDS(\"files/alpha/rdata/its18_ps_pime.rds\")\ndata.frame(sample_data(tmp_x))\nmetadata_tab[,c(2:5)] <- list(NULL)\nfor (i in samp_ps_all) {\n     tmp_get <- get(i)\n     tmp_df <- data.frame(sample_data(tmp_get))\n     tmp_df <- tmp_df[,c(2:9)]\n     tmp_df <- tmp_df %>% tibble::rownames_to_column(\"id\")\n     tmp_df <- tmp_df %>% dplyr::rename(\"no_asvs\" = \"Observed\")\n     tmp_rc <- data.frame(readcount(tmp_get))\n     tmp_rc <- tmp_rc %>% tibble::rownames_to_column(\"id\")\n     tmp_rc <- tmp_rc %>% dplyr::rename(\"no_reads\" = 2)\n     #identical(tmp_df$id, tmp_rc$id)\n     tmp_merge <- dplyr::left_join(tmp_df, tmp_rc)\n     tmp_merge <- tmp_merge[, c(1:6,10,7:9)]\n     tmp_final <- dplyr::left_join(tmp_merge, metadata_tab)\n     tmp_path <- file.path(\"files/anvio/its/\")\n     write.table(tmp_final, paste(tmp_path, i, \"/\", \"additional_views.txt\", sep = \"\"),\n            quote = FALSE, sep = \"\\t\", row.names = FALSE)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nrm(metadata_tab)\n\n\n4. Taxon rank abundance by sample\nTurned out this was a little tricky to figure out, but thanks to a little nifty block of code written by guoyanzhao on the phyloseq Iitses forum, it was a piece of cake. The code can be altered to take any rank. See the post for an explanation.\nAnyway, the goal is to sum each taxon at some rank and present that as a bar chart for each sample in the visualization. Anvi’o has a specific format it needs where each row is a sample and each column is a taxon. Taxa names need the prefix t_<RANK>!. For example, t_class! should be added for Class rank.\n\n\n\n\n\n\nNote\n\n\n\nAdded to profile.db with anvi-import-misc-data command & --target-data-table layers flag.\n\n\n\nCodepick_rank <- \"Order\"\npick_rank_l <- \"order\"\nfor (i in samp_ps_all) {\n# Make the table\n    tmp_get <- get(i)\n    tmp_glom <- tax_glom(tmp_get, taxrank = pick_rank)\n    tmp_melt <- psmelt(tmp_glom)\n    tmp_melt[[pick_rank]] <- as.character(tmp_melt[[pick_rank]])\n    tmp_abund <- aggregate(Abundance ~ Sample + tmp_melt[[pick_rank]], tmp_melt, FUN = sum)\n    colnames(tmp_abund)[2] <- \"tax_rank\"\n    library(reshape2)\n    tmp_abund <- as.data.frame(reshape::cast(tmp_abund, Sample ~ tax_rank))\n    tmp_abund <- tibble::remove_rownames(tmp_abund)\n    tmp_abund <- tibble::column_to_rownames(tmp_abund, \"Sample\")\n# Reorder table column by sum\n    tmp_layers <- tmp_abund[,names(sort(colSums(tmp_abund), decreasing = TRUE))]\n# Add the prefix\n    tmp_layers <- tmp_layers %>% dplyr::rename_all(function(x) paste0(\"t_\", pick_rank_l,\"!\", x))\n    tmp_layers <- tibble::rownames_to_column (tmp_layers, \"taxon\")\n# save the table\n    tmp_path <- file.path(\"files/anvio/its/\")\n    write.table(tmp_layers, paste(tmp_path, i, \"/\", \"tax_layers.txt\", sep = \"\"),\n            quote = FALSE, sep = \"\\t\", row.names = FALSE)\n    rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n5. Construct Dendrograms\nThe last piece we need is to generate dendrograms that order the ASVs by their distribution in the samples and the samples by their ASV composition. For this task we will use anvi’o.\nanvi-matrix-to-newick data.txt --distance euclidean \\\n                               --linkage ward \\\n                               -o asv.tre\nanvi-matrix-to-newick data.txt --distance euclidean \\\n                               --linkage ward \\\n                               -o sample.tre \\\n                               --transpose\nThe first command reads the view data we generated above and uses Euclidean distance and Ward linkage for hierarchical clustering of the ASVs. The second command transposes the view data table and then does the same for the samples. There are several distance metrics and linkage methods available. See the help menu for the command by typing anvi-matrix-to-newick -h. Boom.\n\n\n\n\n\n\n\n\n\nAlternatively, we can generate dendrograms using phyloseq::distance and hclust.\n\nCodepick_dist <- \"bray\"\npick_clust <- \"complete\"\n\nfor (i in samp_ps) {\n# Make the table\n    tmp_get <- get(i)\n    tmp_dist <- phyloseq::distance(physeq = tmp_get, method = pick_dist, type = \"sample\")\n    tmp_dend <- hclust(tmp_dist, method = pick_clust)\n    plot(tmp_dend, hang = -1)\n    tmp_tree <- as.phylo(tmp_dend) \n    \n    tmp_path <- file.path(\"files/anvio/its/\")\n    write.tree(phy = tmp_tree, file = paste(tmp_path, i, \"/\", \"sample_\", pick_dist, \"_\", pick_clust, \".tre\", sep = \"\"))\n    rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nCodepick_dist_asv <- \"euclidean\"\npick_clust_asv <- \"ward\"\n\nfor (i in samp_ps) {\n# Make the table\n    tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_trim\")))\n    tmp_dist <- phyloseq::distance(physeq = tmp_get, method = pick_dist_asv, type = \"taxa\")\n    tmp_dend <- hclust(tmp_dist, method = pick_clust_asv)\n    plot(tmp_dend, hang = -1)\n    tmp_tree <- as.phylo(tmp_dend) \n    \n    tmp_path <- file.path(\"files/anvio/its/\")\n    write.tree(phy = tmp_tree, file = paste(tmp_path, i, \"/\", \"asv_\", pick_dist_asv, \"_\", pick_clust_asv, \".tre\", sep = \"\"))\n    rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe file asv.tre is loaded with anvi-interactive command & the --tree flag.\n\n\nThe ASV tree is fine as is, but the sample tree needs a special format. Specifically, the tree needs to be in a three column, tab delimited, table. This way you can add multiple orderings to the same file and view them all in the interactive. The table needs to be in this format:\n\n\n\n\n\n\n\nitem_name\ndata_type\ndata_value\n\n\n\ntree_1\nnewick\n((P01_D00_010_W8A:0.0250122,P05_D00_010_W8C:0.02,..\n\n\ntree_2\nnewick\n((((((((OTU14195:0.0712585,OTU13230:0.0712585)0:,…\n\n\n(…)\n(…)\n(…)\n\n\n\nThis is easy to do by hand, but I really need the practice so I will do it in R. Anvi’o is very particular about formatting. For example, if this file ends with a blank line, which it will because when anvi’o made the initial dendrogram it add a new line. We need to get rid of that or we get an error when trying to import the table.\n\n\n\n\n\n\nNote\n\n\n\nThe file sample.tre is added to the profile.db with anvi-import-misc-data command & --target-data-table layer_orders flag.\n\n\n\nCodefor (i in samp_ps) {\n      tmp_path <- file.path(\"files/anvio/its/\")\n      tmp_tree <- read_file(paste(tmp_path, i, \"/\", \"sample.tre\", sep = \"\"))\n      tmp_tree <- gsub(\"[\\r\\n]\", \"\", tmp_tree)\n      tmp_item <- c(\"bray_complete\")\n      tmp_type <- c(\"newick\")\n      tmp_df <- c(tmp_tree)\n      tmp_tab <- data.frame(tmp_item, tmp_type, tmp_df)\n      library(janitor)\n      tmp_tab %>% remove_empty(\"rows\")\n      colnames(tmp_tab) <- c(\"item_name\",   \"data_type\",    \"data_value\")\n      write.table(tmp_tab, paste(tmp_path, i, \"/\", \"sample.tre\", sep = \"\"),\n            sep = \"\\t\", quote = FALSE, row.names = FALSE, na = \"\")\n      rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nCode# FOR TRANSFORMED DATA\nfor (i in samp_ps) {\n      tmp_path <- file.path(\"files/anvio/its/\")\n      tmp_tree <- read_file(paste(tmp_path, i, \"/\",\"sample_trans.tre\", sep = \"\"))\n      tmp_tree <- gsub(\"[\\r\\n]\", \"\", tmp_tree)\n      tmp_item <- c(\"bray_complete\")\n      tmp_type <- c(\"newick\")\n      tmp_df <- c(tmp_tree)\n      tmp_tab <- data.frame(tmp_item, tmp_type, tmp_df)\n      library(janitor)\n      tmp_tab %>% remove_empty(\"rows\")\n      colnames(tmp_tab) <- c(\"item_name\",   \"data_type\",    \"data_value\")\n      write.table(tmp_tab, paste(tmp_path, i, \"/\", \"sample_trans.tre\", sep = \"\"),\n            sep = \"\\t\", quote = FALSE, row.names = FALSE, na = \"\")\n      rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()\n\n\n\nCode# FOR HCLUST TREE\nfor (i in samp_ps) {\n      tmp_path <- file.path(\"files/anvio/its/\")\n      tmp_tree <- read_file(paste(tmp_path, i, \"/\", \"sample_\", pick_dist, \"_\", pick_clust, \".tre\", sep = \"\"))\n      tmp_tree <- gsub(\"[\\r\\n]\", \"\", tmp_tree)\n      tmp_item <- c(paste(pick_dist, \"_\", pick_clust, \"_hclust\", sep = \"\"))\n      tmp_type <- c(\"newick\")\n      tmp_df <- c(tmp_tree)\n      tmp_tab <- data.frame(tmp_item, tmp_type, tmp_df)\n      library(janitor)\n      tmp_tab %>% remove_empty(\"rows\")\n      colnames(tmp_tab) <- c(\"item_name\",   \"data_type\",    \"data_value\")\n      write.table(tmp_tab, paste(tmp_path, i, \"/\", \"sample_\", pick_dist, \"_\", pick_clust, \".tre\", sep = \"\"),\n            sep = \"\\t\", quote = FALSE, row.names = FALSE, na = \"\")\n      rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n6. Make a fasta file\nWe don’t need to add a fasta file, but it is a nice way to keep everything in one place. Plus, you can do BLAST searches directly in the interface by right clicking on the ASV of interest, so it is nice to have the sequences.\n\n\n\n\n\n\nNote\n\n\n\nLoaded with anvi-interactive command & --fasta-file flag.\n\n\n\nCodefor (i in samp_ps) {\n       tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_trim\")))\n       tmp_tab <- tax_table(tmp_get)\n       tmp_tab <- tmp_tab[, 7]\n       tmp_df <- data.frame(row.names(tmp_tab), tmp_tab)\n       colnames(tmp_df) <- c(\"ASV_ID\", \"ASV_SEQ\")\n       tmp_df$ASV_ID <- sub(\"^\", \">\", tmp_df$ASV_ID)\n       tmp_path <- file.path(\"files/anvio/its/\")\n\n       write.table(tmp_df, paste(tmp_path, i, \"/\", i, \".fasta\", sep = \"\"),\n            sep = \"\\n\", col.names = FALSE, row.names = FALSE,\n            quote = FALSE, fileEncoding = \"UTF-8\")\n       rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nBuilding the Profile Database\nTime to put all of these pieces together. This gets a little tricky since we do not have a database to start with because some of these files can be loaded directly in the interface but some need to be added to a database. When we fire up the interactive in --manual mode, we must give anvi’o the name of a database and it will create that database for us. Then we can shut down the interactive, add the necessary data files, and start back up.\nanvi-interactive --view-data data.txt \\\n                 --tree asv.tre \\\n                 --additional-layers additional_layers.txt \\\n                 --profile-db profile.db \\\n                 --manual\nNow we have a new profile database that we can add the sample metadata (additional_layers.txt) and the sample dendrogram (sample.tre) using the command anvi-import-misc-data. These commands add the table to the new profile.db. First, kill the interactive.\nanvi-import-misc-data additional_views.txt \\\n                      --pan-or-profile-db profile.db \\\n                      --target-data-table layers\nanvi-import-misc-data sample.tre \\\n                      --pan-or-profile-db profile.db \\\n                      --target-data-table layer_orders\nOne last this is to get the table with the taxonomy total by sample (tax_layers.txt) into the profile database. We will run the same command we just used.\nanvi-import-misc-data tax_layers.txt \\\n                      --pan-or-profile-db profile.db \\\n                      --target-data-table layers\nIn fact, we could just as easily append the taxonomy total data onto the additional_layers.txt and import in one command. But we didn’t.\nInteractive Interface\nWith a populated database in hand, we can now begin modifying the visual by running the interactive command again.\nanvi-interactive --view-data data.txt \\\n                 --tree asv.tre \\\n                 --additional-layers additional_layers.txt \\\n                 --profile-db profile.db\n                 --fasta-file anvio.fasta \\\n                 --manual"
  },
  {
    "objectID": "da.html#data-availability",
    "href": "da.html#data-availability",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Data Availability",
    "text": "Data Availability\nData generated in this workflow and the Rdata needed to run the workflow can be accessed on figshare at 10.25573/data.16828249."
  },
  {
    "objectID": "da.html#last-updated-on",
    "href": "da.html#last-updated-on",
    "title": "7. Differentially Abundant ASVs & Taxa",
    "section": "Last updated on",
    "text": "Last updated on\n\n\n[1] \"2022-06-21 13:25:43 EST\""
  },
  {
    "objectID": "dada2.html",
    "href": "dada2.html",
    "title": "1. DADA2 Workflow",
    "section": "",
    "text": "Click here for setup information.\n\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(919191)\n#pacman::p_depends(ape, local = TRUE)  \n#pacman::p_depends_reverse(ape, local = TRUE)  \nlibrary(dada2); packageVersion(\"dada2\")\nlibrary(ShortRead); packageVersion(\"ShortRead\")\nlibrary(ggplot2); packageVersion(\"ggplot2\")\npacman::p_load(tidyverse, gridExtra, grid, \n               formatR, reactable, downloadthis,\n               captioner, sparkline, \n               install = FALSE, update = FALSE)\n\noptions(scipen=999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\nremove(list = ls())\nsource(file.path(\"assets\", \"functions.R\"))"
  },
  {
    "objectID": "dada2.html#workflow-input",
    "href": "dada2.html#workflow-input",
    "title": "1. DADA2 Workflow",
    "section": "Workflow Input",
    "text": "Workflow Input\nAll files needed to run this workflow can be downloaded from figshare.\n\n\n\nNOTE: The reverse reads for 16S rRNA data set are unusable (see below), therefore this workflow deals with forward reads only.\n\nWe will screen the forward reads for reverse primers and show the quality plots of the reverse reads at the beginning of the workflow. After that, reverse reads are not used."
  },
  {
    "objectID": "dada2.html#overview",
    "href": "dada2.html#overview",
    "title": "1. DADA2 Workflow",
    "section": "Overview",
    "text": "Overview\nSequence Files & Samples\nWe sequenced a total of 15 samples collected from 10 different plots at a single depth in each plot.\n\n\n\n(16S rRNA) Table 1 | Sample data & associated sequencing information.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\nWorkflow\nThe workflow consists of the following steps:\n\n\n\n\n\n\n\nStep\nCommand\nWhat we’re doing\n\n\n\n1\nmultiple\nprepare input file names & paths\n\n\n2\nmultiple\nDefine primers (all orientations)\n\n\n3\ncutadapt\nRemove primers\n\n\n4\nfilterAndTrim()\nassess quality & filter reads\n\n\n5\nlearnErrors()\ngenerate an error model for the data\n\n\n6\nderepFastq\ndereplicate sequences\n\n\n7\ndada()\ninfer ASVs on both forward & reverse reads independently\n\n\n8\nmakeSequenceTable()\ngenerate a count table\n\n\n9\nremoveBimeraDenovo()\nscreen for & remove chimeras\n\n\n10\n\ntrack reads through workflow\n\n\n11\nassignTaxonomy()\nassign taxonomy & finish workflow\n\n\n12\nsave.image()\nsave an image of seqtab & taxtab for next part of workflow"
  },
  {
    "objectID": "dada2.html#set-working-environment",
    "href": "dada2.html#set-working-environment",
    "title": "1. DADA2 Workflow",
    "section": "1. Set Working Environment",
    "text": "1. Set Working Environment\nNext, we need to setup the working environment by renaming the fastq files and defining a path for the working directory.\nRename samples\nTo make the parsing easier, we will eliminate the lane and well number from each sample. If you do not wish to do that you will need to adjust the code accordingly. I am positive there are more elegant ways of doing this.\n\nCAUTION. If you use this for your own data, please check that this code works on a few backup files before proceeding.\n\n\nsetwd(\"RAW\")\norig_fastq <- list.files(pattern = \"*.fastq.gz\")\nnewname_fastq <- gsub(\"_S.*_L001\", \"\", orig_fastq)\nnewname_fastq <- gsub(\"_001\", \"\", newname_fastq)\nfile.rename(orig_fastq, newname_fastq)\nsetwd(\"../\")\n\n\npath <- \"RAW\"\nhead(list.files(path))\n\n\n\n[1] \"P1_R1.fastq.gz\"    \"P1_R2.fastq.gz\"    \"P1-8C_R1.fastq.gz\"\n[4] \"P1-8C_R2.fastq.gz\" \"P10_R1.fastq.gz\"   \"P10_R2.fastq.gz\"  \n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs <- sort(list.files(path, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfnRs <- sort(list.files(path, pattern = \"_R2.fastq.gz\", full.names = TRUE))\n\n\np1a <- plotQualityProfile(fnFs[1:15], aggregate = TRUE)\np2a <- plotQualityProfile(fnRs[1:15], aggregate = TRUE)\n\np3a <- grid.arrange(p1a, p2a, nrow = 1)\nggsave(\"plot_qscores_2018a.png\", p3a, width = 7, height = 3)\n\n\n\n\nUnfortunately, the reverse reads are unusable.\n\n\n\n\n\n(16S rRNA) Figure 1 | Aggregated quality score plots for forward (left) & reverse (right) reads."
  },
  {
    "objectID": "dada2.html#define-primers",
    "href": "dada2.html#define-primers",
    "title": "1. DADA2 Workflow",
    "section": "2. Define Primers",
    "text": "2. Define Primers\nBefore we start the DADA2 workflow we need to run catadapt (Martin 2011) on all fastq.gz files to trim the primers. For bacteria and archaea, we amplified the V4 hypervariable region of the 16S rRNA gene using the primer pair 515F (GTGCCAGCMGCCGCGGTAA) and 806R (GGACTACHVGGGTWTCTAAT) (Caporaso et al. 2011), which should yield an amplicon length of about 253 bp.\nFirst we define the primers.\n\nFWD <- \"GTGCCAGCMGCCGCGGTAA\"\nREV <- \"GGACTACHVGGGTWTCTAAT\"\n\nNext, we check the presence and orientation of these primers in the data. I started doing this for ITS data because of primer read-through but I really like the general idea of doing it just to make sure nothing funny is going of with the data. To do this, we will create all orientations of the input primer sequences. In other words the Forward, Complement, Reverse, and Reverse Complement variations.\n\nallOrients <- function(primer) {\n    require(Biostrings)\n    dna <- DNAString(primer)\n    orients <- c(Forward = dna,\n                 Complement = complement(dna),\n                 Reverse = reverse(dna),\n                 RevComp = reverseComplement(dna))\n    return(sapply(orients, toString))\n}\nFWD.orients <- allOrients(FWD)\nREV.orients <- allOrients(REV)\nFWD.orients\nREV.orients\n\n              Forward            Complement               Reverse\n\"GTGCCAGCMGCCGCGGTAA\" \"CACGGTCGKCGGCGCCATT\" \"AATGGCGCCGMCGACCGTG\"\n              RevComp\n\"TTACCGCGGCKGCTGGCAC\"\n               Forward             Complement                Reverse\n\"GGACTACHVGGGTWTCTAAT\" \"CCTGATGDBCCCAWAGATTA\" \"TAATCTWTGGGVHCATCAGG\"\n               RevComp\n\"ATTAGAWACCCBDGTAGTCC\"\nNow we do a little pre-filter step to eliminate ambiguous bases (Ns) because Ns make mapping of short primer sequences difficult. This step removes any reads with Ns. Again, set some files paths, this time for the filtered reads.\n\nfnFs.filtN <- file.path(path, \"filtN\", basename(fnFs))\nfilterAndTrim(fnFs, fnFs.filtN,\n              maxN = 0, multithread = TRUE)\n\nNice. Time to assess the number of times a primer (and all primer orientations) appear in the forward and reverse reads. According to the workflow, counting the primers on one set of paired end fastq files is sufficient to see if there is a problem. This assumes that all the files were created using the same library prep. Basically for both primers, we will search for all four orientations in both forward and reverse reads. Since this is 16S rRNA we do not anticipate any issues but it is worth checking anyway.\n\nsampnum <- 2\n\n\nprimerHits <- function(primer, fn) {\n    # Counts number of reads in which the primer is found\n    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n    return(sum(nhits > 0))\n}\n\nForward primers\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits,\n                                fn = fnFs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads   74707          0       0       0\nReverse primers\n\nrbind(REV.ForwardReads = sapply(REV.orients, primerHits,\n                                fn = fnFs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nREV.ForwardReads       0          0       0     165\nAs expected, forward primers predominantly in the forward reads and very little evidence of reverse primers."
  },
  {
    "objectID": "dada2.html#remove-primers",
    "href": "dada2.html#remove-primers",
    "title": "1. DADA2 Workflow",
    "section": "3. Remove Primers",
    "text": "3. Remove Primers\nNow we can run catadapt (Martin 2011) to remove the primers from the fastq sequences. A little setup first. If this command executes successfully it means R has found cutadapt.\n\ncutadapt <- \"/Users/rad/miniconda3/envs/cutadapt/bin/cutadapt\"\nsystem2(cutadapt, args = \"--version\") # Run shell commands from R\n\n2.8\nWe set paths and trim the forward primer and the reverse-complement of the reverse primer off of R1 (forward reads) and trim the reverse primer and the reverse-complement of the forward primer off of R2 (reverse reads).\n\npath.cut <- file.path(path, \"cutadapt\")\nif(!dir.exists(path.cut)) dir.create(path.cut)\nfnFs.cut <- file.path(path.cut, basename(fnFs))\n\nFWD.RC <- dada2:::rc(FWD)\nREV.RC <- dada2:::rc(REV)\n\nR1.flags <- paste(\"-g\", FWD, \"-a\", REV.RC)\n\nfor(i in seq_along(fnFs)) {system2(cutadapt,\n                                   args = c(R1.flags,\n                                            \"-m\", 20, \"-n\", 2, \"-e\", 0.12,\n                                            \"-o\", fnFs.cut[i],\n                                            fnFs.filtN[i]))}\n\nThis is cutadapt 2.8 with Python 3.7.6\nCommand line parameters: -g GTGCCAGCMGCCGCGGTAA -a ATTAGAWACCCBDGTAGTCC \\\n                         -m 20 -n 2 -e 0.12 \\\n                         -o RAW/cutadapt/P1_R1.fastq.gz RAW/filtN/P1_R1.fastq.gz\nNote. If the code above removes all of the base pairs in a sequence, you will get downstream errors unless you set the -m flag. This flag sets the minimum length and reads shorter than this will be discarded. Without this flag, reads of length 0 will be kept and cause issues. Also, a lot of output will be written to the screen by cutadapt!.\nWe can now count the number of primers in the sequences from the output of cutadapt.\n\nrbind(\n  FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[sampnum]]),\n  REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads       0          0       0       0\nREV.ForwardReads       0          0       0       0\nBasically, primers are no longer detected in the cutadapted reads. Now, for each sample, we can take a look at how the pre-filtering step and primer removal affected the total number of raw reads.\n\n\n\n\n\n\n(16S rRNA) Table 2 | Total reads per sample after prefiltering and primer removal (using cutadapt).\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "dada2.html#quality-assessment-filtering",
    "href": "dada2.html#quality-assessment-filtering",
    "title": "1. DADA2 Workflow",
    "section": "4. Quality Assessment & Filtering",
    "text": "4. Quality Assessment & Filtering\nWe need the forward and reverse fastq file names plus the sample names.\n\ncutFs <- sort(list.files(path.cut, pattern = \"_R1.fastq.gz\",\n                         full.names = TRUE))\nget.sample.name <- function(fname) strsplit(basename(fname), \"_\")[[1]][1]\nsample.names <- unname(sapply(cutFs, get.sample.name))\nhead(sample.names)\n\n\nc(\"P1\", \"P1-8C\", \"P10\", \"P2\", \"P3\", \"P3-8C\")\n\n[1] \"P1\"    \"P1-8C\" \"P10\"   \"P2\"    \"P3\"    \"P3-8C\"\n\n\nFirst let’s look at the quality of our reads. The numbers in brackets specify which samples to view. Here we are looking at an aggregate plot of all data (except the negative control).\nQuality plots\n\np1 <- plotQualityProfile(cutFs[1:15], aggregate = TRUE)\nggsave(\"ssu18_plot_qscores.png\", p1, width = 7, height = 3)\nggsave(\"ssu18_plot_qscores.png\", p1)\n\n\n\n\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 2 | Aggregated quality score plot for forward reads after prefilter & primer removal.\nThe forward reads look good.\nFiltering\nFirst, we again make some path variables and setup a new directory of filtered reads.\n\nfiltFs <- file.path(path.cut, \"filtered\", basename(cutFs))\n\nAnd then we trim the reads.\n\nout <- filterAndTrim(cutFs, filtFs,\n                     maxN = 0, maxEE = 2,\n                     truncQ = 2, rm.phix = TRUE,\n                     compress = TRUE, multithread = 20)\nout\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample. As you can see, there are a few samples that started with a low read count to begin with—we will likely remove those samples at some point.\n\n\n\n(16S rRNA) Table 3 | Total reads per sample after filtering.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "dada2.html#learn-error-rates",
    "href": "dada2.html#learn-error-rates",
    "title": "1. DADA2 Workflow",
    "section": "5. Learn Error Rates",
    "text": "5. Learn Error Rates\nNow it is time to assess the error rate of the data. The DADA2 algorithm uses a parametric error model. Every amplicon data set has a different set of error rates and the learnErrors method learns this error model from the data. It does this by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. The algorithm begins with an initial guess, for which the maximum possible error rates in the data are used.\nForward Reads\n\nerrF <- learnErrors(filtFs, multithread = FALSE)\n\n103002523 total bases in 443994 reads from 6 samples will be used for learning the error rates.\n\nplotErrors(errF, nominalQ=TRUE)\n\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 3 | Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.\nThe error rates for each possible transition (A to C, A to G, etc.) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected."
  },
  {
    "objectID": "dada2.html#dereplicate-reads",
    "href": "dada2.html#dereplicate-reads",
    "title": "1. DADA2 Workflow",
    "section": "6. Dereplicate Reads",
    "text": "6. Dereplicate Reads\nNow we can use derepFastq to identify the unique sequences in the forward and reverse fastq files.\nForward Reads\n\n\n\n\nderepFs <- derepFastq(filtFs)\nnames(derepFs) <- sample.names\n\n\nDetailed results of derep forward reads\n\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P1_R1.fastq.gz\nEncountered 39950 unique sequences from 79804 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P1-8C_R1.fastq.gz\nEncountered 31163 unique sequences from 72760 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P10_R1.fastq.gz\nEncountered 35274 unique sequences from 62598 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P2_R1.fastq.gz\nEncountered 42119 unique sequences from 81240 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P3_R1.fastq.gz\nEncountered 40336 unique sequences from 86732 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P3-8C_R1.fastq.gz\nEncountered 31104 unique sequences from 60860 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P4_R1.fastq.gz\nEncountered 35169 unique sequences from 63037 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P5_R1.fastq.gz\nEncountered 27752 unique sequences from 54127 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P5-8C_R1.fastq.gz\nEncountered 49045 unique sequences from 109421 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P6_R1.fastq.gz\nEncountered 46853 unique sequences from 94740 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P7_R1.fastq.gz\nEncountered 35363 unique sequences from 71678 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P7-8C_R1.fastq.gz\nEncountered 35567 unique sequences from 73639 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P8_R1.fastq.gz\nEncountered 38518 unique sequences from 72263 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P9_R1.fastq.gz\nEncountered 43426 unique sequences from 89315 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P9-8C_R1.fastq.gz\nEncountered 13673 unique sequences from 33000 total sequences read."
  },
  {
    "objectID": "dada2.html#dada2-asv-inference",
    "href": "dada2.html#dada2-asv-inference",
    "title": "1. DADA2 Workflow",
    "section": "7. DADA2 & ASV Inference",
    "text": "7. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nWe tested all three methods through the full pipeline. Click the + to see the results of the test. For our final analysis, we chose pool = FALSE for this data set.\n\nShow/hide Results of dada pooling options\n\nHere are summary tables of results from the tests. Values are from the final sequence table produced by each option.\n\n\n\n(16S rRNA) Table 4 | Total number of reads, total number of ASVs, minimum and maximum ASVs, followed by the number of singletons, doubletons, etc. for pooling options.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n(16S rRNA) Table 5 | Total number of reads and ASVs by sample for pooling options.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\ndadaFs <- dada(derepFs, err = errF, multithread = TRUE, pool = FALSE)\n\n\nDetailed results of dada on forward reads\n\nSample 1 - 79804 reads in 39950 unique sequences.\nSample 2 - 72760 reads in 31163 unique sequences.\nSample 3 - 62598 reads in 35274 unique sequences.\nSample 4 - 81240 reads in 42119 unique sequences.\nSample 5 - 86732 reads in 40336 unique sequences.\nSample 6 - 60860 reads in 31104 unique sequences.\nSample 7 - 63037 reads in 35169 unique sequences.\nSample 8 - 54127 reads in 27752 unique sequences.\nSample 9 - 109421 reads in 49045 unique sequences.\nSample 10 - 94740 reads in 46853 unique sequences.\nSample 11 - 71678 reads in 35363 unique sequences.\nSample 12 - 73639 reads in 35567 unique sequences.\nSample 13 - 72263 reads in 38518 unique sequences.\nSample 14 - 89315 reads in 43426 unique sequences.\nSample 15 - 33000 reads in 13673 unique sequences.\n\nAs an example, we can inspect the returned dada-class object for the forward and reverse reads from the sample #2:\n\ndadaFs[[sampnum]]\n\ndada-class: object describing DADA2 denoising results\n1759 sequence variants were inferred from 31163 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nThis output tells us how many true sequence variants the DADA2 algorithm inferred from the unique sequences, in this case the sample 2."
  },
  {
    "objectID": "dada2.html#construct-sequence-table",
    "href": "dada2.html#construct-sequence-table",
    "title": "1. DADA2 Workflow",
    "section": "8. Construct Sequence Table",
    "text": "8. Construct Sequence Table\nNow we construct an amplicon sequence variant (ASV) table.\n\nseqtab <- makeSequenceTable(dadaFs)\ndim(seqtab)\n\n\n\n[1]    15 21685\n\n\n\ntable(nchar(getSequences(seqtab)))\n\n   22    23    24    25    27    41    66    73   120   149   185   190   193 \n    2     1     1     2     1     1     1     1     1     1     1     9     1 \n  198   200   201   202   208   209   211   215   217   220   221   222   223 \n    1     1     1     1     1     1     1     1     2     4    14    21     8 \n  224   225   227   228   229   230   231   232   233   234   238   251 \n    3     4     6     4     5     9    20 21342   190    12     2     8 \nThe sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. We have 21685 sequence variants but also a range of sequence lengths. Since many of these sequence variants are singletons or doubletons, we can just select a range that corresponds to the expected amplicon length and eliminate the spurious reads.\n\n\n\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 4 | Distribution of read length by total ASVs before removing extreme length variants.\n\nseqtab.2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(230,235)]\ndim(seqtab.2)\n\n\n\n\n\n\n\nNote\n\n\n\nThe values you select should be based on the sequence table generated above.\n\n\n\n\n[1]    15 21573\n\n\n\ntable(nchar(getSequences(seqtab.2)))\n\n  230   231   232   233   234 \n    9    20 21342   190    12 \nAfter removing the extreme length variants, we have 21573, a reduction of 112 sequence variants."
  },
  {
    "objectID": "dada2.html#remove-chimeras",
    "href": "dada2.html#remove-chimeras",
    "title": "1. DADA2 Workflow",
    "section": "9. Remove Chimeras",
    "text": "9. Remove Chimeras\nEven though the dada method corrects substitution and indel errors, chimeric sequences remain. According to the DADA2 documentation, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant parent sequences.\n\nseqtab.nochim <- removeBimeraDenovo(seqtab.2,\n                                    method = \"consensus\",\n                                    multithread = 20,\n                                    verbose = TRUE)\ndim(seqtab.nochim)\n\nIdentified 1241 bimeras out of 21573 input sequences.\n\n\n[1]    15 20332\n\n\n\nsum(seqtab.nochim)/sum(seqtab.2)\n\n\n\n[1] 0.9596595\n\n\nChimera checking removed an additional 1241 sequence variants however, when we account for the abundances of each variant, we see chimeras accounts for about 4.03405% of the merged sequence reads. Not bad.\nThe last thing we want to do is write the sequence table to an RDS file.\n\nsaveRDS(seqtab.nochim, \"ssu18_seqtab.nochim.rds\")"
  },
  {
    "objectID": "dada2.html#track-reads-through-workflow",
    "href": "dada2.html#track-reads-through-workflow",
    "title": "1. DADA2 Workflow",
    "section": "10. Track Reads through Workflow",
    "text": "10. Track Reads through Workflow\nAt this point we can look at the number of reads that made it through each step of the workflow for every sample.\n\ngetN <- function(x) sum(getUniques(x))\ntrack <- cbind(out,\n               sapply(dadaFs, getN),\n               rowSums(seqtab.nochim))\ncolnames(track) <- c(\"input\", \"filtered\", \"denoisedF\", \"nonchim\")\nrownames(track) <- sample.names\n\n\n\n\n(16S rRNA) Table 6 | Tracking reads changes at each step of the DADA2 workflow.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\nAnd we can save a copy of the table for later use.\n\nwrite.table(track, \"ssu18_read_changes.txt\",\n            sep = \"\\t\", quote = FALSE, col.names=NA)"
  },
  {
    "objectID": "dada2.html#assign-taxonomy",
    "href": "dada2.html#assign-taxonomy",
    "title": "1. DADA2 Workflow",
    "section": "11. Assign Taxonomy",
    "text": "11. Assign Taxonomy\nThe assignTaxonomy command implements the naive Bayesian classifier, so for reproducible results you need to set a random number seed (see issue #538). We did this at the beginning of the workflow. For taxonomic assignment, we are using the Silva version 138 (Quast et al. 2012). The developers of DADA2 maintain a formatted version of the database.\nWe will read in the RDS file containing the sequence table saved above. We also need to run remove(list = ls()) command, otherwise the final image we save will be huge. This way, the image only contains the sample data, seqtab, and taxtab after running removeBimeraDenovo.\n\nremove(list = ls())\nseqtab <- readRDS(\"ssu18_seqtab.nochim.rds\")\n\nAnd then native Bayesian (Wang et al. 2007) classifier against the Silva database.\n\ntax_silva <- assignTaxonomy(\n  seqtab, \"silva_nr_v138_train_set.fa.gz\", multithread = TRUE)"
  },
  {
    "objectID": "dada2.html#save-image",
    "href": "dada2.html#save-image",
    "title": "1. DADA2 Workflow",
    "section": "12. Save Image",
    "text": "12. Save Image\nAnd finally, we save an image for use in the analytic part of the workflow. This R data file will be needed as the input for the phyloseq portion of the workflow. See the Data Availability page for complete details on where to get this file.\n\nsave.image(\"ssu18_dada2_wf.rdata\")"
  },
  {
    "objectID": "dada2.html#r-session-information-code",
    "href": "dada2.html#r-session-information-code",
    "title": "1. DADA2 Workflow",
    "section": "R Session Information & Code",
    "text": "R Session Information & Code\nThis workflow was run on the Smithsonian High Performance Cluster (SI/HPC), Smithsonian Institution. Below are the specific packages and versions used in this workflow using both sessionInfo() and devtools::session_info(). Click the arrow to see the details.\n\nShow/hide R Session Info\nsessionInfo()\nR version 4.0.0 (2020-04-24)\nPlatform: x86_64-conda_cos6-linux-gnu (64-bit)\nRunning under: CentOS Linux 7 (Core)\n\nMatrix products: default\nBLAS/LAPACK: /home/scottjj/miniconda3/envs/R-4/lib/libopenblasp-r0.3.9.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C\n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8\n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8\n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C\n [9] LC_ADDRESS=C               LC_TELEPHONE=C\n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C\n\nattached base packages:\n [1] grid      stats4    parallel  stats     graphics  grDevices utils\n [8] datasets  methods   base\n\nother attached packages:\n [1] gridExtra_2.3               ggplot2_3.3.0\n [3] ShortRead_1.46.0            GenomicAlignments_1.24.0\n [5] SummarizedExperiment_1.18.1 DelayedArray_0.14.0\n [7] matrixStats_0.56.0          Biobase_2.48.0\n [9] Rsamtools_2.4.0             GenomicRanges_1.40.0\n[11] GenomeInfoDb_1.24.0         Biostrings_2.56.0\n[13] XVector_0.28.0              IRanges_2.22.1\n[15] S4Vectors_0.26.0            BiocParallel_1.22.0\n[17] BiocGenerics_0.34.0         dada2_1.16.0\n[19] Rcpp_1.0.4.6\n\nloaded via a namespace (and not attached):\n [1] plyr_1.8.6             RColorBrewer_1.1-2     pillar_1.4.4\n [4] compiler_4.0.0         bitops_1.0-6           tools_4.0.0\n [7] zlibbioc_1.34.0        lifecycle_0.2.0        tibble_3.0.1\n[10] gtable_0.3.0           lattice_0.20-41        png_0.1-7\n[13] pkgconfig_2.0.3        rlang_0.4.6            Matrix_1.2-18\n[16] GenomeInfoDbData_1.2.3 withr_2.2.0            stringr_1.4.0\n[19] hwriter_1.3.2          vctrs_0.3.0            glue_1.4.1\n[22] R6_2.4.1               jpeg_0.1-8.1           latticeExtra_0.6-29\n[25] reshape2_1.4.4         magrittr_1.5           scales_1.1.1\n[28] ellipsis_0.3.1         colorspace_1.4-1       stringi_1.4.6\n[31] RCurl_1.98-1.2         RcppParallel_5.0.1     munsell_0.5.0\n[34] crayon_1.3.4\n\ndevtools::session_info()\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.0.0 (2020-04-24)\n os       CentOS Linux 7 (Core)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2020-06-17\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package              * version  date       lib source\n assertthat             0.2.1    2019-03-21 [1] CRAN (R 4.0.0)\n backports              1.1.7    2020-05-13 [1] CRAN (R 4.0.0)\n Biobase              * 2.48.0   2020-04-27 [1] Bioconductor\n BiocGenerics         * 0.34.0   2020-04-27 [1] Bioconductor\n BiocParallel         * 1.22.0   2020-04-27 [1] Bioconductor\n Biostrings           * 2.56.0   2020-04-27 [1] Bioconductor\n bitops                 1.0-6    2013-08-17 [1] CRAN (R 4.0.0)\n callr                  3.4.3    2020-03-28 [1] CRAN (R 4.0.0)\n cli                    2.0.2    2020-02-28 [1] CRAN (R 4.0.0)\n colorspace             1.4-1    2019-03-18 [1] CRAN (R 4.0.0)\n crayon                 1.3.4    2017-09-16 [1] CRAN (R 4.0.0)\n dada2                * 1.16.0   2020-04-27 [1] Bioconductor\n DelayedArray         * 0.14.0   2020-04-27 [1] Bioconductor\n desc                   1.2.0    2018-05-01 [1] CRAN (R 4.0.0)\n devtools               2.3.0    2020-04-10 [1] CRAN (R 4.0.0)\n digest                 0.6.25   2020-02-23 [1] CRAN (R 4.0.0)\n ellipsis               0.3.1    2020-05-15 [1] CRAN (R 4.0.0)\n fansi                  0.4.1    2020-01-08 [1] CRAN (R 4.0.0)\n fs                     1.4.1    2020-04-04 [1] CRAN (R 4.0.0)\n GenomeInfoDb         * 1.24.0   2020-04-27 [1] Bioconductor\n GenomeInfoDbData       1.2.3    2020-05-22 [1] Bioconductor\n GenomicAlignments    * 1.24.0   2020-04-27 [1] Bioconductor\n GenomicRanges        * 1.40.0   2020-04-27 [1] Bioconductor\n ggplot2              * 3.3.0    2020-03-05 [1] CRAN (R 4.0.0)\n glue                   1.4.1    2020-05-13 [1] CRAN (R 4.0.0)\n gridExtra            * 2.3      2017-09-09 [1] CRAN (R 4.0.0)\n gtable                 0.3.0    2019-03-25 [1] CRAN (R 4.0.0)\n hwriter                1.3.2    2014-09-10 [1] CRAN (R 4.0.0)\n IRanges              * 2.22.1   2020-04-28 [1] Bioconductor\n jpeg                   0.1-8.1  2019-10-24 [1] CRAN (R 4.0.0)\n lattice                0.20-41  2020-04-02 [1] CRAN (R 4.0.0)\n latticeExtra           0.6-29   2019-12-19 [1] CRAN (R 4.0.0)\n lifecycle              0.2.0    2020-03-06 [1] CRAN (R 4.0.0)\n magrittr               1.5      2014-11-22 [1] CRAN (R 4.0.0)\n Matrix                 1.2-18   2019-11-27 [1] CRAN (R 4.0.0)\n matrixStats          * 0.56.0   2020-03-13 [1] CRAN (R 4.0.0)\n memoise                1.1.0    2017-04-21 [1] CRAN (R 4.0.0)\n munsell                0.5.0    2018-06-12 [1] CRAN (R 4.0.0)\n pillar                 1.4.4    2020-05-05 [1] CRAN (R 4.0.0)\n pkgbuild               1.0.8    2020-05-07 [1] CRAN (R 4.0.0)\n pkgconfig              2.0.3    2019-09-22 [1] CRAN (R 4.0.0)\n pkgload                1.0.2    2018-10-29 [1] CRAN (R 4.0.0)\n plyr                   1.8.6    2020-03-03 [1] CRAN (R 4.0.0)\n png                    0.1-7    2013-12-03 [1] CRAN (R 4.0.0)\n prettyunits            1.1.1    2020-01-24 [1] CRAN (R 4.0.0)\n processx               3.4.2    2020-02-09 [1] CRAN (R 4.0.0)\n ps                     1.3.3    2020-05-08 [1] CRAN (R 4.0.0)\n R6                     2.4.1    2019-11-12 [1] CRAN (R 4.0.0)\n RColorBrewer           1.1-2    2014-12-07 [1] CRAN (R 4.0.0)\n Rcpp                 * 1.0.4.6  2020-04-09 [1] CRAN (R 4.0.0)\n RcppParallel           5.0.1    2020-05-06 [1] CRAN (R 4.0.0)\n RCurl                  1.98-1.2 2020-04-18 [1] CRAN (R 4.0.0)\n remotes                2.1.1    2020-02-15 [1] CRAN (R 4.0.0)\n reshape2               1.4.4    2020-04-09 [1] CRAN (R 4.0.0)\n rlang                  0.4.6    2020-05-02 [1] CRAN (R 4.0.0)\n rprojroot              1.3-2    2018-01-03 [1] CRAN (R 4.0.0)\n Rsamtools            * 2.4.0    2020-04-27 [1] Bioconductor\n S4Vectors            * 0.26.0   2020-04-27 [1] Bioconductor\n scales                 1.1.1    2020-05-11 [1] CRAN (R 4.0.0)\n sessioninfo            1.1.1    2018-11-05 [1] CRAN (R 4.0.0)\n ShortRead            * 1.46.0   2020-04-27 [1] Bioconductor\n stringi                1.4.6    2020-02-17 [1] CRAN (R 4.0.0)\n stringr                1.4.0    2019-02-10 [1] CRAN (R 4.0.0)\n SummarizedExperiment * 1.18.1   2020-04-30 [1] Bioconductor\n testthat               2.3.2    2020-03-02 [1] CRAN (R 4.0.0)\n tibble                 3.0.1    2020-04-20 [1] CRAN (R 4.0.0)\n usethis                1.6.1    2020-04-29 [1] CRAN (R 4.0.0)\n vctrs                  0.3.0    2020-05-11 [1] CRAN (R 4.0.0)\n withr                  2.2.0    2020-04-20 [1] CRAN (R 4.0.0)\n XVector              * 0.28.0   2020-04-27 [1] Bioconductor\n zlibbioc               1.34.0   2020-04-27 [1] Bioconductor"
  },
  {
    "objectID": "dada2.html#workflow-input-1",
    "href": "dada2.html#workflow-input-1",
    "title": "1. DADA2 Workflow",
    "section": "Workflow Input",
    "text": "Workflow Input\nAll files needed to run this workflow can be downloaded from figshare."
  },
  {
    "objectID": "dada2.html#overview-1",
    "href": "dada2.html#overview-1",
    "title": "1. DADA2 Workflow",
    "section": "Overview",
    "text": "Overview\nSequence Files & Samples\nWe sequenced a total of 15 samples collected from 10 different plots at a single depth in each plot.\n\n\n\n(ITS) Table 1 | Sample data & associated sequencing information.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\nWorkflow\nOur workflow is pretty much ripped from the DADA2 ITS Workflow (1.8) on the DADA2 website. That webpage contains thorough explanations for each step so we will not repeat most of that here. For more details, check out the post. The workflow consists of the following steps:\n\n\n\n\n\n\n\nStep\nCommand\nWhat we’re doing\n\n\n\n1\nmultiple\nprepare input file names & paths\n\n\n2\nmultiple\nDefine primers (all orientations)\n\n\n3\ncutadapt\nRemove primers\n\n\n4\nfilterAndTrim()\nassess quality & filter reads\n\n\n5\nlearnErrors()\ngenerate an error model for the data\n\n\n6\nderepFastq\ndereplicate sequences\n\n\n7\ndada()\ninfer ASVs on both forward & reverse reads independently\n\n\n8\nmergePairs()\nmerge forward & reverse reads to further refine ASVs\n\n\n9\nmakeSequenceTable()\ngenerate a count table\n\n\n10\nremoveBimeraDenovo()\nscreen for & remove chimeras\n\n\n11\n\ntrack reads through workflow\n\n\n12\nassignTaxonomy()\nassign taxonomy & finish workflow\n\n\n13\nsave.image()\nsave an image of seqtab & taxtab for next part of workflow"
  },
  {
    "objectID": "dada2.html#set-working-environment-1",
    "href": "dada2.html#set-working-environment-1",
    "title": "1. DADA2 Workflow",
    "section": "1. Set Working Environment",
    "text": "1. Set Working Environment\nNext, we need to setup the working environment by renaming the fastq files and defining a path for the working directory.\nRename samples\nTo make the parsing easier, we will eliminate the lane and well number from each sample. If you do not wish to do that you will need to adjust the code accordingly. I am positive there are more elegant ways of doing this.\n\nCAUTION. You should check that this code works on a few backup files before proceeding.\n\n\nsetwd(\"RAW\")\norig_fastq <- list.files(pattern = \"*.fastq.gz\")\nnewname_fastq <- gsub(\"_S.*_L001\", \"\", orig_fastq)\nnewname_fastq <- gsub(\"_001\", \"\", newname_fastq)\nfile.rename(orig_fastq, newname_fastq)\nsetwd(\"../\")\n\n\npath <- \"RAW\"\nhead(list.files(path))\n\n\n\n[1] \"P1_R1.fastq.gz\"    \"P1_R2.fastq.gz\"    \"P1-8C_R1.fastq.gz\"\n[4] \"P1-8C_R2.fastq.gz\" \"P10_R1.fastq.gz\"   \"P10_R2.fastq.gz\"  \n\n\nThen, we generate matched lists of the forward and reverse read files. We also parse out the sample name.\n\nfnFs <- sort(list.files(path, pattern = \"_R1.fastq.gz\", full.names = TRUE))\nfnRs <- sort(list.files(path, pattern = \"_R2.fastq.gz\", full.names = TRUE))"
  },
  {
    "objectID": "dada2.html#define-primers-1",
    "href": "dada2.html#define-primers-1",
    "title": "1. DADA2 Workflow",
    "section": "2. Define Primers",
    "text": "2. Define Primers\nBefore we start the DADA2 workflow we need to run catadapt (Martin 2011) on all fastq.gz files to trim the primers. For this part of the study we used the primer pair ITS1f (CTTGGTCATTTAGAGGAAGTAA) (Gardes and Bruns 1993) and ITS2 (GCTGCGTTCTTCATCGATGC) (White et al. 1990) which should yield variable amplicon lengths between 100 to 400 bp. What we are looking for is potential read-through scenarios that are possible when sequencing the length variable ITS region as described in the DADA2 ITS Workflow (1.8). Please refer to this document for a complete explanation.\nFirst we define the primers.\n\nFWD <- \"CTTGGTCATTTAGAGGAAGTAA\"\nREV <- \"GCTGCGTTCTTCATCGATGC\"\n\nNext, we check the presence and orientation of these primers in the data. To do this we will create all orientations of the input primer sequences. In other words the Forward, Complement, Reverse, and Reverse Complement variations.\n\nallOrients <- function(primer) {\n    require(Biostrings)\n    dna <- DNAString(primer)\n    orients <- c(Forward = dna,\n                 Complement = complement(dna),\n                 Reverse = reverse(dna),\n                 RevComp = reverseComplement(dna))\n    return(sapply(orients, toString))\n}\nFWD.orients <- allOrients(FWD)\nREV.orients <- allOrients(REV)\nFWD.orients\nREV.orients\n\n                 Forward               Complement                  Reverse\n\"CTTGGTCATTTAGAGGAAGTAA\" \"GAACCAGTAAATCTCCTTCATT\" \"AATGAAGGAGATTTACTGGTTC\"\n                 RevComp\n\"TTACTTCCTCTAAATGACCAAG\"\n               Forward             Complement                Reverse\n\"GCTGCGTTCTTCATCGATGC\" \"CGACGCAAGAAGTAGCTACG\" \"CGTAGCTACTTCTTGCGTCG\"\n               RevComp\n\"GCATCGATGAAGAACGCAGC\"\nNow we do a little pre-filter step to eliminate ambiguous bases (Ns) because Ns make mapping of short primer sequences difficult. This step removes any reads with Ns. Again, set some files paths, this time for the filtered reads.\n\nfnFs.filtN <- file.path(path, \"filtN\", basename(fnFs))\nfnRs.filtN <- file.path(path, \"filtN\", basename(fnRs))\nfilterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN,\n              maxN = 0, multithread = TRUE)\n\nSweet. Time to assess the number of times a primer (and all possible primer orientation) appear in the forward and reverse reads. According to the workflow, counting the primers on one set of paired end fastq files is sufficient to see if there is a problem. This assumes that all the files were created using the same library prep. Basically for both primers, we will search for all four orientations in both forward and reverse reads\n\nsampnum <- 1\nprimerHits <- function(primer, fn) {\n    # Counts number of reads in which the primer is found\n    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)\n    return(sum(nhits > 0))\n}\n\nForward primers\n\nrbind(FWD.ForwardReads = sapply(FWD.orients, primerHits,\n                                fn = fnFs.filtN[[sampnum]]),\n    FWD.ReverseReads = sapply(FWD.orients, primerHits,\n                              fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads   48467          0       0       0\nFWD.ReverseReads       0          0       0    5078\nReverse primers\n\nrbind(REV.ForwardReads = sapply(REV.orients, primerHits,\n                                fn = fnFs.filtN[[sampnum]]),\n       REV.ReverseReads = sapply(REV.orients, primerHits,\n                                 fn = fnRs.filtN[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nREV.ForwardReads       0          0       0    6237\nREV.ReverseReads   43902          0       0       0\n\n\n\nWhat does this table mean? I wondered the same thing. Let’s break it down. Sample P1 had 50,894 sequences after the original filtering described earlier. The code searched the forward and reverse fastq files for all 8 primers. If we look at the two outputs, we see the forward primer is found in the forward reads in its forward orientation but also in some reverse reads in its reverse-complement orientation. The reverse primer is found in the reverse reads in its forward orientation but also in some forward reads in its reverse-complement orientation. This is due to read-through when the ITS region is short."
  },
  {
    "objectID": "dada2.html#remove-primers-1",
    "href": "dada2.html#remove-primers-1",
    "title": "1. DADA2 Workflow",
    "section": "3. Remove Primers",
    "text": "3. Remove Primers\nNow we can run catadapt (Martin 2011) to remove the primers from the fastq sequences. A little setup first. If this command executes successfully it means R has found cutadapt.\n\ncutadapt <- \"/PATH/to/cutadapt\"\nsystem2(cutadapt, args = \"--version\") # Run shell commands from R\n\n2.8\nWe set paths and trim the forward primer and the reverse-complement of the reverse primer off of R1 (forward reads) and trim the reverse primer and the reverse-complement of the forward primer off of R2 (reverse reads).\n\npath.cut <- file.path(path, \"cutadapt\")\nif(!dir.exists(path.cut)) dir.create(path.cut)\nfnFs.cut <- file.path(path.cut, basename(fnFs))\nfnRs.cut <- file.path(path.cut, basename(fnRs))\n\nFWD.RC <- dada2:::rc(FWD)\nREV.RC <- dada2:::rc(REV)\n\nR1.flags <- paste(\"-g\", FWD, \"-a\", REV.RC)\nR2.flags <- paste(\"-G\", REV, \"-A\", FWD.RC)\n\nfor(i in seq_along(fnFs)) {system2(cutadapt,\n                                   args = c(R1.flags, R2.flags,\n                                            \"-m\", 20, \"-n\", 2,\n                                            \"-o\", fnFs.cut[i],\n                                            \"-p\", fnRs.cut[i],\n                                            fnFs.filtN[i], fnRs.filtN[i]))}\n\nThis is cutadapt 2.8 with Python 3.7.6\nCommand line parameters: -g CTTGGTCATTTAGAGGAAGTAA -a GCATCGATGAAGAACGCAGC \\\n                         -G GCTGCGTTCTTCATCGATGC -A TTACTTCCTCTAAATGACCAAG \\\n                         -m 20 -n 2 \\\n                         -o RAW/cutadapt/P1_R1.fastq.gz \\\n                         -p RAW/cutadapt/P1_R2.fastq.gz RAW/filtN/P1_R1.fastq.gz RAW/filtN/P1_R2.fastq.gz\nNote. If the code above removes all of the base pairs in a sequence, you will get downstream errors unless you set the -m flag. This flag sets the minimum length and reads shorter than this will be discarded. Without this flag, reads of length 0 will be kept and cause issues. Also, a lot of output will be written to the screen by cutadapt!\nWe can now count the number of primers in the sequences from the output of cutadapt.\n\nrbind(\n  FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[sampnum]]),\n  FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[sampnum]]),\n  REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[sampnum]]),\n  REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[sampnum]]))\n\n                 Forward Complement Reverse RevComp\nFWD.ForwardReads       0          0       0       0\nFWD.ReverseReads       0          0       0       0\nREV.ForwardReads       0          0       0       0\nREV.ReverseReads       0          0       0       0\nBasically, primers are no longer detected in the cutadapted reads. Now, for each sample, we can take a look at how the pre-filtering step and primer removal affected the total number of raw reads.\n\n\n\n\n\n\n(ITS) Table 2 | Total reads per sample after prefiltering and primer removal (using cutadapt).\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "dada2.html#quality-assessment-filtering-1",
    "href": "dada2.html#quality-assessment-filtering-1",
    "title": "1. DADA2 Workflow",
    "section": "4. Quality Assessment & Filtering",
    "text": "4. Quality Assessment & Filtering\nWe need the forward and reverse fastq file names and the sample names.\n\ncutFs <- sort(list.files(path.cut, pattern = \"_R1.fastq.gz\",\n                         full.names = TRUE))\ncutRs <- sort(list.files(path.cut, pattern = \"_R2.fastq.gz\",\n                         full.names = TRUE))\n\nget.sample.name <- function(fname) strsplit(basename(fname), \"_\")[[1]][1]\nsample.names <- unname(sapply(cutFs, get.sample.name))\nhead(sample.names)\n\n\n\n[1] \"P1\"    \"P1-8C\" \"P10\"   \"P2\"    \"P3\"    \"P3-8C\"\n\n\nQuality plots\nThen we inspect read quality.\n\np1 <- plotQualityProfile(cutFs[1:15], aggregate = TRUE)\np2 <- plotQualityProfile(cutRs[1:15], aggregate = TRUE)\n\np3 <- grid.arrange(p1, p2, nrow = 1)\nggsave(\"plot_qscores_2018.png\", p3, width = 7, height = 3)\n\n\n\n\n\n\n\n\n\n(ITS) Figure 1 | Aggregated quality score plots for forward (left) & reverse (right) reads before filtering.\nFiltering\n\nfiltFs <- file.path(path.cut, \"filtered\", basename(cutFs))\nfiltRs <- file.path(path.cut, \"filtered\", basename(cutRs))\n\nWe use a minLen value to get rid of very short length sequences.\n\nout <- filterAndTrim(cutFs, filtFs, cutRs, filtRs,\n                     maxN = 0, maxEE = c(2, 2), truncQ = 2,\n                     minLen = 50, rm.phix = TRUE,\n                     compress = TRUE, multithread = TRUE)\nout\n\n\n\n\n\n\n\nNote\n\n\n\nThese parameters should be set based on the anticipated length of the amplicon and the read quality.\n\n\nAnd here is a table of how the filtering step affected the number of reads in each sample.\n\n\n\n(ITS) Table 3 | Total reads per sample after filtering.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "dada2.html#learn-error-rates-1",
    "href": "dada2.html#learn-error-rates-1",
    "title": "1. DADA2 Workflow",
    "section": "5. Learn Error Rates",
    "text": "5. Learn Error Rates\nTime to assess the error rate of the data. The rest of the workflow is very similar to the 16S workflows presented previously. So I will basically stop talking.\nForward Reads\n\nerrF <- learnErrors(filtFs, multithread = TRUE)\n\n108788735 total bases in 495784 reads from 11 samples will be used for learning the error rates.\n109358094 total bases in 495784 reads from 11 samples will be used for learning the error rates.\n\nplotErrors(errF, nominalQ = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n(ITS) Figure 3 | Forward reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score.\nReverse Reads\n\nerrR <- learnErrors(filtRs, multithread = TRUE)\n\n109358094 total bases in 495784 reads from 11 samples will be used for learning the error rates.\n\nplotErrors(errR, nominalQ = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n(ITS) Figure 4 | Reverse reads: Observed frequency of each transition (e.g., T -> G) as a function of the associated quality score."
  },
  {
    "objectID": "dada2.html#dereplicate-reads-1",
    "href": "dada2.html#dereplicate-reads-1",
    "title": "1. DADA2 Workflow",
    "section": "6. Dereplicate Reads",
    "text": "6. Dereplicate Reads\nForward Reads\n\nderepFs <- derepFastq(filtFs, verbose = TRUE)\nnames(derepFs) <- sample.names\n\n\n\nDetailed results of derep forward reads\n\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P1_R1.fastq.gz\nEncountered 7277 unique sequences from 35999 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P1-8C_R1.fastq.gz\nEncountered 11366 unique sequences from 48230 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P10_R1.fastq.gz\nEncountered 11714 unique sequences from 80877 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P2_R1.fastq.gz\nEncountered 13088 unique sequences from 58956 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P3_R1.fastq.gz\nEncountered 10180 unique sequences from 42563 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P3-8C_R1.fastq.gz\nEncountered 4480 unique sequences from 17548 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P4_R1.fastq.gz\nEncountered 12259 unique sequences from 68259 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P5_R1.fastq.gz\nEncountered 160 unique sequences from 308 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P5-8C_R1.fastq.gz\nEncountered 10436 unique sequences from 48518 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P6_R1.fastq.gz\nEncountered 11230 unique sequences from 51055 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P7_R1.fastq.gz\nEncountered 10316 unique sequences from 43471 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P7-8C_R1.fastq.gz\nEncountered 2121 unique sequences from 9488 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P8_R1.fastq.gz\nEncountered 9236 unique sequences from 41563 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P9_R1.fastq.gz\nEncountered 9000 unique sequences from 35722 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P9-8C_R1.fastq.gz\nEncountered 42 unique sequences from 85 total sequences read.\n\n\nReverse Reads\n\nderepRs <- derepFastq(filtRs, verbose = TRUE)\nnames(derepRs) <- sample.names\n\n\n\nDetailed results of derep reverse reads\n\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P1_R2.fastq.gz\nEncountered 13081 unique sequences from 35999 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P1-8C_R2.fastq.gz\nEncountered 17965 unique sequences from 48230 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P10_R2.fastq.gz\nEncountered 18610 unique sequences from 80877 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P2_R2.fastq.gz\nEncountered 19600 unique sequences from 58956 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P3_R2.fastq.gz\nEncountered 16145 unique sequences from 42563 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P3-8C_R2.fastq.gz\nEncountered 7496 unique sequences from 17548 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P4_R2.fastq.gz\nEncountered 19933 unique sequences from 68259 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P5_R2.fastq.gz\nEncountered 199 unique sequences from 308 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P5-8C_R2.fastq.gz\nEncountered 18495 unique sequences from 48518 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P6_R2.fastq.gz\nEncountered 17216 unique sequences from 51055 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P7_R2.fastq.gz\nEncountered 17111 unique sequences from 43471 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P7-8C_R2.fastq.gz\nEncountered 3809 unique sequences from 9488 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P8_R2.fastq.gz\nEncountered 14140 unique sequences from 41563 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P9_R2.fastq.gz\nEncountered 14856 unique sequences from 35722 total sequences read.\nDereplicating sequence entries in Fastq file: RAW/cutadapt/filtered/P9-8C_R2.fastq.gz\nEncountered 45 unique sequences from 85 total sequences read."
  },
  {
    "objectID": "dada2.html#dada2-asv-inference-1",
    "href": "dada2.html#dada2-asv-inference-1",
    "title": "1. DADA2 Workflow",
    "section": "7. DADA2 & ASV Inference",
    "text": "7. DADA2 & ASV Inference\nAt this point we are ready to apply the core sample inference algorithm (dada) to the filtered and trimmed sequence data. DADA2 offers three options for whether and how to pool samples for ASV inference.\nIf pool = TRUE, the algorithm will pool together all samples prior to sample inference.\nIf pool = FALSE, sample inference is performed on each sample individually.\nIf pool = \"pseudo\", the algorithm will perform pseudo-pooling between individually processed samples.\nWe tested all three methods through the full pipeline. Click the + to see the results of the test. For our final analysis, we chose pool = TRUE for this data set.\n\nShow/hide Results of dada pooling options\nHere are a few summary tables of results from the tests. Values are from the final sequence table prodcuced by each option.\n\n\n\n(ITS) Table 4 | Total number of reads, total number of ASVs, minimum and maximum ASVs, followed by the number of singletons, doubletons, etc. for pooling options.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n(ITS) Table 5 | Total number of reads and ASVs by sample for pooling options.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\nForward Reads\n\ndadaFs <- dada(derepFs, err = errF, pool = TRUE, multithread = TRUE)\n\n15 samples were pooled: 582642 reads in 102887 unique sequences.\n\ndadaFs[[sampnum]]\n\ndada-class: object describing DADA2 denoising results\n1175 sequence variants were inferred from 7277 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\nReverse Reads\n\ndadaRs <- dada(derepRs, err = errR, pool = TRUE, multithread = TRUE)\n\n15 samples were pooled: 582642 reads in 174866 unique sequences.\n\ndadaRs[[sampnum]]\n\ndada-class: object describing DADA2 denoising results\n1116 sequence variants were inferred from 13081 input unique sequences.\nKey parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16\n\nsave.image(\"pre_merge_its_2018.rdata\")"
  },
  {
    "objectID": "dada2.html#merge-paired-reads",
    "href": "dada2.html#merge-paired-reads",
    "title": "1. DADA2 Workflow",
    "section": "8. Merge Paired Reads",
    "text": "8. Merge Paired Reads\n\nmergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)\n\n\nDetailed results of merging reads\n\n29795 paired-reads (in 946 unique pairings) successfully merged out of 32193 (in 1177 pairings) input.\n41880 paired-reads (in 719 unique pairings) successfully merged out of 46715 (in 941 pairings) input.\n64805 paired-reads (in 815 unique pairings) successfully merged out of 66771 (in 1015 pairings) input.\nDuplicate sequences in merged output.\n51144 paired-reads (in 1011 unique pairings) successfully merged out of 57613 (in 1325 pairings) input.\n38442 paired-reads (in 767 unique pairings) successfully merged out of 41985 (in 994 pairings) input.\n14188 paired-reads (in 349 unique pairings) successfully merged out of 16981 (in 452 pairings) input.\n56594 paired-reads (in 1018 unique pairings) successfully merged out of 64411 (in 1306 pairings) input.\n296 paired-reads (in 95 unique pairings) successfully merged out of 301 (in 100 pairings) input.\n35713 paired-reads (in 619 unique pairings) successfully merged out of 41006 (in 786 pairings) input.\n45851 paired-reads (in 955 unique pairings) successfully merged out of 50174 (in 1240 pairings) input.\n36121 paired-reads (in 870 unique pairings) successfully merged out of 41541 (in 1135 pairings) input.\nDuplicate sequences in merged output.\n9172 paired-reads (in 335 unique pairings) successfully merged out of 9403 (in 402 pairings) input.\n38392 paired-reads (in 704 unique pairings) successfully merged out of 40897 (in 886 pairings) input.\n29155 paired-reads (in 748 unique pairings) successfully merged out of 34979 (in 990 pairings) input.\nDuplicate sequences in merged output.\n80 paired-reads (in 14 unique pairings) successfully merged out of 82 (in 16 pairings) input."
  },
  {
    "objectID": "dada2.html#construct-sequence-table-1",
    "href": "dada2.html#construct-sequence-table-1",
    "title": "1. DADA2 Workflow",
    "section": "9. Construct Sequence Table",
    "text": "9. Construct Sequence Table\n\nseqtabF <- makeSequenceTable(mergers)\ndim(seqtabF)\n\n\n\n[1]   15 3364\n\n\n\ntable(nchar(getSequences(seqtabF)))\n\n130 131 132 133 136 139 140 141 143 144 146 147 148 149 150 151 152 153 154 155 \n  2   2   1   1   1   1   1   3   2   1   3   5   5   4   5   5   3   2   5   4 \n156 159 160 161 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 \n  1   2   1   1   2   2   4   9   2   1   2   5   5   5   8   9   2   5  10  13 \n179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 \n 13  14  11  14  12  15  19  12  12  13   9  22  11  11  12  15  17  13  10  11 \n199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 \n 22  16  10  11  13  17  10   8  10  12  13   7  20  18  24  30  39  38  20  19 \n219 220 221 222 223 224 225 226 228 229 230 231 232 233 234 235 236 237 238 239 \n 12  18  21  23  23  29  32  32   6  48   4  73  20  27  29  19  31  33  30  29 \n240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 \n 35  21  36  16  16  18  17  25  22  29  17  27  28  22  20  18  28  19  28  25 \n260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 \n 23  11  15  13  16  18  23  17  12  12  16  15  32  17  22  21  21  18  16  17 \n280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 \n 13  22  13  18  12  13  13  18  17  15  14  18   8  11  13  17  12  11  13   9 \n300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 \n 16  10   9   3   8  12   8  13  20  15   8  13  16  11  10   8  18  10   8  12 \n320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 \n  8   5  12  17  13   9   8   8   9  12  16  13  14  11  12  13  18  10  11  10 \n340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 \n  9  11   8   8   5  15   8  16  11  15   4   9   7   9  12  14   5  12  11   5 \n360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 \n  2   4  11   5   2  14   6  12   7   5   6   4   3   9   5  10   8   9   4   6 \n380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 \n  5   3   7   7   4   2   3   4   3   5   5   1   3   4   3   6   5   4   3   2 \n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 421 \n 11   5   3   3   3   3   2   1   2   3   3   1   3   3   1   2   2   1   2   2 \n422 423 424 427 430 431 432 434 436 437 439 440 441 442 443 446 447 448 \n  2   3   2   1   3   2   4   2   1   2   4   3   1   2   1   1   2   1 \n\n\n\n\n\n\n\n\n\n\n\n\n(ITS) Figure 5 | Distribution of read length by total ASVs before removing extreme length variants.\nAs expected, a wide range of length variability in the the amplified ITS region.\n\nseqtab <- seqtabF[,nchar(colnames(seqtabF)) %in% seq(100,450)]\ndim(seqtab)\n\n\n\n[1]   15 3364\n\n\nAfter removing the extreme length variants, we have 3364, a reduction of 0 sequence variants.\n\ntable(nchar(getSequences(seqtab)))"
  },
  {
    "objectID": "dada2.html#remove-chimeras-1",
    "href": "dada2.html#remove-chimeras-1",
    "title": "1. DADA2 Workflow",
    "section": "10. Remove Chimeras",
    "text": "10. Remove Chimeras\n\nseqtab.nochim <- removeBimeraDenovo(seqtab,\n                                    method = \"consensus\",\n                                    multithread = TRUE, verbose = TRUE)\ndim(seqtab.nochim)\n\nIdentified 7 bimeras out of 3364 input sequences.\n\n\n[1]   15 3357\n\n\n\nsum(seqtab.nochim)/sum(seqtab.2)\n\n\n\n[1] 0.9990135\n\n\nChimera checking removed an additional 7 sequence variants however, when we account for the abundances of each variant, we see chimeras accounts for about 0.09865% of the merged sequence reads. Curious.\n\ntable(colSums(seqtab.nochim>0))\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15\n1247  711  443  289  204  123  102   86   49   32   24   15   23    7    2\n\ntable(colSums(seqtab.nochim>0))\n\n  14   95  335  349  616  702  719  745  765  812  867  945  954 1011 1017\n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1\nThe last thing we want to do is write the sequence table to an RDS file.\n\nsaveRDS(seqtab.nochim, \"its18_seqtab.nochim.rds\")"
  },
  {
    "objectID": "dada2.html#track-reads-through-workflow-1",
    "href": "dada2.html#track-reads-through-workflow-1",
    "title": "1. DADA2 Workflow",
    "section": "11. Track Reads through Workflow",
    "text": "11. Track Reads through Workflow\n\n\n\n(ITS) Table 6 | Tracking reads changes at each step of the DADA2 workflow.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "dada2.html#assign-taxonomy-1",
    "href": "dada2.html#assign-taxonomy-1",
    "title": "1. DADA2 Workflow",
    "section": "12. Assign Taxonomy",
    "text": "12. Assign Taxonomy\nFor taxonomic classification, we used the UNITE (Nilsson et al. 2019) database, specifically the UNITE general FASTA release for Fungi (v. 04.02.2020) (Abarenkov et al. 2020).\n\n\n\n\ntax <- assignTaxonomy(seqtab,\n                      \"sh_general_release_dynamic_s_04.02.2020_dev.fasta\",\n                      multithread = 20, tryRC = TRUE)\nsaveRDS(tax, \"its18_tax_its.rds\")\n\nUNITE fungal taxonomic reference detected."
  },
  {
    "objectID": "dada2.html#save-image-1",
    "href": "dada2.html#save-image-1",
    "title": "1. DADA2 Workflow",
    "section": "13. Save Image",
    "text": "13. Save Image\n\nsave.image(\"its18_dada2_wf.rdata\")"
  },
  {
    "objectID": "dada2.html#r-session-information-code-1",
    "href": "dada2.html#r-session-information-code-1",
    "title": "1. DADA2 Workflow",
    "section": "R Session Information & Code",
    "text": "R Session Information & Code\nThis workflow was run on the Smithsonian High Performance Cluster (SI/HPC), Smithsonian Institution. Below are the specific packages and versions used in this workflow using both sessionInfo() and devtools::session_info(). Click the arrow to see the details.\n\nShow/hide R Session Info\nsessionInfo()\nR version 4.0.0 (2020-04-24)\nPlatform: x86_64-conda_cos6-linux-gnu (64-bit)\nRunning under: CentOS Linux 7 (Core)\n\nMatrix products: default\nBLAS/LAPACK: /home/scottjj/miniconda3/envs/R-4/lib/libopenblasp-r0.3.9.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C\n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8\n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8\n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C\n [9] LC_ADDRESS=C               LC_TELEPHONE=C\n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C\n\nattached base packages:\n [1] grid      stats4    parallel  stats     graphics  grDevices utils\n [8] datasets  methods   base\n\nother attached packages:\n [1] gridExtra_2.3               ggplot2_3.3.0\n [3] ShortRead_1.46.0            GenomicAlignments_1.24.0\n [5] SummarizedExperiment_1.18.1 DelayedArray_0.14.0\n [7] matrixStats_0.56.0          Biobase_2.48.0\n [9] Rsamtools_2.4.0             GenomicRanges_1.40.0\n[11] GenomeInfoDb_1.24.0         Biostrings_2.56.0\n[13] XVector_0.28.0              IRanges_2.22.1\n[15] S4Vectors_0.26.0            BiocParallel_1.22.0\n[17] BiocGenerics_0.34.0         dada2_1.16.0\n[19] Rcpp_1.0.4.6\n\nloaded via a namespace (and not attached):\n [1] plyr_1.8.6             RColorBrewer_1.1-2     pillar_1.4.4\n [4] compiler_4.0.0         bitops_1.0-6           tools_4.0.0\n [7] zlibbioc_1.34.0        lifecycle_0.2.0        tibble_3.0.1\n[10] gtable_0.3.0           lattice_0.20-41        png_0.1-7\n[13] pkgconfig_2.0.3        rlang_0.4.6            Matrix_1.2-18\n[16] GenomeInfoDbData_1.2.3 withr_2.2.0            stringr_1.4.0\n[19] hwriter_1.3.2          vctrs_0.3.0            glue_1.4.1\n[22] R6_2.4.1               jpeg_0.1-8.1           latticeExtra_0.6-29\n[25] reshape2_1.4.4         magrittr_1.5           scales_1.1.1\n[28] ellipsis_0.3.1         colorspace_1.4-1       stringi_1.4.6\n[31] RCurl_1.98-1.2         RcppParallel_5.0.1     munsell_0.5.0\n[34] crayon_1.3.4\n\ndevtools::session_info()\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.0.0 (2020-04-24)\n os       CentOS Linux 7 (Core)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2020-06-04\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package              * version  date       lib source\n assertthat             0.2.1    2019-03-21 [1] CRAN (R 4.0.0)\n backports              1.1.7    2020-05-13 [1] CRAN (R 4.0.0)\n Biobase              * 2.48.0   2020-04-27 [1] Bioconductor\n BiocGenerics         * 0.34.0   2020-04-27 [1] Bioconductor\n BiocParallel         * 1.22.0   2020-04-27 [1] Bioconductor\n Biostrings           * 2.56.0   2020-04-27 [1] Bioconductor\n bitops                 1.0-6    2013-08-17 [1] CRAN (R 4.0.0)\n callr                  3.4.3    2020-03-28 [1] CRAN (R 4.0.0)\n cli                    2.0.2    2020-02-28 [1] CRAN (R 4.0.0)\n colorspace             1.4-1    2019-03-18 [1] CRAN (R 4.0.0)\n crayon                 1.3.4    2017-09-16 [1] CRAN (R 4.0.0)\n dada2                * 1.16.0   2020-04-27 [1] Bioconductor\n DelayedArray         * 0.14.0   2020-04-27 [1] Bioconductor\n desc                   1.2.0    2018-05-01 [1] CRAN (R 4.0.0)\n devtools               2.3.0    2020-04-10 [1] CRAN (R 4.0.0)\n digest                 0.6.25   2020-02-23 [1] CRAN (R 4.0.0)\n ellipsis               0.3.1    2020-05-15 [1] CRAN (R 4.0.0)\n fansi                  0.4.1    2020-01-08 [1] CRAN (R 4.0.0)\n fs                     1.4.1    2020-04-04 [1] CRAN (R 4.0.0)\n GenomeInfoDb         * 1.24.0   2020-04-27 [1] Bioconductor\n GenomeInfoDbData       1.2.3    2020-05-22 [1] Bioconductor\n GenomicAlignments    * 1.24.0   2020-04-27 [1] Bioconductor\n GenomicRanges        * 1.40.0   2020-04-27 [1] Bioconductor\n ggplot2              * 3.3.0    2020-03-05 [1] CRAN (R 4.0.0)\n glue                   1.4.1    2020-05-13 [1] CRAN (R 4.0.0)\n gridExtra            * 2.3      2017-09-09 [1] CRAN (R 4.0.0)\n gtable                 0.3.0    2019-03-25 [1] CRAN (R 4.0.0)\n hwriter                1.3.2    2014-09-10 [1] CRAN (R 4.0.0)\n IRanges              * 2.22.1   2020-04-28 [1] Bioconductor\n jpeg                   0.1-8.1  2019-10-24 [1] CRAN (R 4.0.0)\n lattice                0.20-41  2020-04-02 [1] CRAN (R 4.0.0)\n latticeExtra           0.6-29   2019-12-19 [1] CRAN (R 4.0.0)\n lifecycle              0.2.0    2020-03-06 [1] CRAN (R 4.0.0)\n magrittr               1.5      2014-11-22 [1] CRAN (R 4.0.0)\n Matrix                 1.2-18   2019-11-27 [1] CRAN (R 4.0.0)\n matrixStats          * 0.56.0   2020-03-13 [1] CRAN (R 4.0.0)\n memoise                1.1.0    2017-04-21 [1] CRAN (R 4.0.0)\n munsell                0.5.0    2018-06-12 [1] CRAN (R 4.0.0)\n pillar                 1.4.4    2020-05-05 [1] CRAN (R 4.0.0)\n pkgbuild               1.0.8    2020-05-07 [1] CRAN (R 4.0.0)\n pkgconfig              2.0.3    2019-09-22 [1] CRAN (R 4.0.0)\n pkgload                1.0.2    2018-10-29 [1] CRAN (R 4.0.0)\n plyr                   1.8.6    2020-03-03 [1] CRAN (R 4.0.0)\n png                    0.1-7    2013-12-03 [1] CRAN (R 4.0.0)\n prettyunits            1.1.1    2020-01-24 [1] CRAN (R 4.0.0)\n processx               3.4.2    2020-02-09 [1] CRAN (R 4.0.0)\n ps                     1.3.3    2020-05-08 [1] CRAN (R 4.0.0)\n R6                     2.4.1    2019-11-12 [1] CRAN (R 4.0.0)\n RColorBrewer           1.1-2    2014-12-07 [1] CRAN (R 4.0.0)\n Rcpp                 * 1.0.4.6  2020-04-09 [1] CRAN (R 4.0.0)\n RcppParallel           5.0.1    2020-05-06 [1] CRAN (R 4.0.0)\n RCurl                  1.98-1.2 2020-04-18 [1] CRAN (R 4.0.0)\n remotes                2.1.1    2020-02-15 [1] CRAN (R 4.0.0)\n reshape2               1.4.4    2020-04-09 [1] CRAN (R 4.0.0)\n rlang                  0.4.6    2020-05-02 [1] CRAN (R 4.0.0)\n rprojroot              1.3-2    2018-01-03 [1] CRAN (R 4.0.0)\n Rsamtools            * 2.4.0    2020-04-27 [1] Bioconductor\n S4Vectors            * 0.26.0   2020-04-27 [1] Bioconductor\n scales                 1.1.1    2020-05-11 [1] CRAN (R 4.0.0)\n sessioninfo            1.1.1    2018-11-05 [1] CRAN (R 4.0.0)\n ShortRead            * 1.46.0   2020-04-27 [1] Bioconductor\n stringi                1.4.6    2020-02-17 [1] CRAN (R 4.0.0)\n stringr                1.4.0    2019-02-10 [1] CRAN (R 4.0.0)\n SummarizedExperiment * 1.18.1   2020-04-30 [1] Bioconductor\n testthat               2.3.2    2020-03-02 [1] CRAN (R 4.0.0)\n tibble                 3.0.1    2020-04-20 [1] CRAN (R 4.0.0)\n usethis                1.6.1    2020-04-29 [1] CRAN (R 4.0.0)\n vctrs                  0.3.0    2020-05-11 [1] CRAN (R 4.0.0)\n withr                  2.2.0    2020-04-20 [1] CRAN (R 4.0.0)\n XVector              * 0.28.0   2020-04-27 [1] Bioconductor\n zlibbioc               1.34.0   2020-04-27 [1] Bioconductor"
  },
  {
    "objectID": "data-availability.html",
    "href": "data-availability.html",
    "title": "Data Availability",
    "section": "",
    "text": "Raw 16S rRNA data for each sample (before removing primers) is available on figshare at doi:10.25573/data.14686665. Raw ITS data for each sample (before removing primers) is available on figshare at doi:10.25573/data.14686755.\nAll trimmed sequence data (primers removed) is deposited at the European Nucleotide Archive (ENA) under the study accession number PRJEB45074 (ERP129199). The trimmed 16S rRNA data (primers removed) are deposited under sample accession numbers ERS6485270-ERS6485284. The trimmed ITS data (primers removed) are deposited under sample accession numbers ERS6485285-ERS6485299."
  },
  {
    "objectID": "data-availability.html#pipeline-data",
    "href": "data-availability.html#pipeline-data",
    "title": "Data Availability",
    "section": "Pipeline Data",
    "text": "Pipeline Data\nData for each individual pipeline are available through the Smithsonian figshare under a single collection at doi:10.25573/data.c.5667571.v2. In addition, data from each pipeline are available for download from figshare using the links at the bottom of each workflow page (where applicable).\n\n\n\n\n\n\n\nShow descriptions and DOI’s for each workflow\n\n\n\n\n\nScott, Jarrod (2022): (high-temp/16s) No 0. Raw 16S rRNA fastq data files for the high-temp SWELTR study. Smithsonian Tropical Research Institute. Dataset. https://doi.org/10.25573/data.14686665.v1\nScott, Jarrod (2022): (high-temp/ITS) No 0. Raw ITS fastq data files for the high-temp SWELTR study. Smithsonian Tropical Research Institute. Dataset. https://doi.org/10.25573/data.14686755.v1\nScott, Jarrod (2022): (high-temp) No 1. DADA2 Workflow (16S rRNA/ITS) Output. Smithsonian Tropical Research Institute. Dataset. https://doi.org/10.25573/data.14687184.v1\nScott, Jarrod (2022): (high-temp) No 2. Data Preparation (16S rRNA/ITS) Output. Smithsonian Tropical Research Institute. Dataset. https://doi.org/10.25573/data.14690739.v1\nScott, Jarrod (2022): (high-temp) No 3. Filtering: (16S rRNA/ITS) Output. Smithsonian Tropical Research Institute. Dataset. https://doi.org/10.25573/data.14701440.v1\nScott, Jarrod (2022): (high-temp) No 4. Taxonomic diversity: (16S rRNA/ITS) Output. Smithsonian Tropical Research Institute. Dataset. https://doi.org/10.25573/data.16826737.v1\nScott, Jarrod (2022): (high-temp) No 5. Aplha diversity (16S rRNA/ITS) Output. Smithsonian Tropical Research Institute. Dataset. https://doi.org/10.25573/data.16826779.v1\nScott, Jarrod (2022): (high-temp) No 6. Beta diversity (16S rRNA/ITS) Output. Smithsonian Tropical Research Institute. Dataset. https://doi.org/10.25573/data.16828063.v1\nScott, Jarrod (2022): (high-temp) No 7. Differentially Abundant ASVs & Taxa (16S rRNA/ITS) Output. Smithsonian Tropical Research Institute. Dataset. https://doi.org/10.25573/data.16828249.v1\nScott, Jarrod (2022): (high-temp) No 8. Metadata Analysis (16S rRNA/ITS) Output. Smithsonian Tropical Research Institute. Dataset. https://doi.org/10.25573/data.16828294.v1"
  },
  {
    "objectID": "data-availability.html#submitting-sequence-data",
    "href": "data-availability.html#submitting-sequence-data",
    "title": "Data Availability",
    "section": "Submitting Sequence Data",
    "text": "Submitting Sequence Data\nWe submitted out data to the European Nucleotide Archive (ENA). The ENA does not like RAW data and prefers to have primers removed. So we submitted the trimmed Fastq files to the ENA. You can find these data under the study accession number PRJEB45074 (ERP129199). The RAW files on our figshare site (see above).\nTo submit to the ENA you need two data tables (plus your sequence data). One file describes the samples and the other file describes the sequencing data.\nYou can download our submission data tables here:\n\nSample submission form\nFASTq submission form\n\nNote, these forms are study specific, so please use these as guides only.\nInstructions for Submitting to the ENA\nEven though I have done this dozens of times, the process of submitting sequence data to read archives still baffles me.\n\nNote: I submit data with primers, barcodes, etc removed\n\nRegister Project & Upload Sample Data\n\ngo to https://www.ebi.ac.uk/ena/submit and select Submit to ENA Interactively.\nLogin or Register.\nGo to New Submission tab and, if this is a new project, select Register study (project).\nHit Next\nEnter details and hit Submit.\nNext, Select Checklist. This will be specific to the type of samples you have and basically will create a template so you can add your sample metadata. For this study I chose GSC MIxS soil, checklist accession number ERC000022\n\nNext\nNow go through and select/deselect fields as needed. Note, some fields are mandatory.\nOnce finished, hit Next to fill in any details that apply to All samples and the download the template. Alternatively, you can download the template and fill in the data by hand.\nUpload the sample sheet.\nOnce everything looks good and uploaded, click the New Submission tab.\nUpload Sequence Data\n\nHit Skip and then select Two Fastq files (Paired), Download the template and fill in the details.\nNext, make sure all the trimmed fastq files are gzipped .gz (these are what you submit).\nNavigate to the directory with the trimmed, compressed fastq files and run:\n\nmd5sum *.gz\n\nAdd the checksums and file names to the fastq submission form. You can read more about Preparing A File For Upload here.\n\nSTOP\nBefore uploading the sheet, you first must upload the fastq files. Documentation for Uploading Files To ENA can be found here.\n\nIn the same directory, run:\n\nlftp webin2.ebi.ac.uk -u Webin-XXXXX\nwhere Webin-XXXXX is your user name. Enter your password\nat the prompt run:\nmput *.gz\n# when finished\nbye\nThe files should begin uploading. Depending on internet speed and/or file sizes/numbers, this could take a while.\n\nOnce the upload is finished, upload and submit the fastq submission form. If the sample alias field dows not autofill you may need to upload the sample form again."
  },
  {
    "objectID": "data-prep.html",
    "href": "data-prep.html",
    "title": "2. Data Set Preparation",
    "section": "",
    "text": "Click here for setup information.\n\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(119)\n#library(conflicted)\n#pacman::p_depends(ape, local = TRUE)  \n#pacman::p_depends_reverse(ape, local = TRUE)  \nlibrary(phyloseq); packageVersion(\"phyloseq\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, DT, microbiome, taxa, formatR,\n               captioner, reactable, downloadthis, fontawesome,\n               metacoder, ampvis2, ape, statnet.common, \n               install = FALSE, update = FALSE)\n\noptions(scipen=999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))"
  },
  {
    "objectID": "data-prep.html#prerequisites",
    "href": "data-prep.html#prerequisites",
    "title": "2. Data Set Preparation",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to run this workflow, you either need to run the corresponding DADA2 Workflow for 16S rRNA or begin with the output from that workflow, data_prep_ssu18_wf.rdata. See the Data Availability page for complete details.\nUnless otherwise noted, we primarily use phyloseq (McMurdie and Holmes 2013) in this section of the workflow to prepare the 2018 16S rRNA data set for community analyses. We prepare the data set by curating samples, removing contaminants, and creating phyloseq objects."
  },
  {
    "objectID": "data-prep.html#read-counts-assessment",
    "href": "data-prep.html#read-counts-assessment",
    "title": "2. Data Set Preparation",
    "section": "Read Counts Assessment",
    "text": "Read Counts Assessment\nBefore we begin, let’s create a summary table containing some basic sample metadata and the read count data from the DADA2 workflow. We need to inspect how total reads changed through the workflow. While we are at it, let’s create more intuitive Sample IDs. For more details on how reads changed at each step of the workflow, see the summary table at the end of the DADA2 section. Table headers are as follows:\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSample ID\nthe new sample ID based on Plot number, Depth, Treatment, Temperature, & Pair\n\n\nPlot\nthe experimental plot number\n\n\nDepth\nthe depth where the sample was collected\n\n\nTreatment\nwarm or control\n\n\nTemp\ntemperature of soil heating (0C, +4C, or +8C)\n\n\nPair\ncontrol/treatment coupling\n\n\ninput\nnumber of raw reads\n\n\nnochim\nfinal read count after removing chimeras\n\n\nRemain\npercent of reads remaining from input to nonchim\n\n\n\nFastqID\nbase name of the fastq file\n\n\n\n\n\n\n\n(16S rRNA) Table 1 | Sample summary table including read changes at start and end of DADA2 workflow.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "data-prep.html#defining-groups",
    "href": "data-prep.html#defining-groups",
    "title": "2. Data Set Preparation",
    "section": "Defining Groups",
    "text": "Defining Groups\n\nLoad the data packet produced in the final step of the DADA2 workflow. This packet (ssu18_dada2_wf.rdata) contains the ASV-by-sample table and the ASV taxonomy table.\nRename the samples so names have plot and Depth info.\nAfter we load the data packet, we next need to format sample names and define groups.\n\n\nCodeseq_table <- seq_table[order(seq_table$`root name`), ]\ntmp_names <- seq_table$`Sample_ID`\nload(\"files/dada2/rdata/ssu18_dada2_wf.rdata\")\nidentical(seq_table$`root name`, rownames(seqtab))\nrownames(seqtab) <- tmp_names\nsamples.out <- rownames(seqtab)\n\nsample_name <- substr(samples.out, 1, 999)\nplot <- substr(samples.out, 0, 3)\ndepth <- substr(samples.out, 6, 11)\ntreatment <- substr(samples.out, 13, 13)\ntemp <- substr(samples.out, 14, 14)\npair <- substr(samples.out, 15, 15)\n\n\nWe have a total of 15 samples, 5 sample pairs, from 10 plots (1 depth). There are 2 treatments corresponding to 3 different temperature regimes.\n\nAnd finally we define a sample data frame that holds the different groups we extracted from the sample names.\n\n\nCode#define a sample data frame\nsamdf <- data.frame(SamName = sample_name,\n                    PLOT = plot,\n                    DEPTH = depth,\n                    TREAT = treatment,\n                    TEMP = temp,\n                    PAIR = pair)\nrownames(samdf) <- samples.out\n\n\n\n\n\nFor example, P07_D00_010_W3D is the sample heated at +3C from plot P07, collected from a depth of 00_010cm. This sample is in group D which consists of P07_D00_010_W3D, P07_D00_010_W8D, P08_D00_010_C0D."
  },
  {
    "objectID": "data-prep.html#create-a-phyloseq-object",
    "href": "data-prep.html#create-a-phyloseq-object",
    "title": "2. Data Set Preparation",
    "section": "Create a Phyloseq Object",
    "text": "Create a Phyloseq Object\nA. The first step is to rename the amplicon sequence variants (ASVs) so the designations are a bit more user friendly. By default, DADA2 names each ASV by its unique sequence so that data can be directly compared across studies (which is great). But this convention can get cumbersome downstream, so we rename the ASVs using a simpler convention—ASV1, ASV2, ASV3, and so on.\n\n\n\n\n\n\nNote\n\n\n\nA phyloseq object contains ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications).\n\n\n\nCode# this create the phyloseq object\nps <- phyloseq(otu_table(seqtab, taxa_are_rows = FALSE),\n                   sample_data(samdf), tax_table(tax_silva))\ntax_table(ps) <- cbind(tax_table(ps),\n                           rownames(tax_table(ps)))\n\n# adding unique ASV names\ntaxa_names(ps) <- paste0(\"ASV\", seq(ntaxa(ps)))\ntax_table(ps) <- cbind(tax_table(ps),\n                           rownames(tax_table(ps)))\n\n\n\n\n[1] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" \"ASV5\" \"ASV6\"\n\n\nSo the complete data set contains 20332 ASVs. We can use the microbiome R package (Lahti, Sudarshan, et al. 2017) to get some additional summary data from the phyloseq object.\n\n\n\n\n\n\n\n\n\nMetric\nResults\n\n\n\nMin. number of reads\n25151\n\n\nMax. number of reads\n86841\n\n\nTotal number of reads\n937761\n\n\nAverage number of reads\n62517\n\n\nMedian number of reads\n62171\n\n\nTotal ASVs\n20332\n\n\nSparsity\n0.898\n\n\nAny ASVs sum to 1 or less?\nFALSE\n\n\nNumber of singleton ASVs\nNA\n\n\nPercent of ASVs that are singletons\nNA\n\n\nNumber of sample variables are:\n6 (SamName, PLOT, DEPTH, TREAT, TEMP, PAIR)\n\n\n\nB. Add two final columns containing the ASV sequences and ASV IDs. This will be useful later when trying to export a fasta file. We can also take a look at the phyloseq object.\n\ncolnames(tax_table(ps)) <- c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\",\n    \"Family\", \"Genus\", \"ASV_SEQ\", \"ASV_ID\")\nps\n\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 20332 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 20332 taxa by 8 taxonomic ranks ]\n\n\nC. Export sequence and taxonomy tables for the unadulterated phyloseq object for later use. We will use the prefix full to indicate that these are the raw outputs.\n\nwrite.table(tax_table(ps),\n            \"files/data-prep/tables/ssu18_full_tax_table.txt\",\n            sep = \"\\t\", quote = FALSE, col.names = NA)\nwrite.table(t(otu_table(ps)),\n            \"files/data-prep/tables/ssu18_full_seq_table.txt\",\n            sep = \"\\t\", quote = FALSE, col.names = NA)"
  },
  {
    "objectID": "data-prep.html#remove-contaminants-unwanted-taxa",
    "href": "data-prep.html#remove-contaminants-unwanted-taxa",
    "title": "2. Data Set Preparation",
    "section": "Remove Contaminants & Unwanted Taxa",
    "text": "Remove Contaminants & Unwanted Taxa\nLet’s see if we have any potential contaminants. We can use some inline R code to see the taxonomy table for any taxa of interest.\n\n\n\n\n\n\nNote\n\n\n\nThe code (hidden by default) is written as `r \"Chloroplast\" %in% tax_table(ps)`.\n\n\n\nAre Mitochondria present? TRUE\nAre Chloroplast present? TRUE\nAre Eukaryota present? TRUE\n\nLet’s remove these taxa—Eukaryota because we used bacterial/archaeal primers, Mitochondria because those are likely from eukaryotes, and Chloroplast because those are likely from plants. We must do each of these in turn using phyloseq and it gets a little messy.\nWhy messy? The subset_taxa command removes anything that is NA for the specified taxonomic level or above. For example, lets say you run the subset_taxa command using Family != \"Mitochondria“. Seems like you should get a phyloseq object with everything except Mitochondria. But actually the command not only gets rid of Mitochondria but everything else that has NA for Family and above. In my experience this is not well documented and I had to dig through the files to figure out what was happening.\nAnyway, to remove the taxa we do the following:\n\nSubset the taxa and generate a ps object of just the taxa of interest,\nSelect the ASV column only, turn it into a factor, and use this to remove  from the ps object.\n\n\nRemove Mitochondria ASVs\nRemember the original data set contained 20332 ASVs. Here we generate a file with mitochondria ASVs only.\n\nMT1 <- subset_taxa(ps, Family == \"Mitochondria\")\nMT1 <-  as(tax_table(MT1), \"matrix\")\nMT1 <- MT1[, 8]\nMT1df <- as.factor(MT1)\ngoodTaxa <- setdiff(taxa_names(ps), MT1df)\nps_no_mito <- prune_taxa(goodTaxa, ps)\nps_no_mito\n\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 20291 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 20291 taxa by 8 taxonomic ranks ]\n\n\nLooks like this removed 41 Mitochondria ASVs. We will duplicate the code block to remove other groups.\nRemove Chloroplast ASVs\nAnd again with Chloroplast ASVs only.\n\nCH1 <- subset_taxa(ps_no_mito, Order == \"Chloroplast\")\nCH1 <-  as(tax_table(CH1), \"matrix\")\nCH1 <- CH1[, 8]\nCH1df <- as.factor(CH1)\ngoodTaxa <- setdiff(taxa_names(ps_no_mito), CH1df)\nps_no_chloro <- prune_taxa(goodTaxa, ps_no_mito)\nps_no_chloro\n\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 20274 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 20274 taxa by 8 taxonomic ranks ]\n\n\nThe code removed an additional 17 Chloroplast ASVs.\nRemove Eukaryota ASVs\nAnd again with Eukaryota ASVs only.\n\nEU1 <- subset_taxa(ps_no_chloro, Kingdom == \"Eukaryota\")\nEU1 <-  as(tax_table(EU1), \"matrix\")\nEU1 <- EU1[, 8]\nEU1df <- as.factor(EU1)\ngoodTaxa <- setdiff(taxa_names(ps_no_chloro), EU1df)\nps_no_euk <- prune_taxa(goodTaxa, ps_no_chloro)\nps_no_euk\n\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 20223 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 20223 taxa by 8 taxonomic ranks ]\n\n\nThe code removed an additional 51 Eukaryota ASVs from the ps object.\nRemove any Kingdom NAs\nHere we can just use the straight up subset_taxa command since we do not need to worry about any ranks above Kingdom also being removed.\n\nps_filt <- subset_taxa(ps_no_euk, !is.na(Kingdom))\n\n\nps_work_o <- ps_filt\n\nThe code eliminated an additional 50 Kingdom level NA ASVs from the phyloseq object."
  },
  {
    "objectID": "data-prep.html#rename-na-taxonomic-ranks",
    "href": "data-prep.html#rename-na-taxonomic-ranks",
    "title": "2. Data Set Preparation",
    "section": "Rename NA taxonomic ranks",
    "text": "Rename NA taxonomic ranks\nPhyloseq has an odd way of dealing with taxonomic ranks that have no value—in other words, NA in the tax table. The first thing we are going to do before moving forward is to change all of the NAs to have a value of the next highest classified rank. For example, ASV26 is not classified at the Genus level but is at Family level (Xanthobacteraceae). So we change the Genus name to Family_Xanthobacteraceae. The code for comes from these two posts on the phyloseq GitHub, both by MSMortensen: issue #850 and issue #990.\n\nOne thing this code does is reassign the functions class and order to taxonomic ranks. This can cause issues if you need these functions.\n\nSo you need to run something like this rm(class, order, phylum, kingdom) at the end of the code to remove these as variables. For now, I have not come up with a better solution.\n\nCodetax.clean <- data.frame(tax_table(ps_filt))\nfor (i in 1:6){ tax.clean[,i] <- as.character(tax.clean[,i])}\ntax.clean[is.na(tax.clean)] <- \"\"\n\nfor (i in 1:nrow(tax.clean)){\n    if (tax.clean[i,2] == \"\"){\n        kingdom <- base::paste(\"k_\", tax.clean[i,1], sep = \"\")\n        tax.clean[i, 2:6] <- kingdom\n    } else if (tax.clean[i,3] == \"\"){\n        phylum <- base::paste(\"p_\", tax.clean[i,2], sep = \"\")\n        tax.clean[i, 3:6] <- phylum\n    } else if (tax.clean[i,4] == \"\"){\n        class <- base::paste(\"c_\", tax.clean[i,3], sep = \"\")\n        tax.clean[i, 4:6] <- class\n    } else if (tax.clean[i,5] == \"\"){\n        order <- base::paste(\"o_\", tax.clean[i,4], sep = \"\")\n        tax.clean[i, 5:6] <- order\n    } else if (tax.clean[i,6] == \"\"){\n        tax.clean$Genus[i] <- base::paste(\"f\",tax.clean$Family[i], sep = \"_\")\n        }\n}\ntax_table(ps_filt) <- as.matrix(tax.clean)\nrank_names(ps_filt)\nrm(class, order, phylum, kingdom)\n\n\nStill the same ranks. That’s good. What about the new groups? Let’s take a peak at some families.\n\nhead(get_taxa_unique(ps_filt, \"Family\"), 16)\n\n [1] \"Planococcaceae\"         \"Chthoniobacteraceae\"    \"Nitrososphaeraceae\"    \n [4] \"p_RCP2-54\"              \"o_Rokubacteriales\"      \"Methyloligellaceae\"    \n [7] \"Nitrospiraceae\"         \"Xanthobacteraceae\"      \"o_Vicinamibacterales\"  \n[10] \"Vicinamibacteraceae\"    \"c_Subgroup_5\"           \"Xiphinematobacteraceae\"\n[13] \"Entotheonellaceae\"      \"Chitinophagaceae\"       \"Nitrosomonadaceae\"     \n[16] \"o_Acidobacteriales\"    \n\n\nNice. Bye-bye NA\n\nNote. The original code mentioned above is written for data sets that have species-level designations.\n\nSince this data set does not contain species, I modified the code to stop at the genus level. If your data set has species, my modifications will not work for you.\nFinally, we rename the ps object. This is now our working data set.\n\nps_work <- ps_filt"
  },
  {
    "objectID": "data-prep.html#add-phylogenetic-tree",
    "href": "data-prep.html#add-phylogenetic-tree",
    "title": "2. Data Set Preparation",
    "section": "Add Phylogenetic Tree",
    "text": "Add Phylogenetic Tree\nOne final task is to add a phylogenetic tree to the phyloseq object.\n\nps_work_tree <- rtree(ntaxa(ps_work), rooted = TRUE,\n                      tip.label = taxa_names(ps_work))\nps_work <- merge_phyloseq(ps_work,\n                          sample_data,\n                          ps_work_tree)\nps_work_o_tree <- rtree(ntaxa(ps_work_o), rooted = TRUE,\n                      tip.label = taxa_names(ps_work_o))\nps_work_o <- merge_phyloseq(ps_work_o,\n                          sample_data,\n                          ps_work_o_tree)"
  },
  {
    "objectID": "data-prep.html#data-set-summary",
    "href": "data-prep.html#data-set-summary",
    "title": "2. Data Set Preparation",
    "section": "Data Set Summary",
    "text": "Data Set Summary\nNow that we have removed the NC and sample with zero reads, we can summarize the data set. Let’s start with the in our working phyloseq object using the summarize_phyloseq from the microbiome R package (Lahti, Sudarshan, et al. 2017) as we did above.\n\n\n\n\n\n\n\n\n\nMetric\nResults\n\n\n\nMin. number of reads\n25088\n\n\nMax. number of reads\n86600\n\n\nTotal number of reads\n936640\n\n\nAverage number of reads\n62443\n\n\nMedian number of reads\n62117\n\n\nTotal ASVs\n20173\n\n\nSparsity\n0.891\n\n\nAny ASVs sum to 1 or less?\nFALSE\n\n\nNumber of singleton ASVs\nNA\n\n\nPercent of ASVs that are singletons\nNA\n\n\nNumber of sample variables are:\n6 (SamName, PLOT, DEPTH, TREAT, TEMP, PAIR)\n\n\n\nWe can also generate a summary table of total reads & ASVs for each sample. You can sort the table, download a copy, or filter by search term. Here is the code to generate the data for the table. First, we create data frames that hold total reads and ASVs for each sample.\n\nCodetotal_reads <- sample_sums(ps_work)\ntotal_reads <- as.data.frame(total_reads, make.names = TRUE)\ntotal_reads <- total_reads %>% rownames_to_column(\"Sample_ID\")\n\ntotal_asvs <- estimate_richness(ps_work,\n                                measures = \"Observed\")\ntotal_asvs <- total_asvs %>% rownames_to_column(\"Sample_ID\")\ntotal_asvs$Sample_ID <- gsub('\\\\.', '-', total_asvs$Sample_ID)\n\n\nAnd then we merge these two data frames with the sample data frame.\n\nCodesam_details <- samdf\nrownames(sam_details) <- NULL\ncolnames(sam_details) <- c(\"Sample_ID\", \"Plot\", \"Depth\",\n                                  \"Treatment\", \"Temp\", \"Pair\")\ntmp_tab1 <- seq_table\ntmp_tab1[, c(2:7,11)] <- NULL\ntmp_tab2 <- dplyr::left_join(sam_details, total_reads) %>%\n                left_join(., total_asvs) %>%\n                left_join(., tmp_tab1)\n\nreads_lost <- tmp_tab2$`final reads` - tmp_tab2$total_reads \nasvs_lost <- tmp_tab2$`no. ASVs` - tmp_tab2$Observed \ntmp_tab2$reads_lost <- reads_lost\ntmp_tab2$asvs_lost <- asvs_lost\n\nseq_table2 <- tmp_tab2[, c(1:8,12,13)]\ncolnames(seq_table2) <- c(\"sample_id\", \"PLOT\", \"DEPTH\",\n                          \"TREAT\", \"TEMP\", \"PAIR\", \"total_reads\",\n                          \"total_ASVs\", \"reads_lost\", \"ASVs_lost\")\n\n\n(16S rRNA) Table 2 | Sample summary table showing the final number of reads and ASVs (per sample) as well as the number of reads/ASVs lost during curation.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "data-prep.html#review",
    "href": "data-prep.html#review",
    "title": "2. Data Set Preparation",
    "section": "Review",
    "text": "Review\nWe have a few different options to play with. At this point it is difficult to say which we will use or whether additional objects need to be created. Here is a summary of the objects we have.\nThe full phyloseq object before removing contaminants.\n\nps\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 20332 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 20332 taxa by 8 taxonomic ranks ]\n\nhead(get_taxa_unique(ps, \"Family\"), 16)\n\n [1] \"Planococcaceae\"         \"Chthoniobacteraceae\"    \"Nitrososphaeraceae\"    \n [4] NA                       \"Methyloligellaceae\"     \"Nitrospiraceae\"        \n [7] \"Xanthobacteraceae\"      \"Vicinamibacteraceae\"    \"Xiphinematobacteraceae\"\n[10] \"Entotheonellaceae\"      \"Chitinophagaceae\"       \"Nitrosomonadaceae\"     \n[13] \"B1-7BS\"                 \"WX65\"                   \"Gaiellaceae\"           \n[16] \"Steroidobacteraceae\"   \n\n\nThe phyloseq object before changing NA ranks.\n\nps_work_o\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 20173 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 20173 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 20173 tips and 20172 internal nodes ]\n\nhead(get_taxa_unique(ps_work_o, \"Family\"), 16)\n\n [1] \"A4b\"                            \"Burkholderiaceae\"              \n [3] \"Peptococcaceae\"                 NA                              \n [5] \"Polyangiaceae\"                  \"Pedosphaeraceae\"               \n [7] \"AKYH767\"                        \"Micromonosporaceae\"            \n [9] \"Acidobacteriaceae_(Subgroup_1)\" \"Pyrinomonadaceae\"              \n[11] \"Clostridiaceae\"                 \"Myxococcaceae\"                 \n[13] \"Hyphomonadaceae\"                \"Roseiflexaceae\"                \n[15] \"type_III\"                       \"Ktedonobacteraceae\"            \n\n\nThe phyloseq object after changing NA ranks.\n\nps_work\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 20173 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 20173 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 20173 tips and 20172 internal nodes ]\n\nhead(get_taxa_unique(ps_work, \"Family\"), 16)\n\n [1] \"c_Subgroup_22\"         \"c_OM190\"               \"A4b\"                  \n [4] \"Diplorickettsiaceae\"   \"k_Bacteria\"            \"Gemmataceae\"          \n [7] \"c_Gammaproteobacteria\" \"Microscillaceae\"       \"Entotheonellaceae\"    \n[10] \"Syntrophaceae\"         \"p_WS2\"                 \"Verrucomicrobiaceae\"  \n[13] \"Pyrinomonadaceae\"      \"Caulobacteraceae\"      \"Parachlamydiaceae\"    \n[16] \"c_Phycisphaerae\""
  },
  {
    "objectID": "data-prep.html#final-steps",
    "href": "data-prep.html#final-steps",
    "title": "2. Data Set Preparation",
    "section": "Final Steps",
    "text": "Final Steps\nThe last thing tasks to complete are to a) save copies of the taxonomy and sequence tables and b) save an image of the workflow for the next section.\n\nwrite.table(tax_table(ps_work),\n            \"files/data-prep/tables/ssu18_work_tax_table.txt\",\n            sep = \"\\t\", quote = FALSE, col.names = NA)\nwrite.table(t(otu_table(ps_work)),\n            \"files/data-prep/tables/ssu18_work_seq_table.txt\",\n            sep = \"\\t\", quote = FALSE, col.names = NA)"
  },
  {
    "objectID": "data-prep.html#prerequisites-1",
    "href": "data-prep.html#prerequisites-1",
    "title": "2. Data Set Preparation",
    "section": "Prerequisites",
    "text": "Prerequisites\nIn order to run this workflow, you either need to run the corresponding DADA2 Workflow for 2018 ITS or begin with the output from that workflow, data_prep_its18_wf.rdata. See the Data Availability page for complete details.\nUnless otherwise noted, we primarily use phyloseq (McMurdie and Holmes 2013) in this section of the workflow to prepare the 2018 ITS data set for community analyses. We prepare the data set by curating samples, removing contaminants, and creating phyloseq objects."
  },
  {
    "objectID": "data-prep.html#read-counts-assessment-1",
    "href": "data-prep.html#read-counts-assessment-1",
    "title": "2. Data Set Preparation",
    "section": "Read Counts Assessment",
    "text": "Read Counts Assessment\nBefore we begin, let’s create a summary table containing some basic sample metadata and the read count data from the DADA2 workflow. We need to inspect how total reads changed from to beginning to the end of the workflow. While we are at it, let’s create more intuitive Sample IDs. For more details on how reads changed at each step of the workflow, see the summary table at the end of the DADA2 section.\nTable headers are as follows:\n\n\n\n\n\n\nHeader\nDescription\n\n\n\nSample ID\nthe new sample ID based on Plot number, Depth, Treatment, Temperature, & Pair\n\n\nPlot\nthe experimental plot number\n\n\nDepth\nthe depth where the sample was collected\n\n\nTreatment\nwarm or control\n\n\nTemp\ntemperature of soil heating (0C, +4C, or +8C)\n\n\nPair\ncontrol/treatment coupling\n\n\ninput\nnumber of raw reads\n\n\nnochim\nfinal read count after removing chimeras\n\n\nRemain\npercent of reads remaining from input to nonchim\n\n\n\nFastqID\nbase name of the fastq file\n\n\n\n\n\n\n(ITS) Table 1 | Sample summary table including read changes at start and end of DADA2 workflow.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\nThe following samples have fewer than 1000 reads at the end of the workflow:\nP05_D00_010_W3C, P09_D00_010_W8E.\nWe can also plot the final read count by the temperature where the sample was collected."
  },
  {
    "objectID": "data-prep.html#defining-groups-1",
    "href": "data-prep.html#defining-groups-1",
    "title": "2. Data Set Preparation",
    "section": "Defining Groups",
    "text": "Defining Groups\n\nLoad the data packet produced in the final step of the DADA2 workflow. This packet (its18_dada2_wf.rdata) contains the ASV-by-sample table and the ASV taxonomy table.\nRename the samples so names have plot and Depth info. For convenience, we renamed any Negative Controls (NC) to P00_D00-000_NNN.\nAfter we load the data packet we next need to format sample names and define groups.\n\n\nCodeseq_table <- seq_table[order(seq_table$`root name`), ]\ntmp_names <- seq_table$`Sample_ID`\nload(\"files/dada2/rdata/its18_dada2_wf.rdata\")\nidentical(seq_table$`root name`, rownames(seqtab))\nrownames(seqtab) <- tmp_names\nsamples.out <- rownames(seqtab)\nsample_name <- substr(samples.out, 1, 999)\nplot <- substr(samples.out, 0, 3)\ndepth <- substr(samples.out, 6, 11)\ntreatment <- substr(samples.out, 13, 13)\ntemp <- substr(samples.out, 14, 14)\npair <- substr(samples.out, 15, 15)\n\n\nWe have a total of 15 samples, 5 sample pairs, from 10 plots (1 depth). There are 2 treatments corresponding to 3 different temperature regimes.\n\nAnd finally we define a sample data frame that holds the different groups we extracted from the sample names.\n\n\nCodesamdf <- data.frame(SamName = sample_name,\n                    PLOT = plot,\n                    DEPTH = depth,\n                    TREAT = treatment,\n                    TEMP = temp,\n                    PAIR = pair)\nrownames(samdf) <- samples.out\n\n\n\n\n\nFor example, P08_D00_010_C0D is the sample heated at +0C from plot P08, collected from a depth of 00_010cm. This sample is in group D which consists of P07_D00_010_W3D, P07_D00_010_W8D, P08_D00_010_C0D."
  },
  {
    "objectID": "data-prep.html#create-a-phyloseq-object-1",
    "href": "data-prep.html#create-a-phyloseq-object-1",
    "title": "2. Data Set Preparation",
    "section": "Create a Phyloseq Object",
    "text": "Create a Phyloseq Object\nFix Taxon Names\nThe UNITE database we used for classification adds a little prefix to every rank call, like k__ for Kingdom, o__ for Order, etc. To keep in step with our other analyses, we will remove these tags. Of course, we also make a ps object with the original names just in case. At this point we will also remove the Species rank since it is essentially useless.\n\ntax_mod_o <- data.frame(tax)\ntax_mod_o$Species <- NULL\ntax_mod_o <- as.matrix(tax_mod_o)\ntax_mod <- sub(\"k__\", \"\", tax_mod_o)\ntax_mod <- sub(\"p__\", \"\", tax_mod)\ntax_mod <- sub(\"c__\", \"\", tax_mod)\ntax_mod <- sub(\"o__\", \"\", tax_mod)\ntax_mod <- sub(\"f__\", \"\", tax_mod)\ntax_mod <- sub(\"g__\", \"\", tax_mod)\n\nA. The first step is to rename the amplicon sequence variants (ASVs) so the designations are a bit more user friendly. By default, DADA2 names each ASV by its unique sequence so that data can be directly compared across studies (which is great). But this convention can get cumbersome downstream, so we rename the ASVs using a simpler convention—ASV1, ASV2, ASV3, and so on.\n\n\n\n\n\n\nNote\n\n\n\nA phyloseq object contains ASV table (taxa abundances), sample metadata, and taxonomy table (mapping between ASVs and higher-level taxonomic classifications).\n\n\n\nCode# this create the phyloseq object\nps <- phyloseq(otu_table(seqtab, taxa_are_rows = FALSE),\n                   sample_data(samdf), tax_table(tax_mod))\ntax_table(ps) <- cbind(tax_table(ps),\n                           rownames(tax_table(ps)))\n\n# adding unique ASV names\ntaxa_names(ps) <- paste0(\"ASV\", seq(ntaxa(ps)))\ntax_table(ps) <- cbind(tax_table(ps),\n                           rownames(tax_table(ps)))\nhead(taxa_names(ps))\n\n\n\n\n\n\n\n[1] \"ASV1\" \"ASV2\" \"ASV3\" \"ASV4\" \"ASV5\" \"ASV6\"\n\n\nSo the complete data set contains 3357 ASVs. We can also use the microbiome R package (Lahti, Sudarshan, et al. 2017) to get some additional summary data from the phyloseq object.\n\n\n\n\n\n\n\n\n\nMetric\nResults\n\n\n\nMin. number of reads\n80\n\n\nMax. number of reads\n64636\n\n\nTotal number of reads\n491143\n\n\nAverage number of reads\n32743\n\n\nMedian number of reads\n36063\n\n\nTotal ASVs\n3357\n\n\nSparsity\n0.802\n\n\nAny ASVs sum to 1 or less?\nTRUE\n\n\nNumber of singleton ASVs\n3\n\n\nPercent of ASVs that are singletons\n0.089\n\n\nNumber of sample variables are:\n6 (SamName, PLOT, DEPTH, TREAT, TEMP, PAIR)\n\n\n\nB. Add two final columns containing the ASV sequences and ASV IDs. This will be useful later when trying to export a fasta file. We can also take a look at the phyloseq object.\n\ncolnames(tax_table(ps)) <- c(\"Kingdom\", \"Phylum\", \"Class\", \"Order\",\n    \"Family\", \"Genus\", \"ASV_SEQ\", \"ASV_ID\")\nrank_names(ps)\n\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3357 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3357 taxa by 8 taxonomic ranks ]\n\n\n\n\n\nC. Export sequence and taxonomy tables for the unadulterated phyloseq object for later use. We will use the prefix full to indicate that these are the raw outputs.\n\nCodewrite.table(tax_table(ps),\n            \"files/data-prep/tables/its18_full_tax_table.txt\",\n            sep=\"\\t\", quote = FALSE, col.names=NA)\nwrite.table(t(otu_table(ps)),\n            \"files/data-prep/tables/its18_full_seq_table.txt\",\n            sep=\"\\t\", quote = FALSE, col.names=NA)\n\n\n\n\n\nAt this point we would normally remove unwanted taxa and/or negative controls. Since we do not have any of these for this dataset, we are going to save the phyloseq object with a new name. That way, if we do decide later on to remove some taxa it will be easier to plug in the code here and then continue with the rest of the workflow.\n\nps_filt <- ps"
  },
  {
    "objectID": "data-prep.html#remove-low-count-samples",
    "href": "data-prep.html#remove-low-count-samples",
    "title": "2. Data Set Preparation",
    "section": "Remove Low-Count Samples",
    "text": "Remove Low-Count Samples\nNext, we can remove samples with really low read counts, say less than 500 reads.\nps_filt_no_low <- prune_samples(sample_sums(ps_filt) > 500, ps_filt)\n\nps_filt_no_low\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3357 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3357 taxa by 8 taxonomic ranks ]\n\n\nWe lost 2 sample(s). After removing samples we need to check whether any ASVs ended up with no reads.\n\nno_reads <- taxa_sums(ps_filt_no_low) == 0\n\nAnd we lost 2 ASV(s). So now we must remove these from the ps object.\n\nps_filt_no_nc <- prune_taxa(taxa_sums(ps_filt_no_low) > 0, ps_filt_no_low)\n\n\nps_filt_no_nc\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3355 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3355 taxa by 8 taxonomic ranks ]"
  },
  {
    "objectID": "data-prep.html#rename-na-taxonomic-ranks-1",
    "href": "data-prep.html#rename-na-taxonomic-ranks-1",
    "title": "2. Data Set Preparation",
    "section": "Rename NA taxonomic ranks",
    "text": "Rename NA taxonomic ranks\nPhyloseq has an odd way of dealing with taxonomic ranks that have no value—in other words, NA in the tax table. The first thing we are going to do before moving forward is to change all of the NAs to have a value of the next highest classified rank. For example, ASV4 is not classified at the Genus level but is at Family level (Nitrososphaeraceae). So we change the Genus name to Family_Nitrososphaeraceae. The code for comes from these two posts on the phyloseq GitHub, both by MSMortensen: issue #850 and issue #990.\n\nOne thing this code does is reassign the functions class and order to taxonomic ranks. This can cause issues if you need these functions.\n\nSo you need to run something like this rm(class, order, phylum, kingdom) at the end of the code to remove these as variables. For now, I have not come up with a better solution.\nBefore doing that, we will save a copy of the phyloseq object with the original taxa names.\n\nps_work_o <- ps_filt_no_nc\n\n\nCodetax.clean <- data.frame(tax_table(ps_filt_no_nc))\nfor (i in 1:6){ tax.clean[,i] <- as.character(tax.clean[,i])}\ntax.clean[is.na(tax.clean)] <- \"\"\n\nfor (i in 1:nrow(tax.clean)){\n    if (tax.clean[i,2] == \"\"){\n        kingdom <- base::paste(\"k_\", tax.clean[i,1], sep = \"\")\n        tax.clean[i, 2:6] <- kingdom\n    } else if (tax.clean[i,3] == \"\"){\n        phylum <- base::paste(\"p_\", tax.clean[i,2], sep = \"\")\n        tax.clean[i, 3:6] <- phylum\n    } else if (tax.clean[i,4] == \"\"){\n        class <- base::paste(\"c_\", tax.clean[i,3], sep = \"\")\n        tax.clean[i, 4:6] <- class\n    } else if (tax.clean[i,5] == \"\"){\n        order <- base::paste(\"o_\", tax.clean[i,4], sep = \"\")\n        tax.clean[i, 5:6] <- order\n    } else if (tax.clean[i,6] == \"\"){\n        tax.clean$Genus[i] <- base::paste(\"f\",tax.clean$Family[i], sep = \"_\")\n        }\n}\ntax_table(ps_filt_no_nc) <- as.matrix(tax.clean)\nrank_names(ps_filt_no_nc)\nrm(class, order, phylum, kingdom)\n\n\nStill the same ranks. That’s good. What about the new groups? Let’s take a peak at some families.\n\nhead(get_taxa_unique(ps_filt_no_nc, \"Family\"), 16)\n\n [1] \"Hygrophoraceae\"         \"Mycosphaerellaceae\"     \"p_Ascomycota\"          \n [4] \"Trichosporonaceae\"      \"c_Agaricomycetes\"       \"Archaeorhizomycetaceae\"\n [7] \"Chaetomellaceae\"        \"Geastraceae\"            \"k_Fungi\"               \n[10] \"Metschnikowiaceae\"      \"Clavariaceae\"           \"o_Agaricales\"          \n[13] \"Ascobolaceae\"           \"Trichocomaceae\"         \"Piptocephalidaceae\"    \n[16] \"p_Chytridiomycota\"     \n\n\nNice. Adios NA.\n\nNote. The original code is written for data sets that have species-level designations.\n\nSince this data set does not contain species, I modified the code to stop at the genus level. If your data set has species, my modifications will not work for you.\nFinally, we rename the ps object. This is now our working data set.\n\nps_work <- ps_filt_no_nc"
  },
  {
    "objectID": "data-prep.html#data-set-summary-1",
    "href": "data-prep.html#data-set-summary-1",
    "title": "2. Data Set Preparation",
    "section": "Data Set Summary",
    "text": "Data Set Summary\nNow that we have removed the NC and samples with low reads, we can summarize the data in our working phyloseq object. Let’s start with the data set as a whole using the summarize_phyloseq from the microbiome R package (Lahti, Sudarshan, et al. 2017) as we did above.\n\n\n\n\n\n\n\n\n\nMetric\nResults\n\n\n\nMin. number of reads\n9172\n\n\nMax. number of reads\n64636\n\n\nTotal number of reads\n490767\n\n\nAverage number of reads\n37751\n\n\nMedian number of reads\n38371\n\n\nTotal ASVs\n3355\n\n\nSparsity\n0.671\n\n\nAny ASVs sum to 1 or less?\nTRUE\n\n\nNumber of singleton ASVs\n5\n\n\nPercent of ASVs that are singletons\n0.149\n\n\nNumber of sample variables are:\n6 (SamName, PLOT, DEPTH, TREAT, TEMP, PAIR)\n\n\n\nWe can also generate a summary table of total reads & ASVs for each sample. You can sort the table, download a copy, or filter by search term. Here is the code to generate the data for the table. First, we create data frames that hold total reads and ASVs for each sample.\n\nCodetotal_reads <- sample_sums(ps_work)\ntotal_reads <- as.data.frame(total_reads, make.names = TRUE)\ntotal_reads <- total_reads %>% rownames_to_column(\"Sample_ID\")\n\ntotal_asvs <- estimate_richness(ps_work, measures = \"Observed\")\ntotal_asvs <- total_asvs %>% rownames_to_column(\"Sample_ID\")\ntotal_asvs$Sample_ID <- gsub('\\\\.', '-', total_asvs$Sample_ID)\n\n\nAnd then we merge these two data frames with the sample data frame.\n\nCoderm(list = ls(pattern = \"tmp_\"))\n\nsam_details <- samdf\nrownames(sam_details) <- NULL\ncolnames(sam_details) <- c(\"Sample_ID\", \"Plot\", \"Depth\",\n                                  \"Treatment\", \"Temp\", \"Pair\")\ntmp_tab1 <- seq_table\ntmp_tab1[, c(2:7,11)] <- NULL\ntmp_tab2 <- dplyr::left_join(sam_details, total_reads) %>%\n                left_join(., total_asvs) %>%\n                left_join(., tmp_tab1)\n\nreads_lost <- tmp_tab2$`final reads` - tmp_tab2$total_reads \nasvs_lost <- tmp_tab2$`no. ASVs` - tmp_tab2$Observed \ntmp_tab2$reads_lost <- reads_lost\ntmp_tab2$asvs_lost <- asvs_lost\n\nseq_table2 <- tmp_tab2[, c(1:8,12,13)]\ncolnames(seq_table2) <- c(\"sample_id\", \"PLOT\", \"DEPTH\",\n                          \"TREAT\", \"TEMP\", \"PAIR\", \"total_reads\",\n                          \"total_ASVs\", \"reads_lost\", \"ASVs_lost\")\n\n\n(ITS) Table 2 | Sample summary table showing the final number of reads and ASVs (per sample) as well as the number of reads/ASVs lost during curation.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "data-prep.html#review-1",
    "href": "data-prep.html#review-1",
    "title": "2. Data Set Preparation",
    "section": "Review",
    "text": "Review\nWe have a few different options to play with. At this point it is difficult to say which we will use or whether additional objects need to be created. Here is a summary of the objects we have.\nThe full phyloseq object before removing contaminants.\n\nps\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3357 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3357 taxa by 8 taxonomic ranks ]\n\nhead(get_taxa_unique(ps, \"Family\"), 16)\n\n [1] \"Hygrophoraceae\"                  \"Mycosphaerellaceae\"             \n [3] NA                                \"Trichosporonaceae\"              \n [5] \"Archaeorhizomycetaceae\"          \"Chaetomellaceae\"                \n [7] \"Geastraceae\"                     \"Metschnikowiaceae\"              \n [9] \"Clavariaceae\"                    \"Ascobolaceae\"                   \n[11] \"Trichocomaceae\"                  \"Piptocephalidaceae\"             \n[13] \"Mortierellaceae\"                 \"Pleosporales_fam_Incertae_sedis\"\n[15] \"Glomeraceae\"                     \"Ceratobasidiaceae\"              \n\n\nThe phyloseq object before removing the prefix of taxon ranks. This still has low coverage samples.\n\nps_o\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3357 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3357 taxa by 8 taxonomic ranks ]\n\nhead(get_taxa_unique(ps_o, \"Family\"), 16)\n\n [1] \"f__Hygrophoraceae\"                  \"f__Mycosphaerellaceae\"             \n [3] NA                                   \"f__Trichosporonaceae\"              \n [5] \"f__Archaeorhizomycetaceae\"          \"f__Chaetomellaceae\"                \n [7] \"f__Geastraceae\"                     \"f__Metschnikowiaceae\"              \n [9] \"f__Clavariaceae\"                    \"f__Ascobolaceae\"                   \n[11] \"f__Trichocomaceae\"                  \"f__Piptocephalidaceae\"             \n[13] \"f__Mortierellaceae\"                 \"f__Pleosporales_fam_Incertae_sedis\"\n[15] \"f__Glomeraceae\"                     \"f__Ceratobasidiaceae\"              \n\n\nThe phyloseq object before changing NA ranks.\n\nps_work_o\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3355 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3355 taxa by 8 taxonomic ranks ]\n\nhead(get_taxa_unique(ps_work_o, \"Family\"), 16)\n\n [1] \"Hygrophoraceae\"                  \"Mycosphaerellaceae\"             \n [3] NA                                \"Trichosporonaceae\"              \n [5] \"Archaeorhizomycetaceae\"          \"Chaetomellaceae\"                \n [7] \"Geastraceae\"                     \"Metschnikowiaceae\"              \n [9] \"Clavariaceae\"                    \"Ascobolaceae\"                   \n[11] \"Trichocomaceae\"                  \"Piptocephalidaceae\"             \n[13] \"Mortierellaceae\"                 \"Pleosporales_fam_Incertae_sedis\"\n[15] \"Glomeraceae\"                     \"Ceratobasidiaceae\"              \n\n\nThe phyloseq object after changing NA ranks.\n\nps_work\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3355 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3355 taxa by 8 taxonomic ranks ]\n\nhead(get_taxa_unique(ps_work, \"Family\"), 16)\n\n [1] \"Hygrophoraceae\"         \"Mycosphaerellaceae\"     \"p_Ascomycota\"          \n [4] \"Trichosporonaceae\"      \"c_Agaricomycetes\"       \"Archaeorhizomycetaceae\"\n [7] \"Chaetomellaceae\"        \"Geastraceae\"            \"k_Fungi\"               \n[10] \"Metschnikowiaceae\"      \"Clavariaceae\"           \"o_Agaricales\"          \n[13] \"Ascobolaceae\"           \"Trichocomaceae\"         \"Piptocephalidaceae\"    \n[16] \"p_Chytridiomycota\""
  },
  {
    "objectID": "data-prep.html#final-steps-1",
    "href": "data-prep.html#final-steps-1",
    "title": "2. Data Set Preparation",
    "section": "Final Steps",
    "text": "Final Steps\nThe last thing tasks to complete are to a) save copies of the taxonomy and sequence tables and b) save an image of the workflow for the next part of the workflow.\n\nCodewrite.table(tax_table(ps_work),\n            \"files/data-prep/tables/its18_work_tax_table.txt\",\n            sep=\"\\t\", quote = FALSE, col.names=NA)\nwrite.table(t(otu_table(ps_work)),\n            \"files/data-prep/tables/its18_work_seq_table.txt\",\n            sep=\"\\t\", quote = FALSE, col.names=NA)"
  },
  {
    "objectID": "data-prep.html#data-availability",
    "href": "data-prep.html#data-availability",
    "title": "2. Data Set Preparation",
    "section": "Data Availability",
    "text": "Data Availability\nData generated in this workflow and the .Rdata need to run the workflow can be accessed on figshare at 10.25573/data.14690739."
  },
  {
    "objectID": "data-prep.html#last-updated-on",
    "href": "data-prep.html#last-updated-on",
    "title": "2. Data Set Preparation",
    "section": "Last updated on",
    "text": "Last updated on\n\n\n[1] \"2022-06-19 13:17:22 EST\""
  },
  {
    "objectID": "filtering.html",
    "href": "filtering.html",
    "title": "3. Filtering",
    "section": "",
    "text": "Click here for setup information.\n\nremove(list = ls())\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(119)\n\n#library(conflicted)\n#pacman::p_depends(PERFect, local = TRUE)  \n#pacman::p_depends_reverse(PERFect, local = TRUE)  \nlibrary(phyloseq); packageVersion(\"phyloseq\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, patchwork, \n               agricolae, labdsv, naniar, PERFect, pime, ape, gdata,\n               pairwiseAdonis, microbiome, seqRFLP, microbiomeMarker, \n               reactable, downloadthis, captioner, jamba, \n               install = FALSE, update = FALSE)\n\noptions(scipen=999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))\nsource(file.path(\"assets\", \"functions.R\"))\nIn Part A, we apply arbitrary filtering to the 16S rRNA and ITS data sets. In Part B we use PERFect (PERmutation Filtering test for microbiome data) (Smirnova, Huzurbazar, and Jafari 2019) to filter the data sets. And in Part C of this workflow, we use PIME (Prevalence Interval for Microbiome Evaluation) (Roesch et al. 2020) to filter the FULL 16S rRNA and ITS data sets."
  },
  {
    "objectID": "filtering.html#workflow-input",
    "href": "filtering.html#workflow-input",
    "title": "3. Filtering",
    "section": "Workflow Input",
    "text": "Workflow Input\nFiles needed to run this workflow can be downloaded from figshare."
  },
  {
    "objectID": "filtering.html#s-rrna",
    "href": "filtering.html#s-rrna",
    "title": "3. Filtering",
    "section": "16S rRNA",
    "text": "16S rRNA\n(16S rRNA) Table 1 | Summary of Arbitrary, PERfect, and PIME filtering.\n\n\n\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(16S rRNA) Table 2 | Sample summary showing the number of reads and ASVs after Arbitrary, PERfect, and PIME filtering.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "filtering.html#its",
    "href": "filtering.html#its",
    "title": "3. Filtering",
    "section": "ITS",
    "text": "ITS\n(ITS) Table 1 | Summary of Arbitrary, PERfect, and PIME filtering.\n\n\n\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(ITS) Table 2 | Sample summary showing the number of reads and ASVs after Arbitrary, PERfect, and PIME filtering.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "filtering.html#s-rrna-1",
    "href": "filtering.html#s-rrna-1",
    "title": "3. Filtering",
    "section": "16S rRNA",
    "text": "16S rRNA\n\nCodetmp_low_count <- phyloseq::genefilter_sample(\n                                         ssu18_ps_work, \n                                         filterfun_sample(function(x) x >= 5), \n                                         A = 0.2 * nsamples(ssu18_ps_work))\ntmp_low_count <- phyloseq::prune_taxa(tmp_low_count, ssu18_ps_work)\ntmp_low_var <- phyloseq::filter_taxa(tmp_low_count, \n                                     function(x) var(x) > 0.2, prune = TRUE)\nssu18_ps_filt <- tmp_low_var\nssu18_ps_filt@phy_tree <- NULL\ntmp_tree <- rtree(ntaxa(ssu18_ps_filt), rooted = TRUE,\n                      tip.label = taxa_names(ssu18_ps_filt))\nssu18_ps_filt <- merge_phyloseq(ssu18_ps_filt,\n                          sample_data,\n                          tmp_tree)\n\nrm(list = ls(pattern = \"tmp_\"))\nssu18_ps_filt\n\n\nHere is the filtered 16S rRNA phyloseq object.\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1822 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1822 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 1822 tips and 1821 internal nodes ]\n\n\n\n\n\n(16S rRNA) Table 3 | Summary of arbitrary filtering where ASVs represented by fewer than 5 reads, present in less than 20% of samples, and/or a variance less than 0.2, were removed.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nWe can look at how filtering affected total reads and ASVs for each sample.\n\n\n\n(16S rRNA) Table 4 | Sample summary showing the number of reads and ASVs after Arbitrary filtering.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "filtering.html#its-1",
    "href": "filtering.html#its-1",
    "title": "3. Filtering",
    "section": "ITS",
    "text": "ITS\n\nCodetmp_low_count <- phyloseq::genefilter_sample(\n                                         its18_ps_work, \n                                         filterfun_sample(function(x) x >= 5), \n                                         A = 0.2 * nsamples(its18_ps_work))\ntmp_low_count <- phyloseq::prune_taxa(tmp_low_count, its18_ps_work)\ntmp_low_var <- phyloseq::filter_taxa(tmp_low_count, \n                                     function(x) var(x) > 0.2, prune = TRUE)\nits18_ps_filt <- tmp_low_var\nrm(list = ls(pattern = \"tmp_\"))\nits18_ps_filt\n\n\nAnd the filtered ITS phyloseq object.\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 816 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 816 taxa by 8 taxonomic ranks ]\n\n\n\n\n\n(ITS) Table 3 | Summary of arbitrary filtering where ASVs represented by fewer than 5 reads, present in less than 20% of samples, and/or a variance less than 0.2, were removed.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nAnd again, let’s look how filtering affected total reads and ASVs for each sample.\n\n\n\n(ITS) Table 4 | Sample summary showing the number of reads and ASVs after Arbitrary filtering.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nAnd that’s it for Arbitrary filtering. Moving on."
  },
  {
    "objectID": "filtering.html#setup",
    "href": "filtering.html#setup",
    "title": "3. Filtering",
    "section": "Setup",
    "text": "Setup\n\nCodesamp_ps_filt <- c(\"ssu18_ps_work\", \"its18_ps_work\")\nfor (i in samp_ps_filt) {\n  tmp_get <- get(i)\n  tmp_get_tab <- data.frame(t(otu_table(tmp_get)))\n  tmp_get_tab <- tmp_get_tab %>% tibble::rownames_to_column(\"ID\")\n  tmp_get_tab <- jamba::mixedSortDF(tmp_get_tab, decreasing = FALSE, \n                                        useRownames = FALSE, byCols = 1)\n  tmp_get_tab <- tmp_get_tab %>% tibble::remove_rownames() \n  tmp_get_tab <- tmp_get_tab %>% tibble::column_to_rownames(\"ID\")\n  tmp_get_tab <- data.frame(t(tmp_get_tab))\n\n  tmp_get_tab_ord <- data.frame(t(otu_table(tmp_get)))\n  tmp_get_tab_ord <- tmp_get_tab_ord %>% tibble::rownames_to_column(\"ID\")\n  tmp_get_tab_ord <- jamba::mixedSortDF(tmp_get_tab_ord, decreasing = TRUE, \n                                        useRownames = FALSE, byCols = 1)\n  tmp_get_tab_ord <- tmp_get_tab_ord %>% tibble::remove_rownames() \n  tmp_get_tab_ord <- tmp_get_tab_ord %>% tibble::column_to_rownames(\"ID\")\n  tmp_get_tab_ord <- data.frame(t(tmp_get_tab_ord))\n\n  tmp_tab_name <- purrr::map_chr(i, ~ paste0(., \"_perfect\"))\n  assign(tmp_tab_name, tmp_get_tab)\n  \n  tmp_tab_ord_name <- purrr::map_chr(i, ~ paste0(., \"_ord_perfect\"))\n  assign(tmp_tab_ord_name, tmp_get_tab_ord)\n  \n  rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()"
  },
  {
    "objectID": "filtering.html#filter",
    "href": "filtering.html#filter",
    "title": "3. Filtering",
    "section": "Filter",
    "text": "Filter\nNext we run the filtering analysis using PERFect_sim. The other option is PERFect_perm however I could not get PERFect_perm to work as of this writing. The process never finished :/\nWe need to set an initial p-value cutoff. For 16S rRNA, we use 0.05 and for ITS we use 0.1.\n\n# Set a pvalue cutoff\nssu_per_pval <- 0.05\nits_per_pval <- 0.10\n\n\nCodefor (i in samp_ps_filt) {\n  tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_perfect\")))\n  tmp_pval <- gsub(\"18.*\", \"_per_pval\", i)\n  tmp_get_ord <- get(purrr::map_chr(i, ~ paste0(., \"_ord_perfect\")))\n  tmp_sim <- PERFect_sim(X = tmp_get, alpha = get(tmp_pval), Order = \"NP\", center = FALSE)\n  dim(tmp_sim$filtX)\n  tmp_sim_ord <- PERFect_sim(X = tmp_get_ord, alpha = get(tmp_pval), Order = \"NP\", center = FALSE)\n  dim(tmp_sim_ord$filtX)\n  \n  tmp_sim_name <- purrr::map_chr(i, ~ paste0(., \"_perfect_sim\"))\n  assign(tmp_sim_name, tmp_sim)\n\n  tmp_sim_ord_name <- purrr::map_chr(i, ~ paste0(., \"_ord_perfect_sim\"))\n  assign(tmp_sim_ord_name, tmp_sim_ord)\n  \n  tmp_path <- file.path(\"files/filtering/perfect/rdata/\")\n  saveRDS(tmp_sim, paste(tmp_path, tmp_sim_name, \".rds\", sep = \"\"))\n  saveRDS(tmp_sim_ord, paste(tmp_path, tmp_sim_ord_name, \".rds\", sep = \"\"))\n\n  rm(list = ls(pattern = \"tmp_\"))\n  \n}\nobjects(pattern = \"_sim\")\n\n\n\n\n\n\n\n\n\n\n\nHow many ASVs were retained after filtering?\nFirst the 16S rRNA data set. Default ordering resulted in 20172 ASVs and reordering the data resulted in 4466 ASVs.\nAnd then the ITS data set. Default ordering resulted in 3354 ASVs and reordering the data resulted in 2133 ASVs.\nFor some reason, the package does not remove based on the p value cutoff that we set earlier (0.05). So we need to filter out the ASVs that have a higher p-value than the cutoff.\n\n\nTotal 16S rRNA ASVs with p-value less than 0.05 \n[1] default order: ASVs before checking p value was 20172 and after was 1679\n[1] decreasing order: ASVs before checking p value was 4466 and after was 1659\n[1] --------------------------------------\nTotal ITS ASVs with p-value less than 0.1 \n[1] default order: ASVs before checking p-value was 3354 and after was 306\n[1] decreasing order: ASVs before checking p-value was 2133 and after was 264\n\n\nNow we can make phyloseq objects. Manual inspection of the results from PERFect_sim for the 16S rRNA data indicated that using the decreasing order and filtering p-values less than 0.05 yielded in the best results. Manual inspection of the results from PERFect_sim for the ITS data indicated that using the default order and filtering p-values less than 0.1 yielded in the best results. These approaches limited the number of ASVs found in only 1 or 2 samples. So first we filter out ASVs with p-values lower than the defined cutoff and then make the objects.\n\n# pvalue cutoffs set earlier\nssu_per_pval <- 0.05\nits_per_pval <- 0.10\n# Choose method\nssu_select <- \"_ord_perfect\"\nits_select <- \"_perfect\"\n\n\nCodefor (i in samp_ps_filt) {\n  ## select pval cutoff and method\n    tmp_pval <- gsub(\"18.*\", \"_per_pval\", i)\n    tmp_select <- gsub(\"18.*\", \"_select\", i)\n    \n    tmp_get <- get(purrr::map_chr(i, ~ paste0(., get(tmp_select))))\n    tmp_get_sim <- get(purrr::map_chr(i, ~ paste0(., get(tmp_select), \"_sim\")))\n    tmp_filt <- data.frame(t(tmp_get_sim$filtX))\n    tmp_filt <- tmp_filt %>% tibble::rownames_to_column(\"ID\")\n    tmp_tab <- data.frame(t(tmp_get))\n    tmp_tab <- tmp_tab %>% tibble::rownames_to_column(\"ID\")\n    tmp_pvals <- data.frame(tmp_get_sim$pvals)\n    tmp_pvals <- tmp_pvals %>% tibble::rownames_to_column(\"ID\") %>% \n              dplyr::rename(\"pval\" = 2)\n    tmp_pvals <- tmp_pvals %>% filter(pval <= get(tmp_pval))\n    tmp_merge <- dplyr::left_join(tmp_pvals, tmp_tab, by = \"ID\")\n    tmp_merge[, 2] <- NULL\n    tmp_merge <- tmp_merge %>% tibble::column_to_rownames(\"ID\")\n    tmp_tax <- data.frame(tax_table(get(i))) %>% tibble::rownames_to_column(\"ID\")\n    tmp_tax <- dplyr::left_join(tmp_pvals, tmp_tax, by = \"ID\")\n    tmp_tax[, 2] <- NULL\n    tmp_tax <- tmp_tax %>% tibble::column_to_rownames(\"ID\")\n# Build PS object\n    tmp_samp <- data.frame(sample_data(get(i)))\n    identical(row.names(tmp_tax), row.names(tmp_merge))\n    tmp_merge <- data.frame(t(tmp_merge))\n    tmp_merge <- as.matrix(tmp_merge)\n    tmp_tax <- as.matrix(tmp_tax)\n    tmp_ps <- phyloseq(otu_table(tmp_merge, taxa_are_rows = FALSE),\n                     tax_table(tmp_tax),\n                     sample_data(tmp_samp))\n    \n    tmp_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE,\n                           tip.label = taxa_names(tmp_ps))\n    tmp_ps <- merge_phyloseq(tmp_ps,\n                               sample_data,\n                               tmp_tree)\n    tmp_ps_name <- purrr::map_chr(i, ~ paste0(., \"_perf_filt\"))\n    assign(tmp_ps_name, tmp_ps)\n    rm(list = ls(pattern = \"tmp_\"))\n}  \n\nssu18_ps_perfect <- ssu18_ps_work_perf_filt\nits18_ps_perfect <- its18_ps_work_perf_filt\nrm(list = ls(pattern = \"_perf_filt\"))\n\n\n\n\n[1] 16S rRNA phyloseq object\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1659 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1659 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 1659 tips and 1658 internal nodes ]\n\n\n[1] ITS phyloseq object\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 306 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 306 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 306 tips and 305 internal nodes ]"
  },
  {
    "objectID": "filtering.html#summary",
    "href": "filtering.html#summary",
    "title": "3. Filtering",
    "section": "Summary",
    "text": "Summary\nHow many reads and ASVs were removed following PERfect filtering?\n16S rRNA\n\n\n\n(16S rRNA) Table 5 | Summary of PERfect filtering using the decreased order and filtering p-values < 0.05.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nITS\n\n\n\n(ITS) Table 5 | Summary of PERfect filtering using the defualt order and filtering p-values < 0.10.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nHere is a summary table of total reads and ASVs on a per sample basis after filtering.\n16S rRNA\n\n\n\n(16S rRNA) Table 6 | Sample summary showing the number of reads and ASVs after PERfect filtering.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nITS\n\n\n\n(ITS) Table 6 | Sample summary showing the number of reads and ASVs after PERfect filtering.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nAnd finally, save the PERfect filtered phyloseq objects.\n\nCodesaveRDS(ssu18_ps_perfect, \"files/filtering/perfect/rdata/ssu18_ps_perfect.rds\")\nsaveRDS(its18_ps_perfect, \"files/filtering/perfect/rdata/its18_ps_perfect.rds\")"
  },
  {
    "objectID": "filtering.html#s-rrna-2",
    "href": "filtering.html#s-rrna-2",
    "title": "3. Filtering",
    "section": "16S rRNA",
    "text": "16S rRNA\nSetup\nFirst, choose a phyloseq object and a sample data frame\n\nssu18_pime_ds <- ssu18_ps_work\nssu18_which_pime <- \"ssu18_pime_ds\"\nssu18_pime_ds@phy_tree <- NULL\nssu18_pime_ds\n\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 20173 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 20173 taxa by 8 taxonomic ranks ]\n\n\n\nssu18_pime_sample_d <- data.frame(rowSums(otu_table(ssu18_pime_ds_full)))\nssu18_pime_sample_d <- ssu18_pime_sample_d %>% dplyr::rename(total_reads = 1)\n\nssu18_pime_ds <- rarefy_even_depth(ssu18_pime_ds_full,\n                               sample.size = min(ssu18_pime_sample_d$total_reads),\n                               trimOTUs = TRUE, replace = FALSE,\n                               rngseed = 119)\n\n2741 OTUs were removed because they are no longer \npresent in any sample after random subsampling\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 17432 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 17432 taxa by 8 taxonomic ranks ]\n\n\nThe first step in PIME is to define if the microbial community presents a high relative abundance of taxa with low prevalence, which is considered as noise in PIME analysis. This is calculated by random forests analysis and is the baseline noise detection.\n\nssu18_pime.oob.error <- pime.oob.error(ssu18_pime_ds, \"TEMP\")\n\n\n\n[1] 0.6666667\n\n\nSplit by Predictor Variable\n\ndata.frame(sample_data(ssu18_pime_ds))\nssu18_per_variable_obj <- pime.split.by.variable(ssu18_pime_ds, \"TEMP\")\nssu18_per_variable_obj\n\n\n\n$`0`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 7883 taxa and 5 samples ]\nsample_data() Sample Data:       [ 5 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 7883 taxa by 8 taxonomic ranks ]\n\n$`3`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 7173 taxa and 5 samples ]\nsample_data() Sample Data:       [ 5 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 7173 taxa by 8 taxonomic ranks ]\n\n$`8`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 6027 taxa and 5 samples ]\nsample_data() Sample Data:       [ 5 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 6027 taxa by 8 taxonomic ranks ]\n\n\nCalculate Prevalence Intervals\nUsing the output of pime.split.by.variable, we calculate the prevalence intervals with the function pime.prevalence. This function estimates the highest prevalence possible (no empty ASV table), calculates prevalence for taxa, starting at 5 maximum prevalence possible (no empty ASV table or dropping samples). After prevalence calculation, each prevalence interval are merged.\n\nssu18_prevalences <- pime.prevalence(ssu18_per_variable_obj)\nssu18_prevalences\n\n\nDetailed results for all prevalences intervals\n\n\n$`5`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 17432 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 17432 taxa by 8 taxonomic ranks ]\n\n$`10`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 17432 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 17432 taxa by 8 taxonomic ranks ]\n\n$`15`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 17432 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 17432 taxa by 8 taxonomic ranks ]\n\n$`20`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 2253 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 2253 taxa by 8 taxonomic ranks ]\n\n$`25`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 2253 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 2253 taxa by 8 taxonomic ranks ]\n\n$`30`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 2253 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 2253 taxa by 8 taxonomic ranks ]\n\n$`35`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 2253 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 2253 taxa by 8 taxonomic ranks ]\n\n$`40`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1058 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1058 taxa by 8 taxonomic ranks ]\n\n$`45`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1058 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1058 taxa by 8 taxonomic ranks ]\n\n$`50`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1058 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1058 taxa by 8 taxonomic ranks ]\n\n$`55`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1058 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1058 taxa by 8 taxonomic ranks ]\n\n$`60`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 585 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 585 taxa by 8 taxonomic ranks ]\n\n$`65`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 585 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 585 taxa by 8 taxonomic ranks ]\n\n$`70`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 585 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 585 taxa by 8 taxonomic ranks ]\n\n$`75`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 585 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 585 taxa by 8 taxonomic ranks ]\n\n$`80`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 294 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 294 taxa by 8 taxonomic ranks ]\n\n$`85`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 294 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 294 taxa by 8 taxonomic ranks ]\n\n$`90`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 294 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 294 taxa by 8 taxonomic ranks ]\n\n$`95`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 294 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 294 taxa by 8 taxonomic ranks ]\n\n\nCalculate Best Prevalence\nFinally, we use the function pime.best.prevalence to calculate the best prevalence. The function uses randomForest to build random forests trees for samples classification and variable importance computation. It performs classifications for each prevalence interval returned by pime.prevalence. Variable importance is calculated, returning the Mean Decrease Accuracy (MDA), Mean Decrease Impurity (MDI), overall and by sample group, and taxonomy for each ASV. PIME keeps the top 30 variables with highest MDA each prevalence level.\n\nset.seed(1911)\nssu18_best.prev <- pime.best.prevalence(ssu18_prevalences, \"TEMP\")\n\n\n\n         Interval OOB error rate (%)  OTUs  Nseqs\n1   Prevalence 5%              66.67 17432 376320\n2  Prevalence 10%              66.67 17432 376320\n3  Prevalence 15%                 60 17432 376320\n4  Prevalence 20%                 40  2253 282185\n5  Prevalence 25%              33.33  2253 282185\n6  Prevalence 30%              26.67  2253 282185\n7  Prevalence 35%              26.67  2253 282185\n8  Prevalence 40%               6.67  1058 232532\n9  Prevalence 45%               6.67  1058 232532\n10 Prevalence 50%                  0  1058 232532\n11 Prevalence 55%                  0  1058 232532\n12 Prevalence 60%                  0   585 192484\n13 Prevalence 65%                  0   585 192484\n14 Prevalence 70%                  0   585 192484\n15 Prevalence 75%                  0   585 192484\n16 Prevalence 80%                  0   294 149258\n17 Prevalence 85%                  0   294 149258\n18 Prevalence 90%                  0   294 149258\n19 Prevalence 95%                  0   294 149258\n\n\n\nssu18_what_is_best <- ssu18_best.prev$`OOB error`\nssu18_what_is_best[, c(2:4)] <- sapply(ssu18_what_is_best[, c(2:4)], as.numeric)\nssu18_what_is_best <- ssu18_what_is_best %>%\n  dplyr::rename(\"OOB_error_rate\" = \"OOB error rate (%)\")\nssu18_what_is_best$Interval <- str_replace_all(ssu18_what_is_best$Interval, \"%\", \"\")\nssu18_best <- with(ssu18_what_is_best, Interval[which.min(OOB_error_rate)])\nssu18_best <- paste(\"`\", ssu18_best, \"`\", sep = \"\")\nssu18_prev_choice <- paste(\"ssu18_best.prev$`Importance`$\", ssu18_best, sep = \"\")\nssu18_imp_best <- eval(parse(text = (ssu18_prev_choice)))\n\nLooks like the lowest OOB error rate (%) that retains the most ASVs is 0% from Prevalence 50. We will use this interval.\nBest Prevalence Summary\n\n\n\n(16S rRNA) Table 7 | Summary of top 30 ASVs from the chosen prevalence interval.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nHere is a list of the top 30 ASVs.\n\n\n [1] \"ASV114\"  \"ASV146\"  \"ASV39\"   \"ASV157\"  \"ASV488\"  \"ASV137\"  \"ASV340\" \n [8] \"ASV289\"  \"ASV918\"  \"ASV282\"  \"ASV861\"  \"ASV314\"  \"ASV508\"  \"ASV1035\"\n[15] \"ASV1426\" \"ASV254\"  \"ASV410\"  \"ASV212\"  \"ASV134\"  \"ASV185\"  \"ASV64\"  \n[22] \"ASV472\"  \"ASV575\"  \"ASV558\"  \"ASV454\"  \"ASV686\"  \"ASV199\"  \"ASV412\" \n[29] \"ASV324\"  \"ASV126\" \n\n\nNow we need to create a phyloseq object of ASVs at this cutoff (Prevalence 50).\n\nssu18_best_val <- str_replace_all(ssu18_best, \"Prevalence \", \"\")\nssu18_best_val <- paste(\"ssu18_prevalences$\", ssu18_best_val, sep = \"\")\nssu18_prevalence_best <- eval(parse(text = (ssu18_best_val)))\nsaveRDS(ssu18_prevalence_best, \"files/filtering/pime/rdata/ssu18_prevalence_best.rds\")\n\nAnd look at a summary of the phyloseq object.\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1058 taxa and 15 samples ]\nsample_data() Sample Data:       [ 15 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1058 taxa by 8 taxonomic ranks ]\n\n\n\n\n\n(16S rRNA) Table 5 | Summary of PERfect filtering using the decreased order and filtering p-values < 0.05.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nEstimate Error in Prediction\nUsing the function pime.error.prediction we can estimate the error in prediction. For each prevalence interval, this function randomizes the samples labels into arbitrary groupings using n random permutations, defined by the bootstrap value. For each, randomized and prevalence filtered, data set the OOB error rate is calculated to estimate whether the original differences in groups of samples occur by chance. Results are in a list containing a table and a box plot summarizing the results.\n\nssu18_randomized <- pime.error.prediction(ssu18_pime_ds, \"TEMP\",\n                                          bootstrap = 100, parallel = TRUE,\n                                          max.prev = 95)\n\n\n\n\n(16S rRNA) Table 8 | Results of 100 random permutations for each prevalence interval based on a function that randomizes the samples labels into arbitrary groupings. using n random permutations. For each randomized and prevalence filtered data set, the OOB error rate is calculated to estimate whether the original differences in groups of samples occur by chance. \n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is also possible to estimate the variation of OOB error for each prevalence interval filtering. This is done by running the random forests classification for n times, determined by the bootstrap value. The function will return a box plot figure and a table for each classification error.\n\nssu18_replicated.oob.error <- pime.oob.replicate(ssu18_prevalences, \"TEMP\",\n                                         bootstrap = 100, parallel = TRUE)\n\n\n\n\n\n\n(16S rRNA) Figure 1 | OOB error rate for the original data set (top) & the randomized data (bottom).\nTo obtain the confusion matrix from random forests classification use the following:\n\nssu18_prev_confuse <- paste(\"ssu18_best.prev$`Confusion`$\", ssu18_best, sep = \"\")\neval(parse(text = (ssu18_prev_confuse)))\n\nSave Phyloseq PIME objects\n\nssu18_ps_pime <- ssu18_prevalence_best\nssu18_ps_pime_tree <- rtree(ntaxa(ssu18_ps_pime), rooted = TRUE,\n                           tip.label = taxa_names(ssu18_ps_pime))\nssu18_ps_pime <- merge_phyloseq(ssu18_ps_pime,\n                               sample_data,\n                               ssu18_ps_pime_tree)\nsaveRDS(ssu18_ps_pime, \"files/filtering/pime/rdata/ssu18_ps_pime.rds\")\n\nSplit & save by predictor variable\n\ndata.frame(sample_data(ssu18_ps_pime))\nssu18_ps_pime_split <- pime.split.by.variable(ssu18_ps_pime, \"TEMP\")\nsaveRDS(ssu18_ps_pime_split$`0`, \"files/filtering/pime/rdata/ssu18_ps_pime_0.rds\")\nsaveRDS(ssu18_ps_pime_split$`3`, \"files/filtering/pime/rdata/ssu18_ps_pime_3.rds\")\nsaveRDS(ssu18_ps_pime_split$`8`, \"files/filtering/pime/rdata/ssu18_ps_pime_8.rds\")\n\n\n\n$`0`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 752 taxa and 5 samples ]\nsample_data() Sample Data:       [ 5 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 752 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 752 tips and 751 internal nodes ]\n\n$`3`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 692 taxa and 5 samples ]\nsample_data() Sample Data:       [ 5 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 692 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 692 tips and 691 internal nodes ]\n\n$`8`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 355 taxa and 5 samples ]\nsample_data() Sample Data:       [ 5 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 355 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 355 tips and 354 internal nodes ]\n\n\n\n\n\nSummary\nYou know the routine. How did filtering affect total reads and ASVs for each sample?\n\n\n\n(16S rRNA) Table 9 | Sample summary showing the number of reads and ASVs after PIME filtering.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nAnd here is how the subsets changed through the PIME filtering process.\n\n\n\n(16S rRNA) Table 10 | Changes in read count and total ASVs during PIME filtering.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "filtering.html#its-2",
    "href": "filtering.html#its-2",
    "title": "3. Filtering",
    "section": "ITS",
    "text": "ITS\nSetup\nFirst, choose a phyloseq object and a sample data frame\n\nits18_pime_ds <- its18_ps_work\nits18_which_pime <- \"its18_pime_ds\"\nits18_pime_ds@phy_tree <- NULL\n\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3355 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3355 taxa by 8 taxonomic ranks ]\n\n\n\nits18_pime_sample_d <- data.frame(rowSums(otu_table(its18_pime_ds_full)))\nits18_pime_sample_d <- its18_pime_sample_d %>% dplyr::rename(total_reads = 1)\n\nits18_pime_ds <- rarefy_even_depth(its18_pime_ds_full,\n                               sample.size = min(its18_pime_sample_d$total_reads),\n                               trimOTUs = TRUE, replace = FALSE,\n                               rngseed = 119)\n\n298 OTUs were removed because they are no longer \npresent in any sample after random subsampling\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3057 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3057 taxa by 8 taxonomic ranks ]\n\n\nThe first step in PIME is to define if the microbial community presents a high relative abundance of taxa with low prevalence, which is considered as noise in PIME analysis. This is calculated by random forests analysis and is the baseline noise detection.\n\nits18_pime.oob.error <- pime.oob.error(its18_pime_ds, \"TEMP\")\n\n\n\n[1] 0.3846154\n\n\nSplit by Predictor Variable\n\ndata.frame(sample_data(its18_pime_ds))\nits18_per_variable_obj <- pime.split.by.variable(its18_pime_ds, \"TEMP\")\nits18_per_variable_obj\n\n\n\n$`0`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1932 taxa and 5 samples ]\nsample_data() Sample Data:       [ 5 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1932 taxa by 8 taxonomic ranks ]\n\n$`3`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1682 taxa and 4 samples ]\nsample_data() Sample Data:       [ 4 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1682 taxa by 8 taxonomic ranks ]\n\n$`8`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1306 taxa and 4 samples ]\nsample_data() Sample Data:       [ 4 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1306 taxa by 8 taxonomic ranks ]\n\n\nCalculate Prevalence Intervals\nUsing the output of pime.split.by.variable, we calculate the prevalence intervals with the function pime.prevalence. This function estimates the highest prevalence possible (no empty ASV table), calculates prevalence for taxa, starting at 5 maximum prevalence possible (no empty ASV table or dropping samples). After prevalence calculation, each prevalence interval are merged.\n\nits18_prevalences <- pime.prevalence(its18_per_variable_obj)\nits18_prevalences\n\n\nDetailed results for all prevalences intervals\n\n\n$`5`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3057 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3057 taxa by 8 taxonomic ranks ]\n\n$`10`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3057 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3057 taxa by 8 taxonomic ranks ]\n\n$`15`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 3057 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 3057 taxa by 8 taxonomic ranks ]\n\n$`20`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 2421 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 2421 taxa by 8 taxonomic ranks ]\n\n$`25`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1152 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1152 taxa by 8 taxonomic ranks ]\n\n$`30`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1152 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1152 taxa by 8 taxonomic ranks ]\n\n$`35`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 1152 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 1152 taxa by 8 taxonomic ranks ]\n\n$`40`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 873 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 873 taxa by 8 taxonomic ranks ]\n\n$`45`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 873 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 873 taxa by 8 taxonomic ranks ]\n\n$`50`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 474 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 474 taxa by 8 taxonomic ranks ]\n\n$`55`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 474 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 474 taxa by 8 taxonomic ranks ]\n\n$`60`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 336 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 336 taxa by 8 taxonomic ranks ]\n\n$`65`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 336 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 336 taxa by 8 taxonomic ranks ]\n\n$`70`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 336 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 336 taxa by 8 taxonomic ranks ]\n\n$`75`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 171 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 171 taxa by 8 taxonomic ranks ]\n\n$`80`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 114 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 114 taxa by 8 taxonomic ranks ]\n\n$`85`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 114 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 114 taxa by 8 taxonomic ranks ]\n\n$`90`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 114 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 114 taxa by 8 taxonomic ranks ]\n\n$`95`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 114 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 114 taxa by 8 taxonomic ranks ]\n\n\nCalculate Best Prevalence\nFinally, we use the function pime.best.prevalence to calculate the best prevalence. The function uses randomForest to build random forests trees for samples classification and variable importance computation. It performs classifications for each prevalence interval returned by pime.prevalence. Variable importance is calculated, returning the Mean Decrease Accuracy (MDA), Mean Decrease Impurity (MDI), overall and by sample group, and taxonomy for each ASV. PIME keeps the top 30 variables with highest MDA each prevalence level.\n\nset.seed(1911)\nits18_best.prev <- pime.best.prevalence(its18_prevalences, \"TEMP\")\n\n\n\n         Interval OOB error rate (%) OTUs  Nseqs\n1   Prevalence 5%              38.46 3057 119236\n2  Prevalence 10%              53.85 3057 119236\n3  Prevalence 15%              38.46 3057 119236\n4  Prevalence 20%              38.46 2421 112518\n5  Prevalence 25%              38.46 1152  91780\n6  Prevalence 30%              23.08 1152  91780\n7  Prevalence 35%              15.38 1152  91780\n8  Prevalence 40%              30.77  873  82647\n9  Prevalence 45%              15.38  873  82647\n10 Prevalence 50%               7.69  474  67665\n11 Prevalence 55%                  0  474  67665\n12 Prevalence 60%               7.69  336  60556\n13 Prevalence 65%                  0  336  60556\n14 Prevalence 70%               7.69  336  60556\n15 Prevalence 75%                  0  171  44260\n16 Prevalence 80%                  0  114  33492\n17 Prevalence 85%                  0  114  33492\n18 Prevalence 90%                  0  114  33492\n19 Prevalence 95%                  0  114  33492\n\n\n\nits18_what_is_best <- its18_best.prev$`OOB error`\nits18_what_is_best[, c(2:4)] <- sapply(its18_what_is_best[, c(2:4)], as.numeric)\nits18_what_is_best <- its18_what_is_best %>%\n  dplyr::rename(\"OOB_error_rate\" = \"OOB error rate (%)\")\nits18_what_is_best$Interval <- str_replace_all(its18_what_is_best$Interval, \"%\", \"\")\nits18_best <- with(its18_what_is_best, Interval[which.min(OOB_error_rate)])\nits18_best <- paste(\"`\", its18_best, \"`\", sep = \"\")\nits18_prev_choice <- paste(\"its18_best.prev$`Importance`$\", its18_best, sep = \"\")\nits18_imp_best <- eval(parse(text = (its18_prev_choice)))\n\nLooks like the lowest OOB error rate (%) that retains the most ASVs is 0% from Prevalence 55. We will use this interval.\nBest Prevalence Summary\n\n\n\n(ITS) Table 7 | Summary of top 30 ASVs from the chosen prevalence interval.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n\n [1] \"ASV307\"  \"ASV302\"  \"ASV677\"  \"ASV643\"  \"ASV71\"   \"ASV437\"  \"ASV125\" \n [8] \"ASV318\"  \"ASV502\"  \"ASV104\"  \"ASV40\"   \"ASV1141\" \"ASV481\"  \"ASV349\" \n[15] \"ASV659\"  \"ASV653\"  \"ASV33\"   \"ASV118\"  \"ASV3\"    \"ASV184\"  \"ASV164\" \n[22] \"ASV81\"   \"ASV154\"  \"ASV567\"  \"ASV46\"   \"ASV140\"  \"ASV570\"  \"ASV1318\"\n[29] \"ASV185\"  \"ASV603\" \n\n\nNow we need to create a phyloseq object of ASVs at this cutoff (Prevalence 55).\n\nits18_best_val <- str_replace_all(its18_best, \"Prevalence \", \"\")\nits18_best_val <- paste(\"its18_prevalences$\", its18_best_val, sep = \"\")\nits18_prevalence_best <- eval(parse(text = (its18_best_val)))\nsaveRDS(its18_prevalence_best, \"files/filtering/pime/rdata/its18_asv_prevalence_best.rds\")\n\nAnd look at a summary of the data.\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 474 taxa and 13 samples ]\nsample_data() Sample Data:       [ 13 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 474 taxa by 8 taxonomic ranks ]\n\n\n\n\n\n(ITS) Table 5 | Summary of PERfect filtering using the defualt order and filtering p-values < 0.10.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nEstimate Error in Prediction\nUsing the function pime.error.prediction we can estimate the error in prediction. For each prevalence interval, this function randomizes the samples labels into arbitrary groupings using n random permutations, defined by the bootstrap value. For each, randomized and prevalence filtered, data set the OOB error rate is calculated to estimate whether the original differences in groups of samples occur by chance. Results are in a list containing a table and a box plot summarizing the results.\n\nits18_randomized <- pime.error.prediction(its18_pime_ds, \"TEMP\",\n                                          bootstrap = 100, parallel = TRUE,\n                                          max.prev = 95)\n\n\n\n\n(ITS) Table 8 | Results of 100 random permutations for each prevalence interval based on a function that randomizes the samples labels into arbitrary groupings. using n random permutations. For each randomized and prevalence filtered data set, the OOB error rate is calculated to estimate whether the original differences in groups of samples occur by chance. \n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is also possible to estimate the variation of OOB error for each prevalence interval filtering. This is done by running the random forests classification for n times, determined by the bootstrap value. The function will return a box plot figure and a table for each classification error.\n\nits18_replicated.oob.error <- pime.oob.replicate(its18_prevalences, \"TEMP\",\n                                         bootstrap = 100, parallel = TRUE)\n\n\n\n\n\n\n(ITS) Figure 1 | OOB error rate for the original data set (top) & the randomized data (bottom).\nTo obtain the confusion matrix from random forests classification use the following:\n\nits18_prev_confuse <- paste(\"its18_best.prev$`Confusion`$\", its18_best, sep = \"\")\neval(parse(text = (its18_prev_confuse)))\n\nSave Phyloseq PIME objects\n\nits18_ps_pime <- its18_prevalence_best\nits18_ps_pime_tree <- rtree(ntaxa(its18_ps_pime), rooted = TRUE,\n                           tip.label = taxa_names(its18_ps_pime))\nits18_ps_pime <- merge_phyloseq(its18_ps_pime,\n                               sample_data,\n                               its18_ps_pime_tree)\nsaveRDS(its18_ps_pime, \"files/filtering/pime/rdata/its18_ps_pime.rds\")\n\nSplit & save by predictor variable\n\ndata.frame(sample_data(its18_ps_pime))\nits18_ps_pime_split <- pime.split.by.variable(its18_ps_pime, \"TEMP\")\nsaveRDS(its18_ps_pime_split$`0`, \"files/filtering/pime/rdata/its18_ps_pime_0.rds\")\nsaveRDS(its18_ps_pime_split$`3`, \"files/filtering/pime/rdata/its18_ps_pime_3.rds\")\nsaveRDS(its18_ps_pime_split$`8`, \"files/filtering/pime/rdata/its18_ps_pime_8.rds\")\n\n\n\n$`0`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 315 taxa and 5 samples ]\nsample_data() Sample Data:       [ 5 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 315 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 315 tips and 314 internal nodes ]\n\n$`3`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 234 taxa and 4 samples ]\nsample_data() Sample Data:       [ 4 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 234 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 234 tips and 233 internal nodes ]\n\n$`8`\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 138 taxa and 4 samples ]\nsample_data() Sample Data:       [ 4 samples by 6 sample variables ]\ntax_table()   Taxonomy Table:    [ 138 taxa by 8 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 138 tips and 137 internal nodes ]\n\n\n\n\n\nSummary\nThe influence of filtering on total reads and ASVs for each sample.\n\n\n\n(ITS) Table 9 | Sample summary showing the number of reads and ASVs after PIME filtering.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nAnd here is how the subsets changed through the PIME filtering process.\n\n\n\n(ITS) Table 10 | Changes in read count and total ASVs during PIME filtering.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The S.W.E.L.T.R. Microbial Cookbook",
    "section": "",
    "text": "Welcome to the website of the Soil Warming Experiment in Lowland Tropical Rainforest project, or SWELTR for short. This site contains reproducible workflows and pipelines for all aspects of processing and analyzing the SWELTR high temperature experiment, entitled Microbial diversity decline and community response are decoupled from increased respiration in warmed tropical forest soil. Most (if not all) steps were performed in R. Though we ran some steps using the Smithsonian compute cluster (Hydra), all workflows can be run on a laptop. Workflows have a modular design, meaning they can each be run independently as long as you have the necessary input files described at the beginning of each workflow. Many of the tables have download or copy buttons that allow you to quickly export data in different formats. Citations, which are embedded in the workflows, can be copied for use in reference management software. Generally, all R code is available in each workflow. Code for displaying tables and other non-analytical features are always hidden but can be accessed from the qmd file on the site GitHub repo. At the bottom of each page there are a variety of Appendices including a bibliography, links to source code, instructions on accessing raw data, etc."
  },
  {
    "objectID": "index.html#processing-workflows",
    "href": "index.html#processing-workflows",
    "title": "The S.W.E.L.T.R. Microbial Cookbook",
    "section": "Processing Workflows",
    "text": "Processing Workflows\n\n\nDADA2. Workflow begins with raw fastq files and goes through the process of primer removal (using catadapt (Martin 2011)), filtering, quality screening, trimming, error correction, chimera removal, and taxonomic assignment using DADA2 (Callahan et al. 2016). The workflow ends with the generation of an Rdata file for the next step. Depending on the DADA2 version you have installed, you may get slightly different results due to fundamental changes in the code-base. This is unavoidable at times and the developers do the best they can to maintain fidelity across versions. We set random number seeds at the beginning of each workflow to ensure reproducibility. To see the package versions of tools used in each workflow, please see the R Session Information section on the bottom of each page.\n\nData Set Prep. Workflow begins with the initial sequence and taxonomy tables from the DADA2 workflow and uses phyloseq (McMurdie and Holmes 2013) to define samples and sample groups, create phyloseq objects, and curate data sets. Summary data is provided throughout the workflow. Again, workflow ends with the generation of an Rdata file for the next step.\n\nFiltering. In Part A, we apply arbitrary filtering to the 16S rRNA and ITS data sets. In Part B we use PERFect (PERmutation Filtering test for microbiome data) (Smirnova, Huzurbazar, and Jafari 2019) to filter the data sets. And in Part C of this workflow, we use PIME (Prevalence Interval for Microbiome Evaluation) (Roesch et al. 2020) to filter the FULL 16S rRNA and ITS data sets."
  },
  {
    "objectID": "index.html#diversity-workflows",
    "href": "index.html#diversity-workflows",
    "title": "The S.W.E.L.T.R. Microbial Cookbook",
    "section": "Diversity Workflows",
    "text": "Diversity Workflows\n\n\nTaxonomic. Taxonomic diversity primarily focused on taxon bar charts.\n\nAlpha Diversity. This section covers the distribution of reads across samples and includes histograms of read counts & rarefaction curves. Also covers estimates of alpha diversity using Hill numbers (Alberdi and Gilbert 2019) and includes normality tests, significance tests, & diversity plots.\n\nBeta Diversity Estimates of beta diversity including significance tests, beta dispersion, and ordination plots.\n\nDA ASVs & OTUs. Results and visualizations from Indicator Analysis and LEfSe.\n\nMetadata. Multivariate Analysis Overview of environmental metadata and community composition. Includes metadata Normality Tests, Normalization, Autocorrelation Tests, Dissimilarity Correlation Tests, Best Subset of Variables, Distance-based Redundancy, and Capscale analysis.\n\nThough the workflows are annotated, they are not necessarily meant to be tutorials since we only provide minimal explanation. There are many great tutorials and explanations out there on amplicon processing that you can dive into."
  },
  {
    "objectID": "index.html#manuscript-visualization",
    "href": "index.html#manuscript-visualization",
    "title": "The S.W.E.L.T.R. Microbial Cookbook",
    "section": "Manuscript Visualization",
    "text": "Manuscript Visualization\nHere is a word cloud of the manuscript, just for the fun of it ;) The size of each word in proportional to the number of times it appeared in the manuscript."
  },
  {
    "objectID": "metadata.html",
    "href": "metadata.html",
    "title": "8. Metadata Analysis",
    "section": "",
    "text": "Click here for setup information.\n\nknitr::opts_chunk$set(eval = FALSE, echo = TRUE)\nset.seed(119)\n#library(conflicted)\n#pacman::p_depends(vegan, local = TRUE)  \n#pacman::p_depends_reverse(vegan, local = TRUE)  \nlibrary(phyloseq); packageVersion(\"phyloseq\")\npacman::p_load(ape, bestNormalize, tidyverse, gdata, mctoolsr, \n               ggpubr, ggvegan, ggfortify, vegan, reactable, \n               downloadthis, captioner,\n               install = FALSE, update = FALSE)\n\noptions(scipen=999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))"
  },
  {
    "objectID": "metadata.html#workflow-input",
    "href": "metadata.html#workflow-input",
    "title": "8. Metadata Analysis",
    "section": "Workflow Input",
    "text": "Workflow Input\nFiles needed to run this workflow can be downloaded from figshare. You will also need the metadata table, which can be accessed below."
  },
  {
    "objectID": "metadata.html#metadata",
    "href": "metadata.html#metadata",
    "title": "8. Metadata Analysis",
    "section": "Metadata",
    "text": "Metadata\nIn this section we provide access to the complete environmental metadata containing 61 parameters across all 15 samples.\n\n\n\n\n\n\n\n Download data as csv file\n\n\n\n(common) Table 1 | Metadata values for each sample."
  },
  {
    "objectID": "metadata.html#s-rrna",
    "href": "metadata.html#s-rrna",
    "title": "8. Metadata Analysis",
    "section": "16S rRNA",
    "text": "16S rRNA\nMetadata parameters removed based on autocorrelation results.\n\nedaphic properties: TEB + DON + Na + Al + Ca\nsoil functional response: micN + micNP + enzCN + enzCP + BPase + CEase + LPase + Nase + Pase\n\ntemperature adaptation: NUE + PUE + SI\n\nMetadata parameters removed from capscale analysis based on Degrees of Freedom.\n\nedaphic properties: Mg + Mn\nsoil functional response: NONE\ntemperature adaptation: NONE\n\n\n\n\n\n Download data as csv file\n\n\n\n\n\n\n(16S rRNA) Table 1 | Significant results of envfit & bioenv tests for the 16S rRNA data set."
  },
  {
    "objectID": "metadata.html#its",
    "href": "metadata.html#its",
    "title": "8. Metadata Analysis",
    "section": "ITS",
    "text": "ITS\nMetadata parameters removed based on autocorrelation results.\n\nedaphic properties: TEB + DON + Na + Al + Ca\nsoil functional response: micN + micNP + enzCN + enzCP + BPase + CEase + LPase + Nase + Pase\n\ntemperature adaptation: NUE + PUE + PQ10 + SI\n\nMetadata parameters removed from capscale analysis based on Degrees of Freedom.\n\nedaphic properties: Mg + Mn + Na + Al + Fe + K\nsoil functional response: NONE\ntemperature adaptation: SQ10\n\n\n\n\n\n\n Download data as csv file\n\n\n\n\n\n\n(ITS) Table 1 | Significant results of envfit & bioenv tests for the ITS data set."
  },
  {
    "objectID": "metadata.html#data-formating",
    "href": "metadata.html#data-formating",
    "title": "8. Metadata Analysis",
    "section": "Data Formating",
    "text": "Data Formating\n\nTransform ps Objects\n\nThe first step is to transform the phyloseq objects.\n\nCodesamp_ps <- c(\"ssu18_ps_work\", \"ssu18_ps_pime\", \"ssu18_ps_perfect\", \n             \"ssu18_ps_work_otu\", \"ssu18_ps_pime_otu\", \"ssu18_ps_perfect_otu\")\nfor (i in samp_ps) {\n     tmp_get <- get(i)\n     tmp_ps <- transform_sample_counts(tmp_get, function(otu) 1e5 * otu/sum(otu))\n     tmp_ps@phy_tree <- NULL\n     tmp_ps <- prune_samples(sample_sums(tmp_ps) > 0, tmp_ps)\n     tmp_tree <- rtree(ntaxa(tmp_ps), rooted = TRUE, tip.label = taxa_names(tmp_ps))\n     tmp_samp <- data.frame(sample_data(tmp_get))\n     tmp_samp <- tmp_samp %>% dplyr::rename(\"TREAT_T\" = \"TEMP\")\n     tmp_ps <- merge_phyloseq(tmp_ps, sample_data, tmp_tree)\n     sample_data(tmp_ps) <- tmp_samp\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_prop\"))\n     print(tmp_name)\n     assign(tmp_name, tmp_ps)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nFormat for mctoolsr\n\n\nThen a bit of formatting to make the data compatible with mctoolsr.\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_prop\")))\n     tmp_tax <- data.frame(tax_table(tmp_get))\n     tmp_tax$ASV_SEQ <- NULL\n     \n     tmp_col_names <- colnames(tmp_tax)\n     tmp_tax_merge <- tmp_tax %>% tidyr::unite(taxonomy, \n                                               all_of(tmp_col_names), \n                                               sep = \";\")\n     tmp_tax_merge <- tmp_tax_merge %>% tibble::rownames_to_column(\"#OTU_ID\")\n     tmp_otu <- data.frame(t(otu_table(tmp_get)))\n     tmp_otu <- tmp_otu %>% tibble::rownames_to_column(\"#OTU_ID\")\n     tmp_otu_tax <- dplyr::left_join(tmp_otu, tmp_tax_merge, by = \"#OTU_ID\")\n\n     tmp_samp <- data.frame(sample_data(tmp_get))\n     tmp_samp[,c(1,3)] <- NULL\n     tmp_samp <- tmp_samp %>% tibble::rownames_to_column(\"#SampleID\")\n\n     tmp_metad <- metad %>% dplyr::rename(\"#SampleID\" = \"id\")\n     tmp_metad[,2:5] <- NULL\n     tmp_md <- dplyr::left_join(tmp_samp, tmp_metad, by = \"#SampleID\")\n     \n     tmp_otu_name <- purrr::map_chr(i, ~ paste0(., \"_otu_tax\"))\n     print(tmp_otu_name)\n     assign(tmp_otu_name, tmp_otu_tax)\n\n     tmp_md_name <- purrr::map_chr(i, ~ paste0(., \"_md\"))\n     print(tmp_md_name)\n     assign(tmp_md_name, tmp_md)\n\n     tmp_path <- file.path(\"files/metadata/tables/\")\n     write_delim(tmp_otu_tax, paste(tmp_path, tmp_otu_name, \".txt\", sep = \"\"), delim = \"\\t\")\n     write_delim(tmp_md, paste(tmp_path, tmp_md_name, \".txt\", sep = \"\"), delim = \"\\t\")\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nCodefor (i in samp_ps) {\n     tmp_path <- file.path(\"files/metadata/tables/\")\n     tmp_otu_name <- purrr::map_chr(i, ~ paste0(., \"_otu_tax\"))\n     tmp_md_name <- purrr::map_chr(i, ~ paste0(., \"_md\"))\n     tmp_tax_table_fp <- paste(tmp_path, tmp_otu_name, \".txt\", sep = \"\")\n     tmp_map_fp <- paste(tmp_path, tmp_md_name, \".txt\", sep = \"\")\n     tmp_input <- load_taxa_table(tmp_tax_table_fp, tmp_map_fp)\n     tmp_input_name <- purrr::map_chr(i, ~ paste0(., \"_mc\"))\n     print(tmp_input_name)\n     assign(tmp_input_name, tmp_input)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nrm(list = ls(pattern = \"_md\"))\nrm(list = ls(pattern = \"_otu_tax\"))"
  },
  {
    "objectID": "metadata.html#choose-data-set",
    "href": "metadata.html#choose-data-set",
    "title": "8. Metadata Analysis",
    "section": "Choose Data Set",
    "text": "Choose Data Set\nAt this point in the code we need to choose a data set to use, formatted with mctoolsr. Remember, there are four choices:\n\nFULL, unfiltered data set.\nArbitrary filtered data set.\nPERfect filtered data set.\nPIME filtered data set.\n\nThis way, if we want to test other data sets we only need to change the name here.\n\nobjects(pattern = \"_mc\")\nssu18_select_mc <- ssu18_ps_pime_mc"
  },
  {
    "objectID": "metadata.html#normality-tests",
    "href": "metadata.html#normality-tests",
    "title": "8. Metadata Analysis",
    "section": "Normality Tests",
    "text": "Normality Tests\nBefore proceeding, we need to test each parameter in the metadata to see which ones are and are not normally distributed. For that, we use the Shapiro-Wilk Normality Test. Here we only need one of the metadata files.\n\nCodetemp_md <- ssu18_select_mc$map_loaded\ntemp_md[,1:9] <- NULL\nshap_results <- NULL\n\nfor (i in colnames(temp_md)) {\n  tmp_shap <- shapiro.test(temp_md[[i]])\n  tmp_p <- round(tmp_shap$p.value, digits = 5)\n  tmp_res <- eval(isTRUE(tmp_shap$p.value < 0.05))\n  shap_results <- rbind(shap_results, data.frame(i, tmp_p, tmp_res))\n  rm(list = ls(pattern = \"tmp_\"))\n}\ncolnames(shap_results) <- c(\"parameter\", \"p-value\", \"tranform\")\nshap_results\ndplyr::filter(shap_results, tranform == \"TRUE\")\nmd_to_tranform <- shap_results$parameter[shap_results$tranform == TRUE]\nrm(list = ls(pattern = \"temp_md\"))\n\n\nShow the results of each normality test for metadata parameters\n\n(16S rRNA) Table 2 | Results of the Shapiro-Wilk Normality Tests. P-values in red are significance (p-value < 0.05) meaning the parameter needs to be normalized.\n\n\n\n\n\n\n\nLooks like we need to transform 25 metadata parameters."
  },
  {
    "objectID": "metadata.html#normalize-parameters",
    "href": "metadata.html#normalize-parameters",
    "title": "8. Metadata Analysis",
    "section": "Normalize Parameters",
    "text": "Normalize Parameters\nHere we use the R package bestNormalize to find and execute the best normalizing transformation. The function will test the following normalizing transformations:\n\narcsinh_x performs an arcsinh transformation.\nboxcox Perform a Box-Cox transformation and center/scale a vector to attempt normalization. boxcox estimates the optimal value of lambda for the Box-Cox transformation. The function will return an error if a user attempt to transform nonpositive data.\nyeojohnson Perform a Yeo-Johnson Transformation and center/scale a vector to attempt normalization. yeojohnson estimates the optimal value of lambda for the Yeo-Johnson transformation. The Yeo-Johnson is similar to the Box-Cox method, however it allows for the transformation of nonpositive data as well.\norderNorm The Ordered Quantile (ORQ) normalization transformation, orderNorm(), is a rank-based procedure by which the values of a vector are mapped to their percentile, which is then mapped to the same percentile of the normal distribution. Without the presence of ties, this essentially guarantees that the transformation leads to a uniform distribution.\nlog_x performs a simple log transformation. The parameter a is essentially estimated by the training set by default (estimated as the minimum possible to some extent epsilon), while the base must be specified beforehand. The default base of the log is 10.\nsqrt_x performs a simple square-root transformation. The parameter a is essentially estimated by the training set by default (estimated as the minimum possible), while the base must be specified beforehand.\nexp_x performs a simple exponential transformation.\n\nSee this GitHub issue (#5) for a description on getting reproducible results. Apparently, you can get different results because the bestNormalize() function uses repeated cross-validation (and doesn’t automatically set the seed), so the results will be slightly different each time the function is executed.\n\nCodeset.seed(119)\nfor (i in md_to_tranform) {\n  tmp_md <- ssu18_select_mc$map_loaded\n  tmp_best_norm <- bestNormalize(tmp_md[[i]], r = 1, k = 5, loo = TRUE)\n  tmp_name <- purrr::map_chr(i, ~ paste0(., \"_best_norm_test\"))\n  assign(tmp_name, tmp_best_norm)\n  print(tmp_name)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nShow the chosen transformations\n\n\n\n## bestNormalize Chosen transformation of AST ## \norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n25.800 26.275 28.800 30.280 41.770 \n_____________________________________\n\n## bestNormalize Chosen transformation of H2O ## \norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n   0%   25%   50%   75%  100% \n0.204 0.308 0.370 0.384 0.401 \n_____________________________________\n\n## bestNormalize Chosen transformation of Al ## \nStandardized asinh(x) Transformation with 15 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 0.0059997 \n - sd (before standardization) = 0.008280287 \n_____________________________________\n\n## bestNormalize Chosen transformation of Ca ## \norderNorm Transformation with 15 nonmissing obs and ties\n - 10 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n25.49 27.19 29.23 34.36 39.79 \n_____________________________________\n\n## bestNormalize Chosen transformation of Fe ## \nStandardized asinh(x) Transformation with 15 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 0.0119984 \n - sd (before standardization) = 0.01264569 \n_____________________________________\n\n## bestNormalize Chosen transformation of TEB ## \norderNorm Transformation with 15 nonmissing obs and ties\n - 10 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n41.6 43.3 45.1 51.4 61.7 \n_____________________________________\n\n## bestNormalize Chosen transformation of ECEC ## \norderNorm Transformation with 15 nonmissing obs and ties\n - 10 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n41.7 43.3 45.3 51.5 61.8 \n_____________________________________\n\n## bestNormalize Chosen transformation of minNO3 ## \nStandardized Box Cox Transformation with 15 nonmissing obs.:\n Estimated statistics:\n - lambda = -0.3344538 \n - mean (before standardization) = 1.907818 \n - sd (before standardization) = 0.2388776 \n_____________________________________\n\n## bestNormalize Chosen transformation of minTIN ## \norderNorm Transformation with 15 nonmissing obs and ties\n - 14 unique values \n - Original quantiles:\n     0%     25%     50%     75%    100% \n  8.760  14.995  23.080  39.935 110.750 \n_____________________________________\n\n## bestNormalize Chosen transformation of DOC ## \norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n     0%     25%     50%     75%    100% \n 30.800  44.070  56.680  64.385 118.740 \n_____________________________________\n\n## bestNormalize Chosen transformation of DOCN ## \norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 9.440 10.290 11.150 12.095 19.270 \n_____________________________________\n\n## bestNormalize Chosen transformation of micCN ## \norderNorm Transformation with 15 nonmissing obs and ties\n - 14 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n4.420 4.585 4.720 5.415 5.850 \n_____________________________________\n\n## bestNormalize Chosen transformation of BG_ase ## \norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 3.010  3.735  4.220  5.795 10.480 \n_____________________________________\n\n## bestNormalize Chosen transformation of BP_ase ## \nStandardized Log_b(x + a) Transformation with 15 nonmissing obs.:\n Relevant statistics:\n - a = 0 \n - b = 10 \n - mean (before standardization) = 0.5217757 \n - sd (before standardization) = 0.2001498 \n_____________________________________\n\n## bestNormalize Chosen transformation of CE_ase ## \nStandardized Log_b(x + a) Transformation with 15 nonmissing obs.:\n Relevant statistics:\n - a = 0 \n - b = 10 \n - mean (before standardization) = 0.03306978 \n - sd (before standardization) = 0.1652591 \n_____________________________________\n\n## bestNormalize Chosen transformation of P_ase ## \norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n13.020 15.130 19.210 24.455 44.040 \n_____________________________________\n\n## bestNormalize Chosen transformation of N_ase ## \norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n  0%  25%  50%  75% 100% \n2.57 2.93 3.58 5.04 9.43 \n_____________________________________\n\n## bestNormalize Chosen transformation of XY_ase ## \norderNorm Transformation with 15 nonmissing obs and ties\n - 13 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n0.66 0.81 1.11 1.39 2.52 \n_____________________________________\n\n## bestNormalize Chosen transformation of BG_Q10 ## \norderNorm Transformation with 15 nonmissing obs and ties\n - 13 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n1.310 1.360 1.390 1.545 1.760 \n_____________________________________\n\n## bestNormalize Chosen transformation of BP_Q10 ## \norderNorm Transformation with 15 nonmissing obs and ties\n - 11 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n1.340 1.415 1.440 1.530 1.740 \n_____________________________________\n\n## bestNormalize Chosen transformation of CO2 ## \norderNorm Transformation with 15 nonmissing obs and ties\n - 14 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 2.080  4.370  5.270 10.585 40.470 \n_____________________________________\n\n## bestNormalize Chosen transformation of PX_ase ## \norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n     0%     25%     50%     75%    100% \n 81.250  95.565 119.750 218.370 339.600 \n_____________________________________\n\n## bestNormalize Chosen transformation of PX_Q10 ## \nStandardized Yeo-Johnson Transformation with 15 nonmissing obs.:\n Estimated statistics:\n - lambda = -4.999946 \n - mean (before standardization) = 0.1968247 \n - sd (before standardization) = 0.001294011 \n_____________________________________\n\n## bestNormalize Chosen transformation of CUEcp ## \nStandardized Box Cox Transformation with 15 nonmissing obs.:\n Estimated statistics:\n - lambda = -0.9999576 \n - mean (before standardization) = -4.229896 \n - sd (before standardization) = 0.9115063 \n_____________________________________\n\n## bestNormalize Chosen transformation of PUE ## \norderNorm Transformation with 15 nonmissing obs and ties\n - 7 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n0.770 0.880 0.900 0.905 0.920 \n_____________________________________\n\n\n\nShow the complete bestNormalize results\n\n\n\n## Results of bestNormalize for AST ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 3.4\n - Box-Cox: 1.8\n - Center+scale: 4.2\n - Exp(x): 21.2667\n - Log_b(x+a): 3.4\n - orderNorm (ORQ): 0.2\n - sqrt(x + a): 4.2\n - Yeo-Johnson: 2.8667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n25.800 26.275 28.800 30.280 41.770 \n_____________________________________\n\n## Results of bestNormalize for H2O ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 6.8667\n - Box-Cox: 5.8\n - Center+scale: 5.8\n - Exp(x): 5.8\n - Log_b(x+a): 6.0667\n - orderNorm (ORQ): 0.7333\n - sqrt(x + a): 6.8667\n - Yeo-Johnson: 5.8\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n   0%   25%   50%   75%  100% \n0.204 0.308 0.370 0.384 0.401 \n_____________________________________\n\n## Results of bestNormalize for Al ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 8.2\n - Center+scale: 8.2\n - Exp(x): 8.2\n - Log_b(x+a): 8.2\n - orderNorm (ORQ): 8.2\n - sqrt(x + a): 8.2\n - Yeo-Johnson: 8.2\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized asinh(x) Transformation with 15 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 0.0059997 \n - sd (before standardization) = 0.008280287 \n_____________________________________\n\n## Results of bestNormalize for Ca ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.8\n - Box-Cox: 1.8\n - Center+scale: 3.4\n - Exp(x): 18.0667\n - Log_b(x+a): 1.8\n - orderNorm (ORQ): 0.2\n - sqrt(x + a): 1.8\n - Yeo-Johnson: 1.2667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and ties\n - 10 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n25.49 27.19 29.23 34.36 39.79 \n_____________________________________\n\n## Results of bestNormalize for Fe ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 6.3333\n - Center+scale: 6.3333\n - Exp(x): 6.3333\n - Log_b(x+a): 6.3333\n - orderNorm (ORQ): 6.3333\n - sqrt(x + a): 6.3333\n - Yeo-Johnson: 6.3333\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized asinh(x) Transformation with 15 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 0.0119984 \n - sd (before standardization) = 0.01264569 \n_____________________________________\n\n## Results of bestNormalize for TEB ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.3333\n - Box-Cox: 1.2667\n - Center+scale: 6.0667\n - Exp(x): 18.0667\n - Log_b(x+a): 2.3333\n - orderNorm (ORQ): 0.4667\n - sqrt(x + a): 5.5333\n - Yeo-Johnson: 1.5333\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and ties\n - 10 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n41.6 43.3 45.1 51.4 61.7 \n_____________________________________\n\n## Results of bestNormalize for ECEC ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.2667\n - Box-Cox: 1.2667\n - Center+scale: 6.0667\n - Exp(x): 18.0667\n - Log_b(x+a): 1.2667\n - orderNorm (ORQ): 0.4667\n - sqrt(x + a): 3.9333\n - Yeo-Johnson: 1.5333\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and ties\n - 10 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n41.7 43.3 45.3 51.5 61.8 \n_____________________________________\n\n## Results of bestNormalize for minNO3 ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.8\n - Box-Cox: 1.2667\n - Center+scale: 3.9333\n - Exp(x): 21.2667\n - Log_b(x+a): 1.8\n - orderNorm (ORQ): 1.2667\n - sqrt(x + a): 1.8\n - Yeo-Johnson: 1.2667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized Box Cox Transformation with 15 nonmissing obs.:\n Estimated statistics:\n - lambda = -0.3344538 \n - mean (before standardization) = 1.907818 \n - sd (before standardization) = 0.2388776 \n_____________________________________\n\n## Results of bestNormalize for minTIN ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.8\n - Box-Cox: 1.8\n - Center+scale: 3.9333\n - Exp(x): 21.2667\n - Log_b(x+a): 1.8\n - orderNorm (ORQ): 0.7333\n - sqrt(x + a): 1.8\n - Yeo-Johnson: 1.8\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and ties\n - 14 unique values \n - Original quantiles:\n     0%     25%     50%     75%    100% \n  8.760  14.995  23.080  39.935 110.750 \n_____________________________________\n\n## Results of bestNormalize for DOC ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.3333\n - Box-Cox: 1.2667\n - Center+scale: 2.8667\n - Exp(x): 21.2667\n - Log_b(x+a): 2.3333\n - orderNorm (ORQ): 0.4667\n - sqrt(x + a): 2.8667\n - Yeo-Johnson: 1.2667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n     0%     25%     50%     75%    100% \n 30.800  44.070  56.680  64.385 118.740 \n_____________________________________\n\n## Results of bestNormalize for DOCN ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.3333\n - Box-Cox: 0.7333\n - Center+scale: 3.9333\n - Exp(x): 21.2667\n - Log_b(x+a): 2.3333\n - orderNorm (ORQ): 0.2\n - sqrt(x + a): 3.1333\n - Yeo-Johnson: 0.2\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 9.440 10.290 11.150 12.095 19.270 \n_____________________________________\n\n## Results of bestNormalize for micCN ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 3.1333\n - Box-Cox: 3.1333\n - Center+scale: 3.1333\n - Exp(x): 5.5333\n - Log_b(x+a): 3.1333\n - orderNorm (ORQ): 0.2\n - sqrt(x + a): 3.1333\n - Yeo-Johnson: 3.1333\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and ties\n - 14 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n4.420 4.585 4.720 5.415 5.850 \n_____________________________________\n\n## Results of bestNormalize for BG_ase ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.8667\n - Box-Cox: 2.3333\n - Center+scale: 5\n - Exp(x): 21.2667\n - Log_b(x+a): 2.8667\n - orderNorm (ORQ): 0.4667\n - sqrt(x + a): 2.8667\n - Yeo-Johnson: 2.3333\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 3.010  3.735  4.220  5.795 10.480 \n_____________________________________\n\n## Results of bestNormalize for BP_ase ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.5333\n - Box-Cox: 1.5333\n - Center+scale: 3.1333\n - Exp(x): 11.9333\n - Log_b(x+a): 0.7333\n - orderNorm (ORQ): 0.7333\n - sqrt(x + a): 3.1333\n - Yeo-Johnson: 1.5333\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized Log_b(x + a) Transformation with 15 nonmissing obs.:\n Relevant statistics:\n - a = 0 \n - b = 10 \n - mean (before standardization) = 0.5217757 \n - sd (before standardization) = 0.2001498 \n_____________________________________\n\n## Results of bestNormalize for CE_ase ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1\n - Box-Cox: 0.7333\n - Center+scale: 1.8\n - Exp(x): 4.2\n - Log_b(x+a): 0.4667\n - orderNorm (ORQ): 1\n - sqrt(x + a): 1\n - Yeo-Johnson: 0.4667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized Log_b(x + a) Transformation with 15 nonmissing obs.:\n Relevant statistics:\n - a = 0 \n - b = 10 \n - mean (before standardization) = 0.03306978 \n - sd (before standardization) = 0.1652591 \n_____________________________________\n\n## Results of bestNormalize for P_ase ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.0667\n - Box-Cox: 1.2667\n - Center+scale: 3.6667\n - Exp(x): 21.2667\n - Log_b(x+a): 2.0667\n - orderNorm (ORQ): 0.2\n - sqrt(x + a): 2.3333\n - Yeo-Johnson: 1.2667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n13.020 15.130 19.210 24.455 44.040 \n_____________________________________\n\n## Results of bestNormalize for N_ase ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.2667\n - Box-Cox: 1.8\n - Center+scale: 3.1333\n - Exp(x): 17.8\n - Log_b(x+a): 1.2667\n - orderNorm (ORQ): 0.2\n - sqrt(x + a): 3.1333\n - Yeo-Johnson: 1.2667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n  0%  25%  50%  75% 100% \n2.57 2.93 3.58 5.04 9.43 \n_____________________________________\n\n## Results of bestNormalize for XY_ase ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 0.7333\n - Box-Cox: 0.7333\n - Center+scale: 2.8667\n - Exp(x): 6.3333\n - Log_b(x+a): 0.7333\n - orderNorm (ORQ): 0.2\n - sqrt(x + a): 1.8\n - Yeo-Johnson: 0.7333\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and ties\n - 13 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n0.66 0.81 1.11 1.39 2.52 \n_____________________________________\n\n## Results of bestNormalize for BG_Q10 ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.3333\n - Box-Cox: 2.3333\n - Center+scale: 2.3333\n - Exp(x): 6.0667\n - Log_b(x+a): 2.3333\n - orderNorm (ORQ): 0.2\n - sqrt(x + a): 2.3333\n - Yeo-Johnson: 2.3333\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and ties\n - 13 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n1.310 1.360 1.390 1.545 1.760 \n_____________________________________\n\n## Results of bestNormalize for BP_Q10 ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.6\n - Box-Cox: 2.6\n - Center+scale: 2.6\n - Exp(x): 2.6\n - Log_b(x+a): 2.6\n - orderNorm (ORQ): 0.7333\n - sqrt(x + a): 2.6\n - Yeo-Johnson: 2.3333\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and ties\n - 11 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n1.340 1.415 1.440 1.530 1.740 \n_____________________________________\n\n## Results of bestNormalize for CO2 ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.5333\n - Box-Cox: 2.6\n - Center+scale: 11.9333\n - Exp(x): 21.2667\n - Log_b(x+a): 1.5333\n - orderNorm (ORQ): 1.2667\n - sqrt(x + a): 3.4\n - Yeo-Johnson: 2.0667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and ties\n - 14 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 2.080  4.370  5.270 10.585 40.470 \n_____________________________________\n\n## Results of bestNormalize for PX_ase ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.3333\n - Box-Cox: 1.2667\n - Center+scale: 7.6667\n - Exp(x): 21.2667\n - Log_b(x+a): 2.3333\n - orderNorm (ORQ): 0.2\n - sqrt(x + a): 2.8667\n - Yeo-Johnson: 1.2667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and no ties \n - Original quantiles:\n     0%     25%     50%     75%    100% \n 81.250  95.565 119.750 218.370 339.600 \n_____________________________________\n\n## Results of bestNormalize for PX_Q10 ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 14.6\n - Box-Cox: 4.2\n - Center+scale: 21.2667\n - Exp(x): 21.2667\n - Log_b(x+a): 11.9333\n - orderNorm (ORQ): 2.0667\n - sqrt(x + a): 21.2667\n - Yeo-Johnson: 1.5333\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized Yeo-Johnson Transformation with 15 nonmissing obs.:\n Estimated statistics:\n - lambda = -4.999946 \n - mean (before standardization) = 0.1968247 \n - sd (before standardization) = 0.001294011 \n_____________________________________\n\n## Results of bestNormalize for CUEcp ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 3.9333\n - Box-Cox: 1.5333\n - Center+scale: 3.9333\n - Exp(x): 3.9333\n - Log_b(x+a): 2.0667\n - orderNorm (ORQ): 1.5333\n - sqrt(x + a): 2.0667\n - Yeo-Johnson: 2.0667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized Box Cox Transformation with 15 nonmissing obs.:\n Estimated statistics:\n - lambda = -0.9999576 \n - mean (before standardization) = -4.229896 \n - sd (before standardization) = 0.9115063 \n_____________________________________\n\n## Results of bestNormalize for PUE ## \nBest Normalizing transformation with 15 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 8.2\n - Box-Cox: 9.8\n - Center+scale: 8.2\n - Exp(x): 3.9333\n - Log_b(x+a): 9.8\n - orderNorm (ORQ): 2.0667\n - sqrt(x + a): 9.8\n - Yeo-Johnson: 9.8\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 15 nonmissing obs and ties\n - 7 unique values \n - Original quantiles:\n   0%   25%   50%   75%  100% \n0.770 0.880 0.900 0.905 0.920 \n_____________________________________\n\n\nGreat, now we can add the normalized transformed data back to our mctoolsr metadata file.\n\nCodessu18_select_mc_norm <- ssu18_select_mc\n\nfor (i in md_to_tranform) {\n      tmp_get <- get(purrr::map_chr(i, ~ paste0(i, \"_best_norm_test\")))\n      tmp_new_data <- tmp_get$x.t\n      ssu18_select_mc_norm$map_loaded[[i]] <- tmp_new_data\n      rm(list = ls(pattern = \"tmp_\"))\n}     \n\n\nAnd rerun the Shapiro Tests.\n\nCodetemp_md_norm <- ssu18_select_mc_norm$map_loaded\ntemp_md_norm[,1:9] <- NULL\nshap_results_norm <- NULL\n\nfor (i in colnames(temp_md_norm)) {\n  tmp_shap <- shapiro.test(temp_md_norm[[i]])\n  tmp_p <- round(tmp_shap$p.value, digits = 5)\n  tmp_res <- eval(isTRUE(tmp_shap$p.value < 0.05))\n  shap_results_norm <- rbind(shap_results_norm, data.frame(i, tmp_p, tmp_res))\n  rm(list = ls(pattern = \"tmp_\"))\n}\ncolnames(shap_results_norm) <- c(\"parameter\", \"p-value\", \"tranform\")\nshap_results_norm\nrm(list = ls(pattern = \"temp_md_norm\"))\n\n\nAnd check if there are any parameters that are still significant for the normality test.\n\nshap_results$parameter[shap_results_norm$tranform == TRUE]\n\n[1] \"Al\" \"Fe\"\n\n\nOk. Looks like bestNormalize was unable to find a suitable transformation for Al and Fe. This is likely because there is very little variation in these metadata and/or there are too few significant digits."
  },
  {
    "objectID": "metadata.html#normalized-metadata",
    "href": "metadata.html#normalized-metadata",
    "title": "8. Metadata Analysis",
    "section": "Normalized Metadata",
    "text": "Normalized Metadata\nFinally, here is a new summary table that includes all of the normalized data.\n\n\n\n\n Download data as csv file\n\n\n\n(16S rRNA) Table 3 | Results of bestNormalize function applied to each parameter."
  },
  {
    "objectID": "metadata.html#autocorrelation-tests",
    "href": "metadata.html#autocorrelation-tests",
    "title": "8. Metadata Analysis",
    "section": "Autocorrelation Tests",
    "text": "Autocorrelation Tests\nNext, we test the metadata for autocorrelations. Do we do this on the original data or the transformed data? No idea, so let’s do both.\nSplit Metadata\nWe need to split the data into different groups.\n\nEnvironmental and edaphic properties\nMicrobial functional responses\nTemperature adaptation properties\n\nWe first create lists of metadata parameters.\n\nCodediv <- c(\"PLOT\", \"TREAT\", \"TREAT_T\", \"PAIR\", \"Observed\", \"Shannon_exp\", \n         \"InvSimpson\", \"ATAP\")\n\nedaphic <- c(\"AST\", \"H2O\", \"N\", \"P\", \"Al\", \n             \"Ca\", \"Fe\", \"K\", \"Mg\", \"Mn\", \"Na\", \"TEB\", \"ECEC\", \"pH\", \n             \"NH4\", \"NO3\", \"PO4\", \"DOC\", \"DON\", \"DOCN\")\n\nsoil_funct <- c(\"micC\", \"micN\", \"micP\", \"micCN\", \"micCP\", \"micNP\", \n                \"AG_ase\", \"BG_ase\", \"BP_ase\", \"CE_ase\", \"P_ase\", \"N_ase\", \n                \"S_ase\", \"XY_ase\",  \"LP_ase\", \"PX_ase\", \"CO2\", \n                \"enzCN\", \"enzCP\", \"enzNP\")\n\ntemp_adapt <- c(\"AG_Q10\", \"BG_Q10\", \"BP_Q10\", \"CE_Q10\", \"P_Q10\", \"N_Q10\", \n                \"S_Q10\", \"XY_Q10\", \"LP_Q10\", \"PX_Q10\", \"CUEcn\", \"CUEcp\", \n                \"NUE\",\"PUE\", \"Tmin\", \"SI\")\n\nmd_groups <- c(\"edaphic\", \"soil_funct\", \"temp_adapt\")\n\n# NoT uSed: minPO4, minNH4, minNO3, minTIN\n\n\nAnd then use the lists to split the data sets by metadata group. Here, we do this for the original metadata and the metadata after normalization.\n\nCodeselect_md <- c(\"ssu18_select_mc\", \"ssu18_select_mc_norm\")\nfor (i in select_md) {\n   #tmp_get <- get(purrr::map_chr(i, ~ paste0(i, \"_mc\")))\n   tmp_get <- get(i)\n   tmp_md_all <- tmp_get$map_loaded\n   tmp_div <- tmp_md_all %>% dplyr::select(all_of(div))\n   tmp_div <- tmp_div %>% tibble::rownames_to_column(\"SampleID\")\n## edaphic      \n   tmp_sub_edaphic <- tmp_md_all %>% dplyr::select(all_of(edaphic))\n   tmp_sub_edaphic <- tmp_sub_edaphic %>% tibble::rownames_to_column(\"SampleID\")\n   tmp_edaphic <- dplyr::left_join(tmp_div, tmp_sub_edaphic, by = \"SampleID\")\n   tmp_edaphic <- tmp_edaphic %>% tibble::column_to_rownames(\"SampleID\")\n## soil_funct\n   tmp_sub_soil_funct <- tmp_md_all %>% dplyr::select(all_of(soil_funct))\n   tmp_sub_soil_funct <- tmp_sub_soil_funct %>% tibble::rownames_to_column(\"SampleID\")\n   tmp_soil_funct <- dplyr::left_join(tmp_div, tmp_sub_soil_funct, by = \"SampleID\")\n   tmp_soil_funct <- tmp_soil_funct %>% tibble::column_to_rownames(\"SampleID\")      \n## temp_adapt\n   tmp_sub_temp_adapt <- tmp_md_all %>% dplyr::select(all_of(temp_adapt))\n   tmp_sub_temp_adapt <- tmp_sub_temp_adapt %>% tibble::rownames_to_column(\"SampleID\")\n   tmp_temp_adapt <- dplyr::left_join(tmp_div, tmp_sub_temp_adapt, by = \"SampleID\")\n   tmp_temp_adapt <- tmp_temp_adapt %>% tibble::column_to_rownames(\"SampleID\")      \n## combine\n   tmp_list <- list(data_loaded = ssu18_select_mc$data_loaded, \n                    map_loaded = ssu18_select_mc$map_loaded, \n                    taxonomy_loaded = ssu18_select_mc$taxonomy_loaded,\n                    edaphic = tmp_edaphic, \n                    soil_funct = tmp_soil_funct, \n                    temp_adapt = tmp_temp_adapt)\n   tmp_name <- purrr::map_chr(i, ~ paste0(., \"_split\"))\n   print(tmp_name)\n   assign(tmp_name, tmp_list)\n   rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nGenerate Autocorrelation Plots\nA little housekeeping to get rid of parameters we don’t need (e.g., plot number, pair, etc.).\n\nCodeedaphic_cor <- ssu18_select_mc_split$edaphic\nedaphic_cor[,1:8] <- NULL\nedaphic_norm_cor <- ssu18_select_mc_norm_split$edaphic\nedaphic_norm_cor[,1:8] <- NULL\n\nsoil_funct_cor <- ssu18_select_mc_split$soil_funct\nsoil_funct_cor[,1:8] <- NULL\nsoil_funct_norm_cor <- ssu18_select_mc_norm_split$soil_funct\nsoil_funct_norm_cor[,1:8] <- NULL\n\ntemp_adapt_cor <- ssu18_select_mc_split$temp_adapt\ntemp_adapt_cor[,1:8] <- NULL\ntemp_adapt_norm_cor <- ssu18_select_mc_norm_split$temp_adapt\ntemp_adapt_norm_cor[,1:8] <- NULL\n\n\nAnd finally the code to create the plots.\n\nCodefor (i in objects(pattern = \"_cor$\")) {\n  tmp_get <- get(i)\n  tmp_cormat <- round(cor(tmp_get), 2)\n  tmp_melted_cormat <- reshape2::melt(tmp_cormat)\n  \n  tmp_get_lower_tri <- function(tmp_cormat){\n    tmp_cormat[upper.tri(tmp_cormat)] <- NA\n    return(tmp_cormat)\n    }\n  # Get upper triangle of the correlation matrix\n  tmp_get_upper_tri <- function(tmp_cormat){\n    tmp_cormat[lower.tri(tmp_cormat)] <- NA\n    return(tmp_cormat)\n    }\n  tmp_upper_tri <- tmp_get_upper_tri(tmp_cormat)\n  tmp_melted_cormat <- reshape2::melt(tmp_upper_tri, na.rm = TRUE)\n  ggplot(data = tmp_melted_cormat, aes(x = Var1, y = Var2, fill = value)) + \n    geom_tile()\n  \n  tmp_ggheatmap <- ggplot(data = tmp_melted_cormat, aes(Var2, Var1, fill = value)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n                         midpoint = 0, limit = c(-1,1), space = \"Lab\", \n                         name=\"Pearson\\nCorrelation\") +\n    theme_minimal() + \n    theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                     size = 7, hjust = 1),\n          axis.text.y = element_text(vjust = 1, size = 7, hjust = 1)) + \n    coord_fixed() + \n    geom_text(aes(Var2, Var1, label = value), color = \"black\", size = 1.75) +\n    theme(\n      axis.title.x = element_blank(),\n      axis.title.y = element_blank(),\n      panel.grid.major = element_blank(),\n      panel.border = element_blank(),\n      panel.background = element_blank(),\n      axis.ticks = element_blank(),\n      legend.justification = c(1, 0),\n      legend.position = c(0.6, 0.7),\n      legend.direction = \"horizontal\") +\n      guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                                   title.position = \"top\", \n                                   title.hjust = 0.5))\n  tmp_name <- purrr::map_chr(i, ~ paste0(., \"_ggheatmap\"))\n  assign(tmp_name, tmp_ggheatmap)\n  print(tmp_name)\n  rm(list = ls(pattern = \"tmp_\"))\n}  \nobjects(pattern = \"_ggheatmap\")"
  },
  {
    "objectID": "metadata.html#autocorrelation-plots",
    "href": "metadata.html#autocorrelation-plots",
    "title": "8. Metadata Analysis",
    "section": "Autocorrelation Plots",
    "text": "Autocorrelation Plots\n\n\n\n\nEdaphic properties\nEdaphic properties (normalized)\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 1 | Results of autocorrelation analysis for non-normalized Environmental and edaphic parameters.\n\n\n\n\n\n\n\n(16S rRNA) Figure 2 | Results of autocorrelation analysis for normalized Environmental and edaphic parameters.\n\n\n\n\n\nFunctional responses\nFunctional responses (normalized)\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 3 | Results of autocorrelation analysis for non-normalized Functional response parameters.\n\n\n\n\n\n\n\n(16S rRNA) Figure 4 | Results of autocorrelation analysis for normalized Functional response parameters.\n\n\n\n\n\nTemperature adaptation\nTemperature adaptation (normalized)\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 5 | Results of autocorrelation analysis for non-normalized Temperature adaptation parameters.\n\n\n\n\n\n\n\n(16S rRNA) Figure 6 | Results of autocorrelation analysis for normalized Temperature adaptation parameters.\n\n\n\n\n\n\n\nNow we can remove parameters based on the autocorrelation analysis:\n\nEnvironmental and edaphic properties: TEB, DON, Na, Al, Ca.\nMicrobial functional responses: micN, micNP, enzCN, enzCP, BPase, CEase, LPase, Nase, Pase.\nTemperature adaptation properties: NUE, PUE, SI.\n\n\n\n\n\nedaphic_remove <- c(\"TEB\", \"DON\", \"Na\", \"Al\", \"Ca\")\nsoil_funct_remove <- c(\"micN\", \"micNP\", \"enzCN\", \"enzCP\", \"BP_ase\", \n                       \"CE_ase\", \"LP_ase\", \"N_ase\", \"P_ase\")\ntemp_adapt_remove <- c(\"NUE\", \"PUE\", \"SI\")\n\n\nCodetmp_df <- ssu18_select_mc_split\ntmp_df$edaphic <- tmp_df$edaphic[, ! names(tmp_df$edaphic) %in%  edaphic_remove]\ntmp_df$soil_funct <- tmp_df$soil_funct[, ! names(tmp_df$soil_funct) %in%  soil_funct_remove]\ntmp_df$temp_adapt <- tmp_df$temp_adapt[, ! names(tmp_df$temp_adapt) %in%  temp_adapt_remove]\nssu18_select_mc_split_no_ac <- tmp_df\nrm(list = ls(pattern = \"tmp_\"))\n\ntmp_df <- ssu18_select_mc_norm_split\ntmp_df$edaphic <- tmp_df$edaphic[, ! names(tmp_df$edaphic) %in%  edaphic_remove]\ntmp_df$soil_funct <- tmp_df$soil_funct[, ! names(tmp_df$soil_funct) %in%  soil_funct_remove]\ntmp_df$temp_adapt <- tmp_df$temp_adapt[, ! names(tmp_df$temp_adapt) %in%  temp_adapt_remove]\nssu18_select_mc_norm_split_no_ac <- tmp_df\nrm(list = ls(pattern = \"tmp_\"))"
  },
  {
    "objectID": "metadata.html#dissimilarity-correlation-tests",
    "href": "metadata.html#dissimilarity-correlation-tests",
    "title": "8. Metadata Analysis",
    "section": "Dissimilarity Correlation Tests",
    "text": "Dissimilarity Correlation Tests\nLet’s see if any on the metadata groups are significantly correlated with the community data. Basically, we create distance matrices for the community data and each metadata group and then run Mantel tests for all comparisons. For the community data we calculate Bray-Curtis distances for the community data and Euclidean distances for the metadata. We use the function mantel.test from the ape package and mantel from the vegan package for the analyses.\nIn summary, we test both mantel.test and mantel on Bray-Curtis distance community distances against Euclidean distances for each metadata group (edaphic, soil_funct, temp_adapt) a) before normalizing and before removing autocorrelated parameters, b) before normalizing and after removing autocorrelated parameters, c) after normalizing and before removing autocorrelated parameters, and d) after normalizing and after removing autocorrelated parameters.\n\nCodeman_df <- c(\"ssu18_select_mc_split\", \"ssu18_select_mc_split_no_ac\", \n            \"ssu18_select_mc_norm_split\", \"ssu18_select_mc_norm_split_no_ac\")\n\nfor (i in man_df) {\n  tmp_get <- get(i)\n  tmp_dm_otu <- as.matrix(vegdist(t(tmp_get$data_loaded), \n                             method = \"bray\", binary = FALSE, \n                             diag = TRUE, upper = TRUE, na.rm = FALSE))\n# EDAPHIC\n  tmp_dm_md_edaphic <- as.matrix(vegdist(tmp_get$edaphic[, 8:ncol(tmp_get$edaphic)], \n                                    method =\"euclidean\", binary = FALSE, \n                                    diag = TRUE, upper = TRUE, na.rm = FALSE))\n\n  tmp_man1_edaphic <- mantel.test(tmp_dm_otu, tmp_dm_md_edaphic, nperm = 999, \n                             graph = FALSE, alternative = \"two.sided\")\n  tmp_man2_edaphic <- mantel(tmp_dm_otu, tmp_dm_md_edaphic, permutations = 999)\n# SOIL FUNCT\n  tmp_dm_md_soil_funct <- as.matrix(vegdist(tmp_get$soil_funct[, 8:ncol(tmp_get$soil_funct)], \n                                       method = \"euclidean\", binary = FALSE, \n                                       diag = TRUE, upper = TRUE, na.rm = FALSE))\n\n  tmp_man1_soil_funct <- mantel.test(tmp_dm_otu, tmp_dm_md_soil_funct, nperm = 999, \n                                graph = FALSE, alternative = \"two.sided\")\n  tmp_man2_soil_funct <- mantel(tmp_dm_otu, tmp_dm_md_soil_funct, permutations = 999)\n\n# TEMP ADAPT\n  tmp_dm_md_temp_adapt <- as.matrix(vegdist(tmp_get$temp_adapt[, 8:ncol(tmp_get$temp_adapt)], \n                                       method = \"euclidean\", binary = FALSE, \n                                       diag = TRUE, upper = TRUE, na.rm = FALSE))\n\n  tmp_man1_temp_adapt <- mantel.test(tmp_dm_otu, tmp_dm_md_temp_adapt, nperm = 999, \n                                graph = FALSE, alternative = \"two.sided\")\n  tmp_man2_temp_adapt <- mantel(tmp_dm_otu, tmp_dm_md_temp_adapt, permutations = 999)\n\n  tmp_name <- purrr::map_chr(i, ~ paste0(., \"_mantel_tests\"))\n  tmp_df <- list(edaphic_ape_man = tmp_man1_edaphic, \n                 edaphic_vegan_man = tmp_man2_edaphic,\n                 soil_funct_ape_man = tmp_man1_soil_funct, \n                 soil_funct_vegan_man = tmp_man2_soil_funct,\n                 temp_adapt_ape_man = tmp_man1_temp_adapt, \n                 temp_adapt_vegan_man = tmp_man2_temp_adapt)\n  assign(tmp_name, tmp_df)\n  print(tmp_name)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\n\n\nDissimilarity Correlation Results\n\n(16S rRNA) Table 4 | Summary of Dissimilarity Correlation Tests using mantel.test from the ape package and mantel from the vegan. P-values in red indicate significance (p-value < 0.05)\n\n\n\n\n\n\n\nMoving on."
  },
  {
    "objectID": "metadata.html#best-subset-of-variables",
    "href": "metadata.html#best-subset-of-variables",
    "title": "8. Metadata Analysis",
    "section": "Best Subset of Variables",
    "text": "Best Subset of Variables\nNow we want to know which of the metadata parameters are the most strongly correlated with the community data. For this we use the bioenv function from the vegan package. bioenv—Best Subset of Environmental Variables with Maximum (Rank) Correlation with Community Dissimilarities—finds the best subset of environmental variables, so that the Euclidean distances of scaled environmental variables have the maximum (rank) correlation with community dissimilarities.\nNext we will use the metadata set where autocorrelated parameters were removed and the remainder of the parameters were normalized (where applicable based on the Shapiro tests).\nWe run bioenv against the three groups of metadata parameters. We then run bioenv again, but this time against the individual parameters identified as significantly correlated.\nEdaphic Properties\n\nCodetmp_comm <- data.frame(t(ssu18_select_mc_norm_split_no_ac$data_loaded))\ntmp_env <- data.frame(ssu18_select_mc_norm_split_no_ac$edaphic)\ntmp_env[,1:8] <- NULL\n\nedaphic_bioenv <- bioenv(wisconsin(tmp_comm), tmp_env, \n                         method = \"spearman\", index = \"bray\", \n                         upto = ncol(tmp_env), metric = \"euclidean\")\n\nbioenv_list <- edaphic_bioenv$models[[edaphic_bioenv$whichbest]]$best\nbioenv_best <- bioenvdist(edaphic_bioenv, which = \"best\")\n\nfor (i in bioenv_list) {\n       tmp_dp <- data.frame(edaphic_bioenv$x)\n       tmp_md <- as.matrix(vegdist(tmp_dp[[i]], \n                                   method = \"euclidean\", binary = FALSE, \n                                   diag = TRUE, upper = TRUE, na.rm = FALSE))\n       tmp_man <- mantel(bioenv_best, tmp_md, \n                         permutations = 999, method = \"spearman\")\n       tmp_md_name <- names(tmp_dp)[[i]]\n       tmp_name <- purrr::map_chr(tmp_md_name, ~ paste0(., \"_bioenv_mantel_test\"))\n       assign(tmp_name, tmp_man)\n       rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_bioenv_mantel_test\")\nedaphic_bioenv_ind_mantel <- list(AST = AST_bioenv_mantel_test)\nrm(list = ls(pattern = \"_bioenv_mantel_test\"))\n\n\n\n\n\nCall:\nbioenv(comm = wisconsin(tmp_comm), env = tmp_env, method = \"spearman\",      index = \"bray\", upto = ncol(tmp_env), metric = \"euclidean\") \n\nSubset of environmental variables with best correlation to community data.\n\nCorrelations:    spearman \nDissimilarities: bray \nMetric:          euclidean \n\nBest model has 1 parameters (max. 15 allowed):\nAST\nwith correlation  0.6808861 \n\n\n\nShow the results of individual edaphic metadata Mantel tests\n\n\n$AST\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r:     1 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.177 0.226 0.271 0.326 \nPermutation: free\nNumber of permutations: 999\n\n\nbioenv found the following edaphic properties significantly correlated with the community data: AST\nSoil Functional Response\n\nCodetmp_comm <- data.frame(t(ssu18_select_mc_norm_split_no_ac$data_loaded))\ntmp_env <- data.frame(ssu18_select_mc_norm_split_no_ac$soil_funct)\ntmp_env[,1:8] <- NULL\n\nsoil_funct_bioenv <- bioenv(wisconsin(tmp_comm), tmp_env, \n                         method = \"spearman\", index = \"bray\", \n                         upto = ncol(tmp_env), metric = \"euclidean\")\n\nbioenv_list <- soil_funct_bioenv$models[[soil_funct_bioenv$whichbest]]$best\nbioenv_best <- bioenvdist(soil_funct_bioenv, which = \"best\")\n\nfor (i in bioenv_list) {\n       tmp_dp <- data.frame(soil_funct_bioenv$x)\n       tmp_md <- as.matrix(vegdist(tmp_dp[[i]], \n                                   method = \"euclidean\", binary = FALSE, \n                                   diag = TRUE, upper = TRUE, na.rm = FALSE))\n       tmp_man <- mantel(bioenv_best, tmp_md, \n                         permutations = 999, method = \"spearman\")\n       tmp_md_name <- names(tmp_dp)[[i]]\n       tmp_name <- purrr::map_chr(tmp_md_name, ~ paste0(., \"_bioenv_mantel_test\"))\n       assign(tmp_name, tmp_man)\n       rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_bioenv_mantel_test\")\n\nsoil_funct_bioenv_ind_mantel <- list(AG_ase = AG_ase_bioenv_mantel_test, \n                                     enzNP = enzNP_bioenv_mantel_test, \n                                     S_ase = S_ase_bioenv_mantel_test, \n                                     PX_ase = PX_ase_bioenv_mantel_test,\n                                     XY_ase = XY_ase_bioenv_mantel_test)\nrm(list = ls(pattern = \"_bioenv_mantel_test\"))\n\n\n\n\n\nCall:\nbioenv(comm = wisconsin(tmp_comm), env = tmp_env, method = \"spearman\",      index = \"bray\", upto = ncol(tmp_env), metric = \"euclidean\") \n\nSubset of environmental variables with best correlation to community data.\n\nCorrelations:    spearman \nDissimilarities: bray \nMetric:          euclidean \n\nBest model has 5 parameters (max. 11 allowed):\nAG_ase S_ase XY_ase PX_ase enzNP\nwith correlation  0.7727866 \n\n\n\nShow the results of individual functional response metadata Mantel tests\n\n\n$AG_ase\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.5588 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.186 0.235 0.281 0.330 \nPermutation: free\nNumber of permutations: 999\n\n\n$enzNP\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.4623 \n      Significance: 0.006 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.219 0.280 0.336 0.413 \nPermutation: free\nNumber of permutations: 999\n\n\n$S_ase\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.6139 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.223 0.305 0.384 0.466 \nPermutation: free\nNumber of permutations: 999\n\n\n$PX_ase\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.6115 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.176 0.222 0.289 0.346 \nPermutation: free\nNumber of permutations: 999\n\n\n$XY_ase\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.456 \n      Significance: 0.002 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.189 0.258 0.304 0.347 \nPermutation: free\nNumber of permutations: 999\n\n\nbioenv found the following soil functions significantly correlated with the community data: AG_ase, enzNP, S_ase, PX_ase, XY_ase\nTemperature Adaptation\n\nCodetmp_comm <- data.frame(t(ssu18_select_mc_norm_split_no_ac$data_loaded))\ntmp_env <- data.frame(ssu18_select_mc_norm_split_no_ac$temp_adapt)\ntmp_env[,1:8] <- NULL\n\ntemp_adapt_bioenv <- bioenv(wisconsin(tmp_comm), tmp_env, \n                         method = \"spearman\", index = \"bray\", \n                         upto = ncol(tmp_env), metric = \"euclidean\")\n\nbioenv_list <- temp_adapt_bioenv$models[[temp_adapt_bioenv$whichbest]]$best\nbioenv_best <- bioenvdist(temp_adapt_bioenv, which = \"best\")\n\nfor (i in bioenv_list) {\n       tmp_dp <- data.frame(temp_adapt_bioenv$x)\n       tmp_md <- as.matrix(vegdist(tmp_dp[[i]], \n                                   method = \"euclidean\", binary = FALSE, \n                                   diag = TRUE, upper = TRUE, na.rm = FALSE))\n       tmp_man <- mantel(bioenv_best, tmp_md, \n                         permutations = 999, method = \"spearman\")\n       tmp_md_name <- names(tmp_dp)[[i]]\n       tmp_name <- purrr::map_chr(tmp_md_name, ~ paste0(., \"_bioenv_mantel_test\"))\n       assign(tmp_name, tmp_man)\n       rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_bioenv_mantel_test\")\n\ntemp_adapt_bioenv_ind_mantel <- list(CUEcp = CUEcp_bioenv_mantel_test, \n                                     LP_Q10 = LP_Q10_bioenv_mantel_test, \n                                     P_Q10 = P_Q10_bioenv_mantel_test, \n                                     S_Q10 = S_Q10_bioenv_mantel_test, \n                                     Tmin = Tmin_bioenv_mantel_test)\nrm(list = ls(pattern = \"_bioenv_mantel_test\"))\n\n\n\n\n\nCall:\nbioenv(comm = wisconsin(tmp_comm), env = tmp_env, method = \"spearman\",      index = \"bray\", upto = ncol(tmp_env), metric = \"euclidean\") \n\nSubset of environmental variables with best correlation to community data.\n\nCorrelations:    spearman \nDissimilarities: bray \nMetric:          euclidean \n\nBest model has 5 parameters (max. 13 allowed):\nP_Q10 S_Q10 LP_Q10 CUEcp Tmin\nwith correlation  0.4237093 \n\n\n\nShow the results of individual temperature adaptation metadata Mantel tests\n\n\n$CUEcp\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.325 \n      Significance: 0.013 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.188 0.226 0.280 0.331 \nPermutation: free\nNumber of permutations: 999\n\n\n$LP_Q10\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.3772 \n      Significance: 0.005 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.203 0.272 0.306 0.351 \nPermutation: free\nNumber of permutations: 999\n\n\n$P_Q10\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.5177 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.158 0.211 0.261 0.309 \nPermutation: free\nNumber of permutations: 999\n\n\n$S_Q10\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.4395 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.159 0.205 0.252 0.301 \nPermutation: free\nNumber of permutations: 999\n\n\n$Tmin\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.4044 \n      Significance: 0.005 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.207 0.267 0.305 0.359 \nPermutation: free\nNumber of permutations: 999\n\n\nbioenv found the following temperature adaptations significantly correlated with the community data: CUEcp, LP_Q10, P_Q10, S_Q10, Tmin"
  },
  {
    "objectID": "metadata.html#distance-based-redundancy",
    "href": "metadata.html#distance-based-redundancy",
    "title": "8. Metadata Analysis",
    "section": "Distance-based Redundancy",
    "text": "Distance-based Redundancy\nNow we turn our attention to distance-based redundancy analysis (dbRDA), an ordination method similar to Redundancy Analysis (rda) but it allows non-Euclidean dissimilarity indices, such as Manhattan or Bray–Curtis distance. For this, we use capscale from the vegan package. capscale is a constrained versions of metric scaling (principal coordinates analysis), which are based on the Euclidean distance but can be used, and are more useful, with other dissimilarity measures. The functions can also perform unconstrained principal coordinates analysis, optionally using extended dissimilarities.\nFor each of the three metadata subsets, we perform the following steps:\n\nRun rankindex to compare metadata and community dissimilarity indices for gradient detection. This will help us select the best dissimilarity metric to use.\nRun capscale for distance-based redundancy analysis.\nRun envfit to fit environmental parameters onto the ordination. This function basically calculates correlation scores between the metadata parameters and the ordination axes.\nSelect metadata parameters significant for bioenv (see above) and/or envfit analyses.\nRun envfit on ASVs.\nPlot the ordination and vector overlays.\n\nEdaphic Properties\n\nCodetmp_md <- ssu18_select_mc_norm_split_no_ac$edaphic\ntmp_md$TREAT_T <- as.character(tmp_md$TREAT_T)\ntmp_comm <- data.frame(t(ssu18_select_mc_norm_split_no_ac$data_loaded))\nedaphic_rank <- rankindex(tmp_md[, 8:ncol(tmp_md)], tmp_comm, \n          indices = c(\"euc\", \"man\", \"gow\",\"bra\", \"kul\"), \n          stepacross = FALSE, method = \"spearman\")\n\n\n\n\n      euc       man       gow       bra       kul \n0.3171263 0.4336927 0.1028302 0.4336927 0.4336927 \n\n\nLet’s run capscale using Bray-Curtis. Note, we have 15 metadata parameters in this group but, for some reason, capscale only works with 13 parameters. This may have to do with degrees of freedom?\n\nStarting properties: AST, H2O, N, P, Al, Ca, Fe, K, Mg, Mn, Na, TEB, ECEC, pH, NH4, NO3, PO4, DOC, DON, DOCN\nAutocorrelated removed: TEB, DON, Na, Al, Ca\nRemove for capscale: Mg, Mn\n\n\n\n\n\nedaphic_cap <- capscale(tmp_comm ~ AST + H2O + N + P + Fe + K + ECEC + \n                                   pH + NH4 + NO3 + PO4 + DOC + DOCN, \n                        tmp_md, dist = \"bray\")\ncolnames(tmp_md)\n\n\n\nCall: capscale(formula = tmp_comm ~ AST + H2O + N + P + Fe + K + ECEC +\npH + NH4 + NO3 + PO4 + DOC + DOCN, data = tmp_md, distance = \"bray\")\n\n              Inertia Proportion Rank\nTotal         1.67890    1.00000     \nConstrained   1.63037    0.97110   13\nUnconstrained 0.04853    0.02890    1\nInertia is squared Bray distance \nSpecies scores projected from 'tmp_comm' \n\nEigenvalues for constrained axes:\n  CAP1   CAP2   CAP3   CAP4   CAP5   CAP6   CAP7   CAP8   CAP9  CAP10  CAP11 \n0.6287 0.2955 0.1984 0.1104 0.1085 0.0661 0.0536 0.0511 0.0359 0.0277 0.0247 \n CAP12  CAP13 \n0.0218 0.0080 \n\nEigenvalues for unconstrained axes:\n   MDS1 \n0.04853 \n\n\nNow we can look at the variance against each principal component.\n\n\n\n\n\nAnd then make some quick and dirty plots. This will also come in handy later when we need to parse out data a better plot visualization. The ggplot function autoplot stores these data in a more accessible way than the raw results from capscale\n\nbase::plot(edaphic_cap) \n\n\n\ntmp_auto_plt <- ggplot2::autoplot(edaphic_cap, arrows = TRUE)\ntmp_auto_plt\n\n\n\n\n\n\n\nNext, we need to grab capscale scores for the samples and create a data frame of the first two dimensions. We will also need to add some of the sample details to the data frame. For this we use the vegan function scores which gets species or site scores from the ordination.\n\nCodetmp_samp_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"sites\")\ntmp_samp_scores[,1] <- NULL\ntmp_samp_scores <- tmp_samp_scores %>% dplyr::rename(SampleID = Label)\n\ntmp_md_sub <- tmp_md[, 1:4]\ntmp_md_sub <- tmp_md_sub %>% tibble::rownames_to_column(\"SampleID\")\nedaphic_plot_data <- dplyr::left_join(tmp_md_sub, tmp_samp_scores, by = \"SampleID\")\n\n\nNow we have a new data frame that contains sample details and capscale values.\n\n\n          SampleID PLOT TREAT TREAT_T PAIR        CAP1           CAP2\n1  P10_D00_010_C0E  P10     C       0    E -0.51023381  0.16071056348\n2  P02_D00_010_C0A  P02     C       0    A -0.48448722 -0.19055366181\n3  P04_D00_010_C0B  P04     C       0    B -0.58218619  0.02325887477\n4  P06_D00_010_C0C  P06     C       0    C -0.53370906  0.09973976767\n5  P08_D00_010_C0D  P08     C       0    D -0.68492943  0.13143857676\n6  P01_D00_010_W3A  P01     W       3    A -0.13286443 -0.28661650806\n7  P03_D00_010_W3B  P03     W       3    B -0.25415783  0.00002258673\n8  P05_D00_010_W3C  P05     W       3    C -0.38829574 -0.08695154559\n9  P07_D00_010_W3D  P07     W       3    D -0.01360582 -0.49632449304\n10 P09_D00_010_W3E  P09     W       3    E -0.10639798 -0.54219706978\n11 P01_D00_010_W8A  P01     W       8    A  0.78689725  0.02214444812\n12 P03_D00_010_W8B  P03     W       8    B  0.89574754 -0.92786190328\n13 P05_D00_010_W8C  P05     W       8    C  1.07945322 -0.28874425650\n14 P07_D00_010_W8D  P07     W       8    D  0.41234456  0.75597843926\n15 P09_D00_010_W8E  P09     W       8    E  0.51642495  1.62595618128\n\n\nWe can then do the same with the metadata vectors. Here though we only need the scores and parameter name.\n\nCodeedaphic_md_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"biplot\")\nedaphic_md_scores[,1] <- NULL\nedaphic_md_scores <- edaphic_md_scores %>% \n  dplyr::mutate(parameters = Label, .before = CAP1) %>%\n  tibble::column_to_rownames(\"Label\")\n\n\n\n\n     parameters        CAP1         CAP2\nAST         AST  0.97882379 -0.055074169\nH2O         H2O -0.76548607 -0.129779379\nN             N  0.38130076 -0.131035479\nP             P  0.26536024 -0.132125558\nFe           Fe  0.12138586  0.353093177\nK             K -0.02490580  0.151975508\nECEC       ECEC -0.15365335  0.379848234\npH           pH -0.07327989  0.272676813\nNH4         NH4  0.19922925  0.516074241\nNO3         NO3 -0.27681227  0.288729283\nPO4         PO4 -0.60140669 -0.009980058\nDOC         DOC -0.59979130 -0.399930429\nDOCN       DOCN  0.11210058  0.422975484\n\n\nLet’s run some quick correlations of metadata with ordination axes to see which parameters are significant. For this we use the vegan function envfit.\n\nCodetmp_samp_scores_sub <- edaphic_plot_data[, 6:7]\ntmp_samp_scores_sub <- as.matrix(tmp_samp_scores_sub)\ntmp_param_list <- edaphic_md_scores$parameters\n\ntmp_md_sub <- subset(tmp_md,  select =  tmp_param_list)\n\nenvfit_edaphic_md <- envfit(tmp_samp_scores_sub, tmp_md_sub,\n                 perm = 1000, choices = c(1, 2))\n\n\n\n\n\n***VECTORS\n\n         CAP1     CAP2     r2   Pr(>r)    \nAST   0.99825 -0.05912 0.8287 0.000999 ***\nH2O  -0.98657 -0.16333 0.5189 0.009990 ** \nN     0.94537 -0.32599 0.1403 0.398601    \nP     0.89518 -0.44571 0.0759 0.602398    \nFe    0.32411  0.94602 0.1194 0.432567    \nK    -0.16554  0.98620 0.0204 0.890110    \nECEC -0.37874  0.92550 0.1444 0.368631    \npH   -0.26344  0.96468 0.0685 0.672328    \nNH4   0.35944  0.93317 0.2621 0.152847    \nNO3  -0.69392  0.72006 0.1380 0.417582    \nPO4  -0.99991 -0.01327 0.3118 0.090909 .  \nDOC  -0.83396 -0.55183 0.4459 0.023976 *  \nDOCN  0.25463  0.96704 0.1640 0.318681    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodeedaphic_md_signif_hits <- base::subset(envfit_edaphic_md$vectors$pvals, \n                                  c(envfit_edaphic_md$vectors$pvals \n                                  < 0.05 & envfit_edaphic_md$vectors$r > 0.4))\nedaphic_md_signif_hits <- data.frame(edaphic_md_signif_hits)\nedaphic_md_signif_hits <- rownames(edaphic_md_signif_hits)\nedaphic_md_signif <- edaphic_md_scores[edaphic_md_scores$parameters %in% edaphic_md_signif_hits,]\nedaphic_md_signif$parameters\n\n\nenvfit found that AST, H2O, DOC were significantly correlated.\nNow let’s see if the same parameters are significant for the envfit and bioenv analyses.\n\n\n[1] \"Significant parameters from bioenv analysis.\"\n[1] \"AST\"\n_____________________________________\n[1] \"Significant parameters from envfit analysis.\"\n[1] \"AST\" \"H2O\" \"DOC\"\n_____________________________________\n[1] \"Found in bioenv but not envfit.\"\ncharacter(0)\n_____________________________________\n[1] \"Found in envfit but not bioenv.\"\n[1] \"H2O\" \"DOC\"\n_____________________________________\n[1] \"Found in envfit and bioenv.\"\n[1] \"AST\" \"H2O\" \"DOC\"\n\n\n\nCodenew_edaphic_md_signif_hits <- edaphic_sig_diff\n#new_edaphic_md_signif_hits <- append(edaphic_md_signif_hits, edaphic_sig_diff)\nedaphic_md_signif_all <- edaphic_md_scores[edaphic_md_scores$parameters %in% new_edaphic_md_signif_hits,]\n\n\nCheck. Next, we run envfit for the ASVs.\n\nCodeenvfit_edaphic_asv <- envfit(tmp_samp_scores_sub, \n                             tmp_comm[, order(colSums(-tmp_comm))][, 1:10],\n                             perm = 1000, choices = c(1, 2))\nedaphic_asv_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"species\")\nedaphic_asv_scores <- edaphic_asv_scores %>% \n  dplyr::mutate(parameters = Label, .before = CAP1) %>%\n  tibble::column_to_rownames(\"Label\")\nedaphic_asv_scores[,1] <- NULL\n\n\n\n\n\n***VECTORS\n\n          CAP1     CAP2     r2   Pr(>r)    \nASV1   0.99502  0.09969 0.6491 0.005994 ** \nASV2   0.54850 -0.83615 0.8984 0.000999 ***\nASV3   0.79245  0.60994 0.9429 0.000999 ***\nASV4  -0.12958  0.99157 0.2913 0.120879    \nASV5  -0.56363 -0.82603 0.1586 0.362637    \nASV6  -0.10651  0.99431 0.7559 0.000999 ***\nASV8  -0.84329  0.53746 0.4370 0.030969 *  \nASV7  -0.52510 -0.85104 0.0681 0.655345    \nASV9   0.51828 -0.85521 0.2559 0.185814    \nASV10 -0.34456  0.93876 0.8331 0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodeedaphic_asv_signif_hits <- base::subset(envfit_edaphic_asv$vectors$pvals, \n                                  c(envfit_edaphic_asv$vectors$pvals \n                                  < 0.05 & envfit_edaphic_asv$vectors$r > 0.5))\nedaphic_asv_signif_hits <- data.frame(edaphic_asv_signif_hits)\nedaphic_asv_signif_hits <- rownames(edaphic_asv_signif_hits)\nedaphic_asv_signif <- edaphic_asv_scores[edaphic_asv_scores$parameters %in% edaphic_asv_signif_hits,]\n\n\n\n\n      parameters        CAP1        CAP2 variable_type\nASV3        ASV3  0.48974117  0.51152737           ASV\nASV6        ASV6 -0.02123424  0.24919713           ASV\nASV2        ASV2  0.23670463 -0.49287683           ASV\nASV10      ASV10 -0.02578511  0.09240257           ASV\nASV1        ASV1  1.18949655  0.17867402           ASV\n\n\n\nCodeedaphic_md_signif_all$variable_type <- \"metadata\"\nedaphic_asv_signif$variable_type <- \"ASV\"\nedaphic_bioplot_data <- rbind(edaphic_md_signif_all, edaphic_asv_signif)\n\n\nThe last thing to do is categorize parameters scores and ASV scores into different variable types for plotting.\n\nCodeedaphic_bioplot_data_md <- subset(edaphic_bioplot_data, \n                                  edaphic_bioplot_data$variable_type == \"metadata\")\nedaphic_bioplot_data_asv <- subset(edaphic_bioplot_data, \n                                   edaphic_bioplot_data$variable_type == \"ASV\")\n\n\n\nShow the code for the plot\n\n\n\n\n\n\nSoil Functional Response\n\nCodetmp_md <- ssu18_select_mc_norm_split_no_ac$soil_funct\ntmp_md$TREAT_T <- as.character(tmp_md$TREAT_T)\ntmp_comm <- data.frame(t(ssu18_select_mc_norm_split_no_ac$data_loaded))\nsoil_funct_rank <- rankindex(tmp_md[, 8:ncol(tmp_md)], tmp_comm, \n          indices = c(\"euc\", \"man\", \"gow\",\"bra\", \"kul\"), \n          stepacross = FALSE, method = \"spearman\")\n\n\n\n\n      euc       man       gow       bra       kul \n0.5840452 0.7341800 0.2168775 0.7341800 0.7341800 \n\n\nLet’s run capscale using Bray-Curtis. Note, we have 11 metadata parameters in this group but, for some reason, capscale only works with 13 parameters. This may have to do with degrees of freedom?\n\nStarting properties: micC, micN, micP, micCN, micCP, micNP, AG_ase, BG_ase, BP_ase, CE_ase, P_ase, N_ase, S_ase, XY_ase, LP_ase, PX_ase, CO2, enzCN, enzCP, enzNP\nAutocorrelated removed: micN, micNP, enzCN, enzCP, BP_ase, CE_ase, LP_ase, N_ase, P_ase\nRemove for capscale: NONE\n\n\n\n\n\nsoil_funct_cap <- capscale(tmp_comm ~ micC + micP + micCN + micCP + AG_ase + BG_ase + \n                                      S_ase + XY_ase + PX_ase + CO2 + enzNP, \n                           tmp_md, dist = \"bray\")\n\n\n\nCall: capscale(formula = tmp_comm ~ micC + micP + micCN + micCP +\nAG_ase + BG_ase + S_ase + XY_ase + PX_ase + CO2 + enzNP, data = tmp_md,\ndistance = \"bray\")\n\n              Inertia Proportion Rank\nTotal         1.67890    1.00000     \nConstrained   1.52672    0.90936   11\nUnconstrained 0.15218    0.09064    3\nInertia is squared Bray distance \nSpecies scores projected from 'tmp_comm' \n\nEigenvalues for constrained axes:\n  CAP1   CAP2   CAP3   CAP4   CAP5   CAP6   CAP7   CAP8   CAP9  CAP10  CAP11 \n0.6202 0.2950 0.1827 0.1164 0.0950 0.0555 0.0491 0.0468 0.0290 0.0246 0.0126 \n\nEigenvalues for unconstrained axes:\n   MDS1    MDS2    MDS3 \n0.06264 0.05568 0.03386 \n\n\nNow we can look at the variance against each principal component.\n\n\n\n\n\nAnd then make some quick and dirty plots. This will also come in handy later when we need to parse out data a better plot visualization. The ggplot function autoplot stores these data in a more accessible way than the raw results from capscale\n\nbase::plot(soil_funct_cap) \n\n\n\ntmp_auto_plt <- autoplot(soil_funct_cap, arrows = TRUE)\ntmp_auto_plt\n\n\n\n\n\n\n\nNext, we need to grab capscale scores for the samples and create a data frame of the first two dimensions. We will also need to add some of the sample details to the data frame. For this we use the vegan function scores which gets species or site scores from the ordination.\n\nCodetmp_samp_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"sites\")\ntmp_samp_scores[,1] <- NULL\ntmp_samp_scores <- tmp_samp_scores %>% dplyr::rename(SampleID = Label)\n\ntmp_md_sub <- tmp_md[, 1:4]\ntmp_md_sub <- tmp_md_sub %>% tibble::rownames_to_column(\"SampleID\")\nsoil_funct_plot_data <- dplyr::left_join(tmp_md_sub, tmp_samp_scores, \n                                         by = \"SampleID\")\n\n\nNow we have a new data frame that contains sample details and capscale values.\n\n\n          SampleID PLOT TREAT TREAT_T PAIR        CAP1        CAP2\n1  P10_D00_010_C0E  P10     C       0    E -0.53163678  0.19467717\n2  P02_D00_010_C0A  P02     C       0    A -0.49565928 -0.17246578\n3  P04_D00_010_C0B  P04     C       0    B -0.58149488  0.04815608\n4  P06_D00_010_C0C  P06     C       0    C -0.53700662  0.10581563\n5  P08_D00_010_C0D  P08     C       0    D -0.69408551  0.15871504\n6  P01_D00_010_W3A  P01     W       3    A -0.13789308 -0.32241637\n7  P03_D00_010_W3B  P03     W       3    B -0.23991258 -0.02936566\n8  P05_D00_010_W3C  P05     W       3    C -0.37861217 -0.07503400\n9  P07_D00_010_W3D  P07     W       3    D -0.01748558 -0.49326034\n10 P09_D00_010_W3E  P09     W       3    E -0.10498973 -0.54355896\n11 P01_D00_010_W8A  P01     W       8    A  0.78557974  0.01332924\n12 P03_D00_010_W8B  P03     W       8    B  0.89342218 -0.94105534\n13 P05_D00_010_W8C  P05     W       8    C  1.07904998 -0.30014637\n14 P07_D00_010_W8D  P07     W       8    D  0.41449841  0.74331915\n15 P09_D00_010_W8E  P09     W       8    E  0.54622591  1.61329051\n\n\nWe can then do the same with the metadata vectors. Here though we only need the scores and parameter name.\n\nCodesoil_funct_md_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"biplot\")\nsoil_funct_md_scores[,1] <- NULL\nsoil_funct_md_scores <- soil_funct_md_scores %>% \n  dplyr::mutate(parameters = Label, .before = CAP1) %>% \n  tibble::column_to_rownames(\"Label\")\n\n\n\n\n       parameters        CAP1       CAP2\nmicC         micC  0.05033428  0.3114850\nmicP         micP -0.28522874  0.2555622\nmicCN       micCN  0.18231162 -0.4195484\nmicCP       micCP  0.23124711 -0.2302962\nAG_ase     AG_ase -0.26468219  0.6079572\nBG_ase     BG_ase  0.42574674  0.6072615\nS_ase       S_ase  0.29987944  0.7961890\nXY_ase     XY_ase  0.64821470  0.3027833\nPX_ase     PX_ase  0.80304377  0.3296233\nCO2           CO2  0.68903357 -0.1628577\nenzNP       enzNP -0.77637066  0.1324578\n\n\nLet’s run some quick correlations of metadata with ordination axes to see which parameters are significant. For this we use the vegan function envfit.\n\nCodetmp_samp_scores_sub <- soil_funct_plot_data[, 6:7]\ntmp_samp_scores_sub <- as.matrix(tmp_samp_scores_sub)\ntmp_param_list <- soil_funct_md_scores$parameters\n\ntmp_md_sub <- subset(tmp_md,  select =  tmp_param_list)\n\nenvfit_soil_funct_md <- envfit(tmp_samp_scores_sub, tmp_md_sub,\n                 perm = 1000, choices = c(1, 2))\n\n\n\n\n\n***VECTORS\n\n           CAP1     CAP2     r2   Pr(>r)    \nmicC    0.16467  0.98635 0.1012 0.511489    \nmicP   -0.74314  0.66913 0.1473 0.385614    \nmicCN   0.39228 -0.91985 0.2111 0.244755    \nmicCP   0.70633 -0.70788 0.1070 0.504496    \nAG_ase -0.39292  0.91957 0.4435 0.025974 *  \nBG_ase  0.57346  0.81923 0.5604 0.006993 ** \nS_ase   0.35512  0.93482 0.7371 0.001998 ** \nXY_ase  0.90321  0.42921 0.5193 0.008991 ** \nPX_ase  0.92246  0.38609 0.7640 0.000999 ***\nCO2     0.97421 -0.22565 0.5041 0.012987 *  \nenzNP  -0.98661  0.16307 0.6242 0.003996 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodesoil_funct_md_signif_hits <- base::subset(envfit_soil_funct_md$vectors$pvals, \n                                  c(envfit_soil_funct_md$vectors$pvals \n                                  < 0.05 & envfit_soil_funct_md$vectors$r > 0.4))\nsoil_funct_md_signif_hits <- data.frame(soil_funct_md_signif_hits)\nsoil_funct_md_signif_hits <- rownames(soil_funct_md_signif_hits)\nsoil_funct_md_signif <- soil_funct_md_scores[soil_funct_md_scores$parameters %in% \n                                               soil_funct_md_signif_hits,]\nsoil_funct_md_signif$parameters\n\n\nenvfit found that AG_ase, BG_ase, S_ase, XY_ase, PX_ase, CO2, enzNP were significantly correlated.\nNow let’s see if the same parameters are significant for the envfit and bioenv analyses.\n\n\n[1] \"Significant parameters from bioenv analysis.\"\n[1] \"AG_ase\" \"enzNP\"  \"S_ase\"  \"PX_ase\" \"XY_ase\"\n_____________________________________\n[1] \"Significant parameters from envfit analysis.\"\n[1] \"AG_ase\" \"BG_ase\" \"S_ase\"  \"XY_ase\" \"PX_ase\" \"CO2\"    \"enzNP\" \n_____________________________________\n[1] \"Found in bioenv but not envfit.\"\ncharacter(0)\n_____________________________________\n[1] \"Found in envfit but not bioenv.\"\n[1] \"BG_ase\" \"CO2\"   \n_____________________________________\n[1] \"Found in envfit and bioenv.\"\n[1] \"AG_ase\" \"BG_ase\" \"S_ase\"  \"XY_ase\" \"PX_ase\" \"CO2\"    \"enzNP\" \n\n\n\nCode#new_soil_funct_md_signif_hits <- append(soil_funct_md_signif_hits, soil_funct_sig_diff)\nnew_soil_funct_md_signif_hits <- soil_funct_sig_diff\nsoil_funct_md_signif_all <- soil_funct_md_scores[soil_funct_md_scores$parameters %in% \n                                                   new_soil_funct_md_signif_hits,]\n\n\nCheck. Next, we run envfit for the ASVs.\n\nCodeenvfit_soil_funct_asv <- envfit(tmp_samp_scores_sub, \n                                tmp_comm[, order(colSums(-tmp_comm))][, 1:10], \n                                perm = 1000, choices = c(1, 2))\nsoil_funct_asv_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"species\")\nsoil_funct_asv_scores <- soil_funct_asv_scores %>% \n  dplyr::mutate(parameters = Label, .before = CAP1) %>% \n  tibble::column_to_rownames(\"Label\")\nsoil_funct_asv_scores[,1] <- NULL\n\n\n\n\n\n***VECTORS\n\n          CAP1     CAP2     r2   Pr(>r)    \nASV1   0.99572  0.09244 0.6448 0.008991 ** \nASV2   0.53392 -0.84554 0.9003 0.000999 ***\nASV3   0.80024  0.59968 0.9414 0.000999 ***\nASV4  -0.10124  0.99486 0.2814 0.142857    \nASV5  -0.56790 -0.82310 0.1663 0.313686    \nASV6  -0.08053  0.99675 0.7514 0.000999 ***\nASV8  -0.83447  0.55105 0.4303 0.017982 *  \nASV7  -0.57807 -0.81599 0.0697 0.619381    \nASV9   0.49314 -0.86995 0.2612 0.147852    \nASV10 -0.32480  0.94578 0.8408 0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodesoil_funct_asv_signif_hits <- base::subset(envfit_soil_funct_asv$vectors$pvals, \n                                  c(envfit_soil_funct_asv$vectors$pvals \n                                  < 0.05 & envfit_soil_funct_asv$vectors$r > 0.5))\nsoil_funct_asv_signif_hits <- data.frame(soil_funct_asv_signif_hits)\nsoil_funct_asv_signif_hits <- rownames(soil_funct_asv_signif_hits)\nsoil_funct_asv_signif <- soil_funct_asv_scores[soil_funct_asv_scores$parameters %in% \n                                                 soil_funct_asv_signif_hits,]\n\n\n\n\n      parameters        CAP1        CAP2 variable_type\nASV3        ASV3  0.49966533  0.50499834           ASV\nASV6        ASV6 -0.01098408  0.25326180           ASV\nASV2        ASV2  0.22435017 -0.49303800           ASV\nASV10      ASV10 -0.02420323  0.09642322           ASV\nASV1        ASV1  1.17846608  0.16123440           ASV\n\n\n\nCodesoil_funct_md_signif_all$variable_type <- \"metadata\"\nsoil_funct_asv_signif$variable_type <- \"ASV\"\nsoil_funct_bioplot_data <- rbind(soil_funct_md_signif_all, soil_funct_asv_signif)\n\n\nThe last thing to do is categorize parameters scores and ASV scores into different variable types for plotting.\n\nCodesoil_funct_bioplot_data_md <- subset(soil_funct_bioplot_data, \n                                  soil_funct_bioplot_data$variable_type == \"metadata\")\nsoil_funct_bioplot_data_asv <- subset(soil_funct_bioplot_data, \n                                   soil_funct_bioplot_data$variable_type == \"ASV\")\n\n\n\nShow the code for the plot\n\nsoil_funct_cap_vals <- data.frame(soil_funct_cap$CCA$eig[1:2])\nsoil_funct_cap1 <- signif((soil_funct_cap_vals[1,] * 100), digits=3)\nsoil_funct_cap2 <- signif((soil_funct_cap_vals[2,] * 100), digits=3)\n\ncpa1_lab <- paste(\"CAP1\", \" (\", soil_funct_cap1, \"%)\", sep = \"\")\ncpa2_lab <- paste(\"CAP2\", \" (\", soil_funct_cap2, \"%)\", sep = \"\")\n\nswel_col <- c(\"#2271B2\", \"#71B222\", \"#B22271\")\n\nsoil_funct_plot <- ggplot(soil_funct_plot_data) +\n  geom_point(mapping = aes(x = CAP1, y = CAP2, shape = TREAT,\n                           colour = TREAT_T), size = 4) +\n  scale_colour_manual(values = swel_col) +\n#  geom_text(data = soil_funct_plot_data, aes(x = CAP1, y = CAP2, #UNCOMMENT to add sample labels\n#                              label = SampleID), size = 3) + \n  geom_segment(aes(x = 0, y = 0, xend = CAP1, yend = CAP2),\n               data = soil_funct_bioplot_data_md, linetype = \"solid\",\n               arrow = arrow(length = unit(0.3, \"cm\")), size = 0.4, \n               color = \"#191919\") +\n  geom_text(data = soil_funct_bioplot_data_md, \n            aes(x = CAP1, y = CAP2, label = parameters), size = 3,\n            nudge_x = 0.1, nudge_y = 0.05) +\n  ### This code adds ASV vectors, Uncomment to add\n  #geom_segment(aes(x = 0, y = 0, xend = CAP1, yend = CAP2),\n  #             data = soil_funct_bioplot_data_asv, linetype = \"solid\",\n  #             arrow = arrow(length = unit(0.3, \"cm\")), size = 0.2,\n  #             color = \"#676767\") +\n  #geom_text(data = soil_funct_bioplot_data_asv, \n  #          aes(x = CAP1, y = CAP2, label = parameters), size = 2.5,\n  #          nudge_x = 0.05, nudge_y = 0.05) +\n  theme_classic(base_size = 12) + \n  labs(title = \"Capscale Analysis\",\n       subtitle = \"Soil Functional Response\", \n       x = cpa1_lab, \n       y = cpa2_lab)\nsoil_funct_plot <- soil_funct_plot + coord_fixed() + theme(aspect.ratio=1)\nsoil_funct_plot\npng(\"files/metadata/figures/ssu18_soil_funct_capscale.png\",\n    height = 16, width = 20, units = 'cm', res = 600, bg = \"white\")\nsoil_funct_plot\ninvisible(dev.off())\npdf(\"files/metadata/figures/ssu18_soil_funct_capscale.pdf\",\n    height = 5, width = 6)\nsoil_funct_plot\ndev.off()\n\n\n\n\nTemperature Adaptation\n\nCodetmp_md <- ssu18_select_mc_norm_split_no_ac$temp_adapt\ntmp_md$TREAT_T <- as.character(tmp_md$TREAT_T)\ntmp_comm <- data.frame(t(ssu18_select_mc_norm_split_no_ac$data_loaded))\ntemp_adapt_rank <- rankindex(tmp_md[, 8:ncol(tmp_md)], tmp_comm, \n          indices = c(\"euc\", \"man\", \"gow\",\"bra\", \"kul\"), \n          stepacross = FALSE, method = \"spearman\")\n\n\n\n\n      euc       man       gow       bra       kul \n0.2732117 0.3330085 0.3456355 0.3330085 0.3330085 \n\n\nLet’s run capscale using Bray-Curtis. Note, we have 13 metadata parameters in this group but, for some reason, capscale only works with 13 parameters. This may have to do with degrees of freedom?\n\nStarting properties: AG_Q10, BG_Q10, BP_Q10, CE_Q10, P_Q10, N_Q10, S_Q10, XY_Q10, LP_Q10, PX_Q10, CUEcn, CUEcp, NUE, PUE, Tmin, SI\nAutocorrelated removed: NUE, PUE, SI\nRemove for capscale: NONE\n\n\n\n\n\ntemp_adapt_cap <- capscale(tmp_comm ~  AG_Q10 + BG_Q10 + BP_Q10 + CE_Q10 + \n                                       P_Q10 + N_Q10 + S_Q10 + XY_Q10 + \n                                       LP_Q10 + PX_Q10 + CUEcn + CUEcp + Tmin, \n                             tmp_md, dist = \"bray\")\n\n\n\nCall: capscale(formula = tmp_comm ~ AG_Q10 + BG_Q10 + BP_Q10 + CE_Q10 +\nP_Q10 + N_Q10 + S_Q10 + XY_Q10 + LP_Q10 + PX_Q10 + CUEcn + CUEcp +\nTmin, data = tmp_md, distance = \"bray\")\n\n              Inertia Proportion Rank\nTotal         1.67890    1.00000     \nConstrained   1.57681    0.93919   13\nUnconstrained 0.10209    0.06081    1\nInertia is squared Bray distance \nSpecies scores projected from 'tmp_comm' \n\nEigenvalues for constrained axes:\n  CAP1   CAP2   CAP3   CAP4   CAP5   CAP6   CAP7   CAP8   CAP9  CAP10  CAP11 \n0.6298 0.2776 0.1985 0.1087 0.0849 0.0614 0.0528 0.0450 0.0352 0.0273 0.0244 \n CAP12  CAP13 \n0.0218 0.0095 \n\nEigenvalues for unconstrained axes:\n   MDS1 \n0.10209 \n\n\nNow we can look at the variance against each principal component.\n\n\n\n\n\nAnd then make some quick and dirty plots. This will also come in handy later when we need to parse out data a better plot visualization. The ggplot function autoplot stores these data in a more accessible way than the raw results from capscale\n\nbase::plot(temp_adapt_cap) \n\n\n\ntmp_auto_plt <- autoplot(temp_adapt_cap, arrows = TRUE)\ntmp_auto_plt\n\n\n\n\n\n\n\nNext, we need to grab capscale scores for the samples and create a data frame of the first two dimensions. We will also need to add some of the sample details to the data frame. For this we use the vegan function scores which gets species or site scores from the ordination.\n\nCodetmp_samp_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"sites\")\ntmp_samp_scores[,1] <- NULL\ntmp_samp_scores <- tmp_samp_scores %>% dplyr::rename(SampleID = Label)\n\ntmp_md_sub <- tmp_md[, 1:4]\ntmp_md_sub <- tmp_md_sub %>% tibble::rownames_to_column(\"SampleID\")\ntemp_adapt_plot_data <- dplyr::left_join(tmp_md_sub, tmp_samp_scores, \n                                         by = \"SampleID\")\n\n\nNow we have a new data frame that contains sample details and capscale values.\n\n\n          SampleID PLOT TREAT TREAT_T PAIR        CAP1        CAP2\n1  P10_D00_010_C0E  P10     C       0    E -0.50625375  0.10348626\n2  P02_D00_010_C0A  P02     C       0    A -0.48509579 -0.14580527\n3  P04_D00_010_C0B  P04     C       0    B -0.57845671  0.03776429\n4  P06_D00_010_C0C  P06     C       0    C -0.53336078  0.08936059\n5  P08_D00_010_C0D  P08     C       0    D -0.68345909  0.10974979\n6  P01_D00_010_W3A  P01     W       3    A -0.13776351 -0.22674265\n7  P03_D00_010_W3B  P03     W       3    B -0.25679729  0.01972197\n8  P05_D00_010_W3C  P05     W       3    C -0.38722889 -0.11236725\n9  P07_D00_010_W3D  P07     W       3    D -0.01518359 -0.46741081\n10 P09_D00_010_W3E  P09     W       3    E -0.11194937 -0.54460136\n11 P01_D00_010_W8A  P01     W       8    A  0.78329851  0.09997647\n12 P03_D00_010_W8B  P03     W       8    B  0.89642936 -1.09522700\n13 P05_D00_010_W8C  P05     W       8    C  1.07566246 -0.26764258\n14 P07_D00_010_W8D  P07     W       8    D  0.41185806  0.81673870\n15 P09_D00_010_W8E  P09     W       8    E  0.52830039  1.58299886\n\n\nWe can then do the same with the metadata vectors. Here though we only need the scores and parameter name.\n\nCodetemp_adapt_md_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"biplot\")\ntemp_adapt_md_scores[,1] <- NULL\ntemp_adapt_md_scores <- temp_adapt_md_scores %>% \n  dplyr::mutate(parameters = Label, .before = CAP1) %>% \n  tibble::column_to_rownames(\"Label\")\n\n\n\n\n       parameters        CAP1        CAP2\nAG_Q10     AG_Q10 -0.07373951  0.23793282\nBG_Q10     BG_Q10  0.72604945  0.10579764\nBP_Q10     BP_Q10  0.07015779 -0.12413231\nCE_Q10     CE_Q10 -0.37489864  0.55022094\nP_Q10       P_Q10  0.87190011 -0.25252818\nN_Q10       N_Q10  0.09422795 -0.32515404\nS_Q10       S_Q10  0.91309230  0.55878516\nXY_Q10     XY_Q10  0.92245895 -0.02395029\nLP_Q10     LP_Q10 -0.90356227 -0.36479560\nPX_Q10     PX_Q10 -0.87475774  0.18121787\nCUEcn       CUEcn -0.23872665 -0.39707742\nCUEcp       CUEcp  0.57949983 -0.62675335\nTmin         Tmin  0.99158847  0.19050834\n\n\nLet’s run some quick correlations of metadata with ordination axes to see which parameters are significant. For this we use the vegan function envfit.\n\nCodetmp_samp_scores_sub <- temp_adapt_plot_data[, 6:7]\ntmp_samp_scores_sub <- as.matrix(tmp_samp_scores_sub)\ntmp_param_list <- temp_adapt_md_scores$parameters\n\ntmp_md_sub <- subset(tmp_md,  select =  tmp_param_list)\n\nenvfit_temp_adapt_md <- envfit(tmp_samp_scores_sub, tmp_md_sub,\n                 perm = 1000, choices = c(1, 2))\n\n\n\n\n\n***VECTORS\n\n           CAP1     CAP2     r2  Pr(>r)  \nAG_Q10 -0.30810  0.95136 0.0260 0.83117  \nBG_Q10  0.99031  0.13889 0.2354 0.17782  \nBP_Q10  0.50944 -0.86050 0.0086 0.95504  \nCE_Q10 -0.58116  0.81379 0.1875 0.25075  \nP_Q10   0.96427 -0.26494 0.3590 0.05395 .\nN_Q10   0.28973 -0.95711 0.0479 0.76723  \nS_Q10   0.86347  0.50440 0.4956 0.01499 *\nXY_Q10  0.99973 -0.02334 0.3725 0.04895 *\nLP_Q10 -0.93292 -0.36008 0.4131 0.04096 *\nPX_Q10 -0.98131  0.19244 0.3483 0.07992 .\nCUEcn  -0.53407 -0.84544 0.0908 0.55245  \nCUEcp   0.69625 -0.71780 0.3103 0.12288  \nTmin    0.98341  0.18140 0.4456 0.02997 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodetemp_adapt_md_signif_hits <- base::subset(envfit_temp_adapt_md$vectors$pvals, \n                                  c(envfit_temp_adapt_md$vectors$pvals \n                                  < 0.05 & envfit_temp_adapt_md$vectors$r > 0.4))\ntemp_adapt_md_signif_hits <- data.frame(temp_adapt_md_signif_hits)\ntemp_adapt_md_signif_hits <- rownames(temp_adapt_md_signif_hits)\ntemp_adapt_md_signif <- temp_adapt_md_scores[temp_adapt_md_scores$parameters %in% \n                                               temp_adapt_md_signif_hits,]\n\n\nenvfit found that S_Q10, LP_Q10, Tmin were significantly correlated.\nNow let’s see if the same parameters are significant for the envfit and bioenv analyses.\n\n\n[1] \"Significant parameters from bioenv analysis.\"\n[1] \"CUEcp\"  \"LP_Q10\" \"P_Q10\"  \"S_Q10\"  \"Tmin\"  \n_____________________________________\n[1] \"Significant parameters from envfit analysis.\"\n[1] \"S_Q10\"  \"LP_Q10\" \"Tmin\"  \n_____________________________________\n[1] \"Found in bioenv but not envfit.\"\n[1] \"CUEcp\" \"P_Q10\"\n_____________________________________\n[1] \"Found in envfit but not bioenv.\"\ncharacter(0)\n_____________________________________\n[1] \"Found in envfit and  bioenv.\"\n[1] \"S_Q10\"  \"LP_Q10\" \"Tmin\"   \"CUEcp\"  \"P_Q10\" \n\n\n\nCode#new_temp_adapt_md_signif_hits <- base::append(temp_adapt_md_signif_hits, temp_adapt_sig_diff)\nnew_temp_adapt_md_signif_hits <- temp_adapt_sig_diff\ntemp_adapt_md_signif_all <- temp_adapt_md_scores[temp_adapt_md_scores$parameters %in% \n                                                   new_temp_adapt_md_signif_hits,]\n\n\nCheck. Next, we run envfit for the ASVs.\n\nCodeenvfit_temp_adapt_asv <- envfit(tmp_samp_scores_sub, \n                                tmp_comm[, order(colSums(-tmp_comm))][, 1:10],\n                                perm = 1000, choices = c(1, 2))\n\ntemp_adapt_asv_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"species\")\n\ntemp_adapt_asv_scores <- temp_adapt_asv_scores %>% \n                         dplyr::mutate(parameters = Label, .before = CAP1) %>%\n                         tibble::column_to_rownames(\"Label\")\ntemp_adapt_asv_scores[,1] <- NULL\n\n\n\n\n\n***VECTORS\n\n          CAP1     CAP2     r2   Pr(>r)    \nASV1   0.98984  0.14217 0.6541 0.009990 ** \nASV2   0.55070 -0.83470 0.8953 0.000999 ***\nASV3   0.79919  0.60108 0.9513 0.000999 ***\nASV4  -0.12171  0.99257 0.3110 0.118881    \nASV5  -0.67644 -0.73649 0.1181 0.487512    \nASV6  -0.10078  0.99491 0.7028 0.003996 ** \nASV8  -0.85434  0.51972 0.4279 0.043956 *  \nASV7  -0.57853 -0.81566 0.0602 0.676324    \nASV9   0.53361 -0.84573 0.2425 0.185814    \nASV10 -0.34761  0.93764 0.8031 0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodetemp_adapt_asv_signif_hits <- base::subset(envfit_temp_adapt_asv$vectors$pvals, \n                                  c(envfit_temp_adapt_asv$vectors$pvals \n                                  < 0.05 & envfit_temp_adapt_asv$vectors$r > 0.5))\ntemp_adapt_asv_signif_hits <- data.frame(temp_adapt_asv_signif_hits)\ntemp_adapt_asv_signif_hits <- rownames(temp_adapt_asv_signif_hits)\ntemp_adapt_asv_signif <- temp_adapt_asv_scores[temp_adapt_asv_scores$parameters %in% \n                                                 temp_adapt_asv_signif_hits,]\n\n\n\n\n      parameters        CAP1        CAP2 variable_type\nASV3        ASV3  0.49583686  0.48216614           ASV\nASV6        ASV6 -0.01759546  0.20461984           ASV\nASV2        ASV2  0.23578207 -0.44144325           ASV\nASV10      ASV10 -0.02475900  0.08142465           ASV\nASV1        ASV1  1.18724098  0.33621587           ASV\n\n\n\nCodetemp_adapt_md_signif_all$variable_type <- \"metadata\"\ntemp_adapt_asv_signif$variable_type <- \"ASV\"\ntemp_adapt_bioplot_data <- rbind(temp_adapt_md_signif_all, temp_adapt_asv_signif)\n\n\nThe last thing to do is categorize parameters scores and ASV scores into different variable types for plotting.\n\nCodetemp_adapt_bioplot_data_md <- subset(temp_adapt_bioplot_data, \n                                  temp_adapt_bioplot_data$variable_type == \"metadata\")\ntemp_adapt_bioplot_data_asv <- subset(temp_adapt_bioplot_data, \n                                   temp_adapt_bioplot_data$variable_type == \"ASV\")\n\n\n\nShow the code for the plot\n\ntemp_adapt_cap_vals <- data.frame(temp_adapt_cap$CCA$eig[1:2])\ntemp_adapt_cap1 <- signif((temp_adapt_cap_vals[1,] * 100), digits=3)\ntemp_adapt_cap2 <- signif((temp_adapt_cap_vals[2,] * 100), digits=3)\n\ncpa1_lab <- paste(\"CAP1\", \" (\", temp_adapt_cap1, \"%)\", sep = \"\")\ncpa2_lab <- paste(\"CAP2\", \" (\", temp_adapt_cap2, \"%)\", sep = \"\")\n\nswel_col <- c(\"#2271B2\", \"#71B222\", \"#B22271\")\n\ntemp_adapt_plot <- ggplot(temp_adapt_plot_data) +\n  geom_point(mapping = aes(x = CAP1, y = CAP2, shape = TREAT,\n                           colour = TREAT_T), size = 4) +\n  scale_colour_manual(values = swel_col) +\n#  geom_text(data = temp_adapt_plot_data, aes(x = CAP1, y = CAP2, #UNCOMMENT to add sample labels\n#                              label = SampleID), size = 3) + \n  geom_segment(aes(x = 0, y = 0, xend = CAP1, yend = CAP2),\n               data = temp_adapt_bioplot_data_md, linetype = \"solid\",\n               arrow = arrow(length = unit(0.3, \"cm\")), size = 0.4,\n               color = \"#191919\", inherit.aes = FALSE) +\n  geom_text(data = temp_adapt_bioplot_data_md, \n            aes(x = CAP1, y = CAP2, label = parameters), size = 3,\n            nudge_x = 0.1, nudge_y = 0.05) +\n  ### This code adds ASV vectors, Uncomment to add\n#  geom_segment(aes(x = 0, y = 0, xend = CAP1, yend = CAP2),\n#               data = temp_adapt_bioplot_data_asv, linetype = \"solid\",\n#               arrow = arrow(length = unit(0.3, \"cm\")), size = 0.2,\n#               color = \"#676767\") +\n#  geom_text(data = temp_adapt_bioplot_data_asv, \n#            aes(x = CAP1, y = CAP2, label = parameters), size = 2.5,\n#            nudge_x = 0.05, nudge_y = 0.05) +\n  theme_classic(base_size = 12) + \n  labs(title = \"Capscale Analysis\",\n       subtitle = \"Temperature Adaptation\",\n       x = cpa1_lab, \n       y = cpa2_lab)\ntemp_adapt_plot <- temp_adapt_plot + coord_fixed() + theme(aspect.ratio=1)\ntemp_adapt_plot\npng(\"files/metadata/figures/ssu18_temp_adapt_capscale.png\",\n    height = 16, width = 20, units = 'cm', res = 600, bg = \"white\")\ntemp_adapt_plot\ninvisible(dev.off())\npdf(\"files/metadata/figures/ssu18_temp_adapt_capscale.pdf\",\n    height = 5, width = 6)\ntemp_adapt_plot\ndev.off()"
  },
  {
    "objectID": "metadata.html#capscale-plots",
    "href": "metadata.html#capscale-plots",
    "title": "8. Metadata Analysis",
    "section": "Capscale Plots",
    "text": "Capscale Plots\n\n\n\nEdaphic properties\nSoil Functional Response\nTemperature Adaptation\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 7 | Distance-based Redundancy Analysis (db-RDA) of PIME filtered data based on Bray-Curtis dissimilarity showing the relationships between community composition change versus edaphic properties.\n\n\n\n\n\n\n\n(16S rRNA) Figure 8 | Distance-based Redundancy Analysis (db-RDA) of PIME filtered data based on Bray-Curtis dissimilarity showing the relationships between community composition change versus microbial functional response.\n\n\n\n\n\n\n\n(16S rRNA) Figure 9 | Distance-based Redundancy Analysis (db-RDA) of PIME filtered data based on Bray-Curtis dissimilarity showing the relationships between community composition change versus temperature adaptation."
  },
  {
    "objectID": "metadata.html#data-formating-1",
    "href": "metadata.html#data-formating-1",
    "title": "8. Metadata Analysis",
    "section": "Data Formating",
    "text": "Data Formating\n\nTransform ps Objects\n\nThe first step is to transform the phyloseq objects.\n\nCodesamp_ps <- c(\"its18_ps_work\", \"its18_ps_pime\", \"its18_ps_perfect\", \n             \"its18_ps_work_otu\", \"its18_ps_pime_otu\", \"its18_ps_perfect_otu\")\nfor (i in samp_ps) {\n     tmp_get <- get(i)\n     tmp_ps <- transform_sample_counts(tmp_get, function(otu) 1e5 * otu/sum(otu))\n     tmp_ps@phy_tree <- NULL\n     tmp_ps <- prune_samples(sample_sums(tmp_ps) > 0, tmp_ps)\n     tmp_samp <- data.frame(sample_data(tmp_get))\n     tmp_samp <- tmp_samp %>% dplyr::rename(\"TREAT_T\" = \"TEMP\")\n     tmp_ps <- merge_phyloseq(tmp_ps, sample_data)\n     sample_data(tmp_ps) <- tmp_samp\n     tmp_name <- purrr::map_chr(i, ~ paste0(., \"_prop\"))\n     print(tmp_name)\n     assign(tmp_name, tmp_ps)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nFormat for mctoolsr\n\n\nThen a bit of formatting to make the data compatible with mctoolsr.\n\nCodefor (i in samp_ps) {\n     tmp_get <- get(purrr::map_chr(i, ~ paste0(., \"_prop\")))\n     tmp_tax <- data.frame(tax_table(tmp_get))\n     tmp_tax$ASV_SEQ <- NULL\n     \n     tmp_col_names <- colnames(tmp_tax)\n     tmp_tax_merge <- tmp_tax %>% tidyr::unite(taxonomy, \n                                               all_of(tmp_col_names), \n                                               sep = \";\")\n     tmp_tax_merge <- tmp_tax_merge %>% tibble::rownames_to_column(\"#OTU_ID\")\n     tmp_otu <- data.frame(t(otu_table(tmp_get)))\n     tmp_otu <- tmp_otu %>% tibble::rownames_to_column(\"#OTU_ID\")\n     tmp_otu_tax <- dplyr::left_join(tmp_otu, tmp_tax_merge, by = \"#OTU_ID\")\n\n     tmp_samp <- data.frame(sample_data(tmp_get))\n     tmp_samp[,c(1,3)] <- NULL\n     tmp_samp <- tmp_samp %>% tibble::rownames_to_column(\"#SampleID\")\n\n     tmp_metad <- metad %>% dplyr::rename(\"#SampleID\" = \"id\")\n     tmp_metad[,2:5] <- NULL\n     tmp_md <- dplyr::left_join(tmp_samp, tmp_metad, by = \"#SampleID\")\n     \n     tmp_otu_name <- purrr::map_chr(i, ~ paste0(., \"_otu_tax\"))\n     print(tmp_otu_name)\n     assign(tmp_otu_name, tmp_otu_tax)\n\n     tmp_md_name <- purrr::map_chr(i, ~ paste0(., \"_md\"))\n     print(tmp_md_name)\n     assign(tmp_md_name, tmp_md)\n\n     tmp_path <- file.path(\"files/metadata/tables/\")\n     write_delim(tmp_otu_tax, paste(tmp_path, tmp_otu_name, \".txt\", sep = \"\"), delim = \"\\t\")\n     write_delim(tmp_md, paste(tmp_path, tmp_md_name, \".txt\", sep = \"\"), delim = \"\\t\")\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nCodefor (i in samp_ps) {\n     tmp_path <- file.path(\"files/metadata/tables/\")\n     tmp_otu_name <- purrr::map_chr(i, ~ paste0(., \"_otu_tax\"))\n     tmp_md_name <- purrr::map_chr(i, ~ paste0(., \"_md\"))\n     tmp_tax_table_fp <- paste(tmp_path, tmp_otu_name, \".txt\", sep = \"\")\n     tmp_map_fp <- paste(tmp_path, tmp_md_name, \".txt\", sep = \"\")\n     tmp_input <- load_taxa_table(tmp_tax_table_fp, tmp_map_fp)\n     tmp_input_name <- purrr::map_chr(i, ~ paste0(., \"_mc\"))\n     print(tmp_input_name)\n     assign(tmp_input_name, tmp_input)\n     rm(list = ls(pattern = \"tmp_\"))\n}\nrm(list = ls(pattern = \"_md\"))\nrm(list = ls(pattern = \"_otu_tax\"))"
  },
  {
    "objectID": "metadata.html#choose-data-set-1",
    "href": "metadata.html#choose-data-set-1",
    "title": "8. Metadata Analysis",
    "section": "Choose Data Set",
    "text": "Choose Data Set\nAt this point in the code we need to choose a data set to use, formatted with mctoolsr. Remember, there are four choices:\n\nComplete ASV data set.\nArbitrary filtered ASV data set.\nPERfect filtered ASV data set.\nPIME filtered ASV data set.\n\nThis way, if we want to test other data sets we only need to change the name here.\n\nobjects(pattern = \"_mc\")\nits18_select_mc <- its18_ps_pime_mc"
  },
  {
    "objectID": "metadata.html#normality-tests-1",
    "href": "metadata.html#normality-tests-1",
    "title": "8. Metadata Analysis",
    "section": "Normality Tests",
    "text": "Normality Tests\nBefore proceeding, we need to test each parameter in the metadata to see which ones are and are not normally distributed. For that, we use the Shapiro-Wilk Normality Test. Here we only need one of the metadata files.\n\nCodetemp_md <- its18_select_mc$map_loaded\ntemp_md[,1:9] <- NULL\nshap_results <- NULL\n\nfor (i in colnames(temp_md)) {\n  tmp_shap <- shapiro.test(temp_md[[i]])\n  tmp_p <- round(tmp_shap$p.value, digits = 5)\n  tmp_res <- eval(isTRUE(tmp_shap$p.value < 0.05))\n  shap_results <- rbind(shap_results, data.frame(i, tmp_p, tmp_res))\n  rm(list = ls(pattern = \"tmp_\"))\n}\ncolnames(shap_results) <- c(\"parameter\", \"p-value\", \"tranform\")\nshap_results\ndplyr::filter(shap_results, tranform == \"TRUE\")\nmd_to_tranform <- shap_results$parameter[shap_results$tranform == TRUE]\nrm(list = ls(pattern = \"temp_md\"))\n\n\nShow the results of each normality test for metadata parameters\n\n(ITS) Table 2 | Results of the Shapiro-Wilk Normality Tests. P-values in red are significance (p-value < 0.05) meaning the parameter needs to be normalized.\n\n\n\n\n\n\n\nLooks like we need to transform 21 metadata parameters."
  },
  {
    "objectID": "metadata.html#normalize-parameters-1",
    "href": "metadata.html#normalize-parameters-1",
    "title": "8. Metadata Analysis",
    "section": "Normalize Parameters",
    "text": "Normalize Parameters\nHere we use the R package bestNormalize to find and execute the best normalizing transformation. The function will test the following normalizing transformations:\n\narcsinh_x performs an arcsinh transformation.\nboxcox Perform a Box-Cox transformation and center/scale a vector to attempt normalization. boxcox estimates the optimal value of lambda for the Box-Cox transformation. The function will return an error if a user attempt to transform nonpositive data.\nyeojohnson Perform a Yeo-Johnson Transformation and center/scale a vector to attempt normalization. yeojohnson estimates the optimal value of lambda for the Yeo-Johnson transformation. The Yeo-Johnson is similar to the Box-Cox method, however it allows for the transformation of nonpositive data as well.\norderNorm The Ordered Quantile (ORQ) normalization transformation, orderNorm(), is a rank-based procedure by which the values of a vector are mapped to their percentile, which is then mapped to the same percentile of the normal distribution. Without the presence of ties, this essentially guarantees that the transformation leads to a uniform distribution.\nlog_x performs a simple log transformation. The parameter a is essentially estimated by the training set by default (estimated as the minimum possible to some extent epsilon), while the base must be specified beforehand. The default base of the log is 10.\nsqrt_x performs a simple square-root transformation. The parameter a is essentially estimated by the training set by default (estimated as the minimum possible), while the base must be specified beforehand.\nexp_x performs a simple exponential transformation.\n\nSee this GitHub issue (#5) for a description on getting reproducible results. Apparently, you can get different results because the bestNormalize() function uses repeated cross-validation (and doesn’t automatically set the seed), so the results will be slightly different each time the function is executed.\n\nCodeset.seed(119)\n\nfor (i in md_to_tranform) {\n  tmp_md <- its18_select_mc$map_loaded\n  tmp_best_norm <- bestNormalize(tmp_md[[i]], r = 1, k = 5, loo = TRUE)\n  tmp_name <- purrr::map_chr(i, ~ paste0(., \"_best_norm_test\"))\n  assign(tmp_name, tmp_best_norm)\n  print(tmp_name)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nShow the chosen transformations\n\n\n\n## bestNormalize Chosen transformation of AST ## \norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n   0%   25%   50%   75%  100% \n25.80 26.26 28.80 29.90 41.77 \n_____________________________________\n\n## bestNormalize Chosen transformation of H2O ## \norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n   0%   25%   50%   75%  100% \n0.204 0.261 0.377 0.385 0.401 \n_____________________________________\n\n## bestNormalize Chosen transformation of Al ## \nStandardized asinh(x) Transformation with 13 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 0.005384372 \n - sd (before standardization) = 0.007762057 \n_____________________________________\n\n## bestNormalize Chosen transformation of Fe ## \nStandardized asinh(x) Transformation with 13 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 0.01076821 \n - sd (before standardization) = 0.0103749 \n_____________________________________\n\n## bestNormalize Chosen transformation of TEB ## \norderNorm Transformation with 13 nonmissing obs and ties\n - 10 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n41.6 43.3 45.1 48.7 61.7 \n_____________________________________\n\n## bestNormalize Chosen transformation of ECEC ## \norderNorm Transformation with 13 nonmissing obs and ties\n - 10 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n41.7 43.3 45.3 48.8 61.8 \n_____________________________________\n\n## bestNormalize Chosen transformation of minNO3 ## \norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 11.31  19.67  22.71  37.33 106.42 \n_____________________________________\n\n## bestNormalize Chosen transformation of minTIN ## \norderNorm Transformation with 13 nonmissing obs and ties\n - 12 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 12.96  21.50  23.99  42.08 110.75 \n_____________________________________\n\n## bestNormalize Chosen transformation of DOC ## \norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 33.20  44.30  56.68  63.79 118.74 \n_____________________________________\n\n## bestNormalize Chosen transformation of BG_ase ## \norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n  0%  25%  50%  75% 100% \n3.04 3.92 4.22 4.62 9.09 \n_____________________________________\n\n## bestNormalize Chosen transformation of BP_ase ## \norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n  0%  25%  50%  75% 100% \n1.77 2.33 3.02 3.50 7.28 \n_____________________________________\n\n## bestNormalize Chosen transformation of P_ase ## \nStandardized Box Cox Transformation with 13 nonmissing obs.:\n Estimated statistics:\n - lambda = -0.9999576 \n - mean (before standardization) = 0.9480149 \n - sd (before standardization) = 0.01593697 \n_____________________________________\n\n## bestNormalize Chosen transformation of N_ase ## \norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n  0%  25%  50%  75% 100% \n2.57 2.95 3.58 4.52 8.34 \n_____________________________________\n\n## bestNormalize Chosen transformation of XY_ase ## \norderNorm Transformation with 13 nonmissing obs and ties\n - 11 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n0.66 0.76 1.11 1.36 2.52 \n_____________________________________\n\n## bestNormalize Chosen transformation of BG_Q10 ## \norderNorm Transformation with 13 nonmissing obs and ties\n - 11 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n1.31 1.36 1.39 1.56 1.76 \n_____________________________________\n\n## bestNormalize Chosen transformation of CE_Q10 ## \norderNorm Transformation with 13 nonmissing obs and ties\n - 12 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n1.76 1.87 1.95 2.00 2.36 \n_____________________________________\n\n## bestNormalize Chosen transformation of CO2 ## \nStandardized asinh(x) Transformation with 13 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 2.626573 \n - sd (before standardization) = 0.7396036 \n_____________________________________\n\n## bestNormalize Chosen transformation of PX_ase ## \norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 81.25  94.13 119.75 211.18 294.39 \n_____________________________________\n\n## bestNormalize Chosen transformation of PX_Q10 ## \nStandardized Yeo-Johnson Transformation with 13 nonmissing obs.:\n Estimated statistics:\n - lambda = -4.999963 \n - mean (before standardization) = 0.1968145 \n - sd (before standardization) = 0.001394949 \n_____________________________________\n\n## bestNormalize Chosen transformation of CUEcp ## \norderNorm Transformation with 13 nonmissing obs and ties\n - 9 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n0.15 0.17 0.19 0.21 0.33 \n_____________________________________\n\n## bestNormalize Chosen transformation of PUE ## \norderNorm Transformation with 13 nonmissing obs and ties\n - 6 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n0.77 0.88 0.90 0.91 0.92 \n_____________________________________\n\n\n\nShow the complete bestNormalize results\n\n\n\n## Results of bestNormalize for AST ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 3.2051\n - Box-Cox: 3.2051\n - Center+scale: 4.1282\n - Exp(x): 17.9744\n - Log_b(x+a): 3.2051\n - orderNorm (ORQ): 0.1282\n - sqrt(x + a): 4.1282\n - Yeo-Johnson: 2.2821\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n   0%   25%   50%   75%  100% \n25.80 26.26 28.80 29.90 41.77 \n_____________________________________\n\n## Results of bestNormalize for H2O ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 7.2051\n - Box-Cox: 6.2821\n - Center+scale: 8.1282\n - Exp(x): 6.2821\n - Log_b(x+a): 7.2051\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 7.2051\n - Yeo-Johnson: 6.2821\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n   0%   25%   50%   75%  100% \n0.204 0.261 0.377 0.385 0.401 \n_____________________________________\n\n## Results of bestNormalize for Al ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 7.5128\n - Center+scale: 7.5128\n - Exp(x): 7.5128\n - Log_b(x+a): 9.359\n - orderNorm (ORQ): 7.5128\n - sqrt(x + a): 7.5128\n - Yeo-Johnson: 7.5128\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized asinh(x) Transformation with 13 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 0.005384372 \n - sd (before standardization) = 0.007762057 \n_____________________________________\n\n## Results of bestNormalize for Fe ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 7.2051\n - Center+scale: 7.2051\n - Exp(x): 7.2051\n - Log_b(x+a): 7.2051\n - orderNorm (ORQ): 7.2051\n - sqrt(x + a): 7.2051\n - Yeo-Johnson: 7.2051\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized asinh(x) Transformation with 13 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 0.01076821 \n - sd (before standardization) = 0.0103749 \n_____________________________________\n\n## Results of bestNormalize for TEB ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.6667\n - Box-Cox: 1.6667\n - Center+scale: 4.7436\n - Exp(x): 17.9744\n - Log_b(x+a): 1.6667\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 3.2051\n - Yeo-Johnson: 1.359\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and ties\n - 10 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n41.6 43.3 45.1 48.7 61.7 \n_____________________________________\n\n## Results of bestNormalize for ECEC ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.6667\n - Box-Cox: 1.6667\n - Center+scale: 3.2051\n - Exp(x): 17.9744\n - Log_b(x+a): 1.6667\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 3.2051\n - Yeo-Johnson: 1.359\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and ties\n - 10 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n41.7 43.3 45.3 48.8 61.8 \n_____________________________________\n\n## Results of bestNormalize for minNO3 ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 3.5128\n - Box-Cox: 3.5128\n - Center+scale: 5.359\n - Exp(x): 17.9744\n - Log_b(x+a): 3.5128\n - orderNorm (ORQ): 0.7436\n - sqrt(x + a): 2.5897\n - Yeo-Johnson: 3.5128\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 11.31  19.67  22.71  37.33 106.42 \n_____________________________________\n\n## Results of bestNormalize for minTIN ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.359\n - Box-Cox: 2.2821\n - Center+scale: 4.1282\n - Exp(x): 17.9744\n - Log_b(x+a): 1.359\n - orderNorm (ORQ): 0.7436\n - sqrt(x + a): 2.2821\n - Yeo-Johnson: 2.2821\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and ties\n - 12 unique values \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 12.96  21.50  23.99  42.08 110.75 \n_____________________________________\n\n## Results of bestNormalize for DOC ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.8974\n - Box-Cox: 2.8974\n - Center+scale: 3.5128\n - Exp(x): 17.9744\n - Log_b(x+a): 2.8974\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 1.9744\n - Yeo-Johnson: 2.8974\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 33.20  44.30  56.68  63.79 118.74 \n_____________________________________\n\n## Results of bestNormalize for BG_ase ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 3.8205\n - Box-Cox: 2.2821\n - Center+scale: 4.1282\n - Exp(x): 14.8974\n - Log_b(x+a): 3.8205\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 3.5128\n - Yeo-Johnson: 1.6667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n  0%  25%  50%  75% 100% \n3.04 3.92 4.22 4.62 9.09 \n_____________________________________\n\n## Results of bestNormalize for BP_ase ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.6667\n - Box-Cox: 1.6667\n - Center+scale: 4.1282\n - Exp(x): 5.6667\n - Log_b(x+a): 1.6667\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 1.6667\n - Yeo-Johnson: 1.6667\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n  0%  25%  50%  75% 100% \n1.77 2.33 3.02 3.50 7.28 \n_____________________________________\n\n## Results of bestNormalize for P_ase ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.6667\n - Box-Cox: 0.4359\n - Center+scale: 3.2051\n - Exp(x): 17.9744\n - Log_b(x+a): 1.6667\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 1.9744\n - Yeo-Johnson: 0.4359\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized Box Cox Transformation with 13 nonmissing obs.:\n Estimated statistics:\n - lambda = -0.9999576 \n - mean (before standardization) = 0.9480149 \n - sd (before standardization) = 0.01593697 \n_____________________________________\n\n## Results of bestNormalize for N_ase ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.9744\n - Box-Cox: 2.2821\n - Center+scale: 2.8974\n - Exp(x): 17.9744\n - Log_b(x+a): 1.9744\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 2.8974\n - Yeo-Johnson: 0.4359\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n  0%  25%  50%  75% 100% \n2.57 2.95 3.58 4.52 8.34 \n_____________________________________\n\n## Results of bestNormalize for XY_ase ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.0513\n - Box-Cox: 0.4359\n - Center+scale: 2.8974\n - Exp(x): 9.0513\n - Log_b(x+a): 1.0513\n - orderNorm (ORQ): 0.1282\n - sqrt(x + a): 1.359\n - Yeo-Johnson: 0.4359\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and ties\n - 11 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n0.66 0.76 1.11 1.36 2.52 \n_____________________________________\n\n## Results of bestNormalize for BG_Q10 ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.9744\n - Box-Cox: 1.9744\n - Center+scale: 2.2821\n - Exp(x): 5.359\n - Log_b(x+a): 1.9744\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 1.9744\n - Yeo-Johnson: 1.9744\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and ties\n - 11 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n1.31 1.36 1.39 1.56 1.76 \n_____________________________________\n\n## Results of bestNormalize for CE_Q10 ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.6667\n - Box-Cox: 1.6667\n - Center+scale: 1.6667\n - Exp(x): 4.1282\n - Log_b(x+a): 1.6667\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 1.6667\n - Yeo-Johnson: 0.7436\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and ties\n - 12 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n1.76 1.87 1.95 2.00 2.36 \n_____________________________________\n\n## Results of bestNormalize for CO2 ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.0513\n - Box-Cox: 3.5128\n - Center+scale: 11.5128\n - Exp(x): 17.9744\n - Log_b(x+a): 1.0513\n - orderNorm (ORQ): 1.0513\n - sqrt(x + a): 2.8974\n - Yeo-Johnson: 1.9744\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized asinh(x) Transformation with 13 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 2.626573 \n - sd (before standardization) = 0.7396036 \n_____________________________________\n\n## Results of bestNormalize for PX_ase ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 1.9744\n - Box-Cox: 2.2821\n - Center+scale: 4.1282\n - Exp(x): 17.9744\n - Log_b(x+a): 1.9744\n - orderNorm (ORQ): 0.4359\n - sqrt(x + a): 1.9744\n - Yeo-Johnson: 2.2821\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and no ties \n - Original quantiles:\n    0%    25%    50%    75%   100% \n 81.25  94.13 119.75 211.18 294.39 \n_____________________________________\n\n## Results of bestNormalize for PX_Q10 ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 14.5897\n - Box-Cox: 3.2051\n - Center+scale: 17.9744\n - Exp(x): 17.9744\n - Log_b(x+a): 11.5128\n - orderNorm (ORQ): 2.2821\n - sqrt(x + a): 17.9744\n - Yeo-Johnson: 1.0513\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\nStandardized Yeo-Johnson Transformation with 13 nonmissing obs.:\n Estimated statistics:\n - lambda = -4.999963 \n - mean (before standardization) = 0.1968145 \n - sd (before standardization) = 0.001394949 \n_____________________________________\n\n## Results of bestNormalize for CUEcp ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 3.5128\n - Box-Cox: 1.359\n - Center+scale: 3.5128\n - Exp(x): 4.4359\n - Log_b(x+a): 1.359\n - orderNorm (ORQ): 0.7436\n - sqrt(x + a): 3.5128\n - Yeo-Johnson: 2.5897\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and ties\n - 9 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n0.15 0.17 0.19 0.21 0.33 \n_____________________________________\n\n## Results of bestNormalize for PUE ## \nBest Normalizing transformation with 13 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 5.9744\n - Box-Cox: 7.2051\n - Center+scale: 5.9744\n - Exp(x): 5.9744\n - Log_b(x+a): 7.2051\n - orderNorm (ORQ): 2.5897\n - sqrt(x + a): 5.9744\n - Yeo-Johnson: 5.9744\nEstimation method: Out-of-sample via leave-one-out CV\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 13 nonmissing obs and ties\n - 6 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n0.77 0.88 0.90 0.91 0.92 \n_____________________________________\n\n\nGreat, now we can add the normalized transformed data back to our mctoolsr metadata file.\n\nCodeits18_select_mc_norm <- its18_select_mc\n\nfor (i in md_to_tranform) {\n      tmp_get <- get(purrr::map_chr(i, ~ paste0(i, \"_best_norm_test\")))\n      tmp_new_data <- tmp_get$x.t\n      its18_select_mc_norm$map_loaded[[i]] <- tmp_new_data\n      rm(list = ls(pattern = \"tmp_\"))\n}     \n\n\nAnd rerun the Shapiro Tests.\n\nCodetemp_md_norm <- its18_select_mc_norm$map_loaded\ntemp_md_norm[,1:9] <- NULL\nshap_results_norm <- NULL\n\nfor (i in colnames(temp_md_norm)) {\n  tmp_shap <- shapiro.test(temp_md_norm[[i]])\n  tmp_p <- round(tmp_shap$p.value, digits = 5)\n  tmp_res <- eval(isTRUE(tmp_shap$p.value < 0.05))\n  shap_results_norm <- rbind(shap_results_norm, data.frame(i, tmp_p, tmp_res))\n  rm(list = ls(pattern = \"tmp_\"))\n}\ncolnames(shap_results_norm) <- c(\"parameter\", \"p-value\", \"tranform\")\nshap_results_norm\nrm(list = ls(pattern = \"temp_md_norm\"))\n\n\nAnd check if there are any parameters that are still significant for the normality test.\n\nshap_results$parameter[shap_results_norm$tranform == TRUE]\n\n[1] \"Al\" \"Fe\"\n\n\nOk. Looks like bestNormalize was unable to find a suitable transformation for Al and Fe. This is likely because there is very little variation in these metadata and/or there are too few significant digits."
  },
  {
    "objectID": "metadata.html#normalized-metadata-1",
    "href": "metadata.html#normalized-metadata-1",
    "title": "8. Metadata Analysis",
    "section": "Normalized Metadata",
    "text": "Normalized Metadata\nFinally, here is a new summary table that includes all of the normalized data.\n\n\n\n\n Download data as csv file\n\n\n\n(ITS) Table 3 | Results of bestNormalize function applied to each parameter."
  },
  {
    "objectID": "metadata.html#autocorrelation-tests-1",
    "href": "metadata.html#autocorrelation-tests-1",
    "title": "8. Metadata Analysis",
    "section": "Autocorrelation Tests",
    "text": "Autocorrelation Tests\nNext, we test the metadata for autocorrelations. Do we do this on the original data or the transformed data? No idea, so let’s do both.\nSplit Metadata\nWe need to split the data into different groups.\n\nEnvironmental and edaphic properties\nMicrobial functional responses\nTemperature adaptation properties\n\nWe first create lists of metadata parameters.\n\nCodediv <- c(\"PLOT\", \"TREAT\", \"TREAT_T\", \"PAIR\", \"Observed\", \"Shannon_exp\", \n         \"InvSimpson\", \"ATAP\")\n\nedaphic <- c(\"AST\", \"H2O\", \"N\", \"P\", \"Al\", \n             \"Ca\", \"Fe\", \"K\", \"Mg\", \"Mn\", \"Na\", \"TEB\", \"ECEC\", \"pH\", \n             \"NH4\", \"NO3\", \"PO4\", \"DOC\", \"DON\", \"DOCN\")\n\nsoil_funct <- c(\"micC\", \"micN\", \"micP\", \"micCN\", \"micCP\", \"micNP\", \n                \"AG_ase\", \"BG_ase\", \"BP_ase\", \"CE_ase\", \"P_ase\", \"N_ase\", \n                \"S_ase\", \"XY_ase\",  \"LP_ase\", \"PX_ase\", \"CO2\", \n                \"enzCN\", \"enzCP\", \"enzNP\")\n\ntemp_adapt <- c(\"AG_Q10\", \"BG_Q10\", \"BP_Q10\", \"CE_Q10\", \"P_Q10\", \"N_Q10\", \n                \"S_Q10\", \"XY_Q10\", \"LP_Q10\", \"PX_Q10\", \"CUEcn\", \"CUEcp\", \n                \"NUE\", \"PUE\", \"Tmin\", \"SI\")\n\nmd_groups <- c(\"edaphic\", \"soil_funct\", \"temp_adapt\")\n\n# NoT uSed: minPO4, minNH4, minNO3, minTIN\n\n\nAnd then use the lists to split the data sets by metadata group. Here, we do this for the original metadata and the metadata after normalization.\n\nCodeselect_md <- c(\"its18_select_mc\", \"its18_select_mc_norm\")\nfor (i in select_md) {\n   #tmp_get <- get(purrr::map_chr(i, ~ paste0(i, \"_mc\")))\n   tmp_get <- get(i)\n   tmp_md_all <- tmp_get$map_loaded\n   tmp_div <- tmp_md_all %>% dplyr::select(all_of(div))\n   tmp_div <- tmp_div %>% tibble::rownames_to_column(\"SampleID\")\n## edaphic      \n   tmp_sub_edaphic <- tmp_md_all %>% dplyr::select(all_of(edaphic))\n   tmp_sub_edaphic <- tmp_sub_edaphic %>% tibble::rownames_to_column(\"SampleID\")\n   tmp_edaphic <- dplyr::left_join(tmp_div, tmp_sub_edaphic, by = \"SampleID\")\n   tmp_edaphic <- tmp_edaphic %>% tibble::column_to_rownames(\"SampleID\")\n## soil_funct\n   tmp_sub_soil_funct <- tmp_md_all %>% dplyr::select(all_of(soil_funct))\n   tmp_sub_soil_funct <- tmp_sub_soil_funct %>% tibble::rownames_to_column(\"SampleID\")\n   tmp_soil_funct <- dplyr::left_join(tmp_div, tmp_sub_soil_funct, by = \"SampleID\")\n   tmp_soil_funct <- tmp_soil_funct %>% tibble::column_to_rownames(\"SampleID\")      \n## temp_adapt\n   tmp_sub_temp_adapt <- tmp_md_all %>% dplyr::select(all_of(temp_adapt))\n   tmp_sub_temp_adapt <- tmp_sub_temp_adapt %>% tibble::rownames_to_column(\"SampleID\")\n   tmp_temp_adapt <- dplyr::left_join(tmp_div, tmp_sub_temp_adapt, by = \"SampleID\")\n   tmp_temp_adapt <- tmp_temp_adapt %>% tibble::column_to_rownames(\"SampleID\")      \n## combine\n   tmp_list <- list(data_loaded = its18_select_mc$data_loaded, \n                    map_loaded = its18_select_mc$map_loaded, \n                    taxonomy_loaded = its18_select_mc$taxonomy_loaded,\n                    edaphic = tmp_edaphic, \n                    soil_funct = tmp_soil_funct, \n                    temp_adapt = tmp_temp_adapt)\n   tmp_name <- purrr::map_chr(i, ~ paste0(., \"_split\"))\n   print(tmp_name)\n   assign(tmp_name, tmp_list)\n   rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nGenerate Autocorrelation Plots\nA little housekeeping to get rid of parameters we don’t need (e.g., plot number, pair, etc.).\n\nCodeedaphic_cor <- its18_select_mc_split$edaphic\nedaphic_cor[,1:8] <- NULL\nedaphic_norm_cor <- its18_select_mc_norm_split$edaphic\nedaphic_norm_cor[,1:8] <- NULL\n\nsoil_funct_cor <- its18_select_mc_split$soil_funct\nsoil_funct_cor[,1:8] <- NULL\nsoil_funct_norm_cor <- its18_select_mc_norm_split$soil_funct\nsoil_funct_norm_cor[,1:8] <- NULL\n\ntemp_adapt_cor <- its18_select_mc_split$temp_adapt\ntemp_adapt_cor[,1:8] <- NULL\ntemp_adapt_norm_cor <- its18_select_mc_norm_split$temp_adapt\ntemp_adapt_norm_cor[,1:8] <- NULL\n\n\nAnd finally the code to create the plots.\n\nCodefor (i in objects(pattern = \"_cor$\")) {\n  tmp_get <- get(i)\n  tmp_cormat <- round(cor(tmp_get), 2)\n  tmp_melted_cormat <- reshape2::melt(tmp_cormat)\n  \n  tmp_get_lower_tri <- function(tmp_cormat){\n    tmp_cormat[upper.tri(tmp_cormat)] <- NA\n    return(tmp_cormat)\n    }\n  # Get upper triangle of the correlation matrix\n  tmp_get_upper_tri <- function(tmp_cormat){\n    tmp_cormat[lower.tri(tmp_cormat)] <- NA\n    return(tmp_cormat)\n    }\n  tmp_upper_tri <- tmp_get_upper_tri(tmp_cormat)\n  tmp_melted_cormat <- reshape2::melt(tmp_upper_tri, na.rm = TRUE)\n  ggplot(data = tmp_melted_cormat, aes(x = Var1, y = Var2, fill = value)) + \n    geom_tile()\n  \n  tmp_ggheatmap <- ggplot(data = tmp_melted_cormat, aes(Var2, Var1, fill = value)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n                         midpoint = 0, limit = c(-1,1), space = \"Lab\", \n                         name=\"Pearson\\nCorrelation\") +\n    theme_minimal() + \n    theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                     size = 7, hjust = 1),\n          axis.text.y = element_text(vjust = 1, size = 7, hjust = 1)) + \n    coord_fixed() + \n    geom_text(aes(Var2, Var1, label = value), color = \"black\", size = 1.75) +\n    theme(\n      axis.title.x = element_blank(),\n      axis.title.y = element_blank(),\n      panel.grid.major = element_blank(),\n      panel.border = element_blank(),\n      panel.background = element_blank(),\n      axis.ticks = element_blank(),\n      legend.justification = c(1, 0),\n      legend.position = c(0.6, 0.7),\n      legend.direction = \"horizontal\") +\n      guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                                   title.position = \"top\", \n                                   title.hjust = 0.5))\n  tmp_name <- purrr::map_chr(i, ~ paste0(., \"_ggheatmap\"))\n  assign(tmp_name, tmp_ggheatmap)\n  print(tmp_name)\n  rm(list = ls(pattern = \"tmp_\"))\n}  \nobjects(pattern = \"_ggheatmap\")"
  },
  {
    "objectID": "metadata.html#autocorrelation-plots-1",
    "href": "metadata.html#autocorrelation-plots-1",
    "title": "8. Metadata Analysis",
    "section": "Autocorrelation Plots",
    "text": "Autocorrelation Plots\n\n\n\n\nEdaphic properties\nEdaphic properties (normalized)\n\n\n\n\n\n\n\n\n(ITS) Figure 1 | Results of autocorrelation analysis for non-normalized Environmental and edaphic parameters.\n\n\n\n\n\n\n\n(ITS) Figure 2 | Results of autocorrelation analysis for normalized Environmental and edaphic parameters.\n\n\n\n\n\nFunctional responses\nFunctional responses (normalized)\n\n\n\n\n\n\n\n\n(ITS) Figure 3 | Results of autocorrelation analysis for non-normalized Functional response parameters.\n\n\n\n\n\n\n\n(ITS) Figure 4 | Results of autocorrelation analysis for normalized Functional response parameters.\n\n\n\n\n\nTemperature adaptation\nTemperature adaptation (normalized)\n\n\n\n\n\n\n\n\n(ITS) Figure 5 | Results of autocorrelation analysis for non-normalized Temperature adaptation parameters.\n\n\n\n\n\n\n\n(ITS) Figure 6 | Results of autocorrelation analysis for normalized Temperature adaptation parameters.\n\n\n\n\n\n\n\nNow we can remove parameters based on the autocorrelation analysis:\n\nEnvironmental and edaphic properties: TEB, DON, Na, Al, Ca.\nMicrobial functional responses: micN, micNP, enzCN, enzCP, BP_ase, CE_ase, LP_ase, N_ase, P_ase.\nTemperature adaptation properties: NUE, PUE, P_Q10, SI.\n\n\n\n\n\nedaphic_remove <- c(\"TEB\", \"DON\", \"Na\", \"Al\", \"Ca\")\nsoil_funct_remove <- c(\"micN\", \"micNP\", \"enzCN\", \"enzCP\", \"BP_ase\", \n                       \"CE_ase\", \"LP_ase\", \"N_ase\", \"P_ase\")\ntemp_adapt_remove <- c(\"NUE\", \"PUE\", \"P_Q10\", \"SI\")\n\n\nCodetmp_df <- its18_select_mc_split\ntmp_df$edaphic <- tmp_df$edaphic[, ! names(tmp_df$edaphic) %in%  edaphic_remove]\ntmp_df$soil_funct <- tmp_df$soil_funct[, ! names(tmp_df$soil_funct) %in%  soil_funct_remove]\ntmp_df$temp_adapt <- tmp_df$temp_adapt[, ! names(tmp_df$temp_adapt) %in%  temp_adapt_remove]\nits18_select_mc_split_no_ac <- tmp_df\nrm(list = ls(pattern = \"tmp_\"))\n\ntmp_df <- its18_select_mc_norm_split\ntmp_df$edaphic <- tmp_df$edaphic[, ! names(tmp_df$edaphic) %in%  edaphic_remove]\ntmp_df$soil_funct <- tmp_df$soil_funct[, ! names(tmp_df$soil_funct) %in%  soil_funct_remove]\ntmp_df$temp_adapt <- tmp_df$temp_adapt[, ! names(tmp_df$temp_adapt) %in%  temp_adapt_remove]\nits18_select_mc_norm_split_no_ac <- tmp_df\nrm(list = ls(pattern = \"tmp_\"))"
  },
  {
    "objectID": "metadata.html#dissimilarity-correlation-tests-1",
    "href": "metadata.html#dissimilarity-correlation-tests-1",
    "title": "8. Metadata Analysis",
    "section": "Dissimilarity Correlation Tests",
    "text": "Dissimilarity Correlation Tests\nLet’s see if any on the metadata groups are significantly correlated with the community data. Basically, we create distance matrices for the community data and each metadata group and then run Mantel tests for all comparisons. For the community data we calculate Bray-Curtis distances for the community data and Euclidean distances for the metadata. We use the function mantel.test from the ape package and mantel from the vegan package for the analyses.\nIn summary, we test both mantel.test and mantel on Bray-Curtis distance community distances against Euclidean distances for each metadata group (edaphic, soil_funct, temp_adapt) a) before normalizing and before removing autocorrelated parameters, b) before normalizing and after removing autocorrelated parameters, c) after normalizing and before removing autocorrelated parameters, and d) after normalizing and after removing autocorrelated parameters.\n\nCodeman_df <- c(\"its18_select_mc_split\", \"its18_select_mc_split_no_ac\", \n            \"its18_select_mc_norm_split\", \"its18_select_mc_norm_split_no_ac\")\n\nfor (i in man_df) {\n  tmp_get <- get(i)\n  tmp_dm_otu <- as.matrix(vegdist(t(tmp_get$data_loaded), \n                             method = \"bray\", binary = FALSE, \n                             diag = TRUE, upper = TRUE, na.rm = FALSE))\n# EDAPHIC\n  tmp_dm_md_edaphic <- as.matrix(vegdist(tmp_get$edaphic[, 8:ncol(tmp_get$edaphic)], \n                                    method =\"euclidean\", binary = FALSE, \n                                    diag = TRUE, upper = TRUE, na.rm = FALSE))\n\n  tmp_man1_edaphic <- mantel.test(tmp_dm_otu, tmp_dm_md_edaphic, nperm = 999, \n                             graph = FALSE, alternative = \"two.sided\")\n  tmp_man2_edaphic <- mantel(tmp_dm_otu, tmp_dm_md_edaphic, permutations = 999)\n# SOIL FUNCT\n  tmp_dm_md_soil_funct <- as.matrix(vegdist(tmp_get$soil_funct[, 8:ncol(tmp_get$soil_funct)], \n                                       method = \"euclidean\", binary = FALSE, \n                                       diag = TRUE, upper = TRUE, na.rm = FALSE))\n\n  tmp_man1_soil_funct <- mantel.test(tmp_dm_otu, tmp_dm_md_soil_funct, nperm = 999, \n                                graph = FALSE, alternative = \"two.sided\")\n  tmp_man2_soil_funct <- mantel(tmp_dm_otu, tmp_dm_md_soil_funct, permutations = 999)\n\n# TEMP ADAPT\n  tmp_dm_md_temp_adapt <- as.matrix(vegdist(tmp_get$temp_adapt[, 8:ncol(tmp_get$temp_adapt)], \n                                       method = \"euclidean\", binary = FALSE, \n                                       diag = TRUE, upper = TRUE, na.rm = FALSE))\n\n  tmp_man1_temp_adapt <- mantel.test(tmp_dm_otu, tmp_dm_md_temp_adapt, nperm = 999, \n                                graph = FALSE, alternative = \"two.sided\")\n  tmp_man2_temp_adapt <- mantel(tmp_dm_otu, tmp_dm_md_temp_adapt, permutations = 999)\n\n  tmp_name <- purrr::map_chr(i, ~ paste0(., \"_mantel_tests\"))\n  tmp_df <- list(edaphic_ape_man = tmp_man1_edaphic, \n                 edaphic_vegan_man = tmp_man2_edaphic,\n                 soil_funct_ape_man = tmp_man1_soil_funct, \n                 soil_funct_vegan_man = tmp_man2_soil_funct,\n                 temp_adapt_ape_man = tmp_man1_temp_adapt, \n                 temp_adapt_vegan_man = tmp_man2_temp_adapt)\n  assign(tmp_name, tmp_df)\n  print(tmp_name)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\n\n\nDissimilarity Correlation Results\n(ITS) Table 4 | Summary of Dissimilarity Correlation Tests using mantel.test from the ape package and mantel from the vegan. P-values in red indicate significance (p-value < 0.05)\n\n\n\n\n\n\n\nMoving on."
  },
  {
    "objectID": "metadata.html#best-subset-of-variables-1",
    "href": "metadata.html#best-subset-of-variables-1",
    "title": "8. Metadata Analysis",
    "section": "Best Subset of Variables",
    "text": "Best Subset of Variables\nNow we want to know which of the metadata parameters are the most strongly correlated with the community data. For this we use the bioenv function from the vegan package. bioenv—Best Subset of Environmental Variables with Maximum (Rank) Correlation with Community Dissimilarities—finds the best subset of environmental variables, so that the Euclidean distances of scaled environmental variables have the maximum (rank) correlation with community dissimilarities.\nSince we know that each of the Mantel tests we ran above are significant, here we will use the metadata set where autocorrelated parameters were removed and the remainder of the parameters were normalized (where applicable based on the Shapiro tests).\nWe run bioenv against the three groups of metadata parameters. We then run bioenv again, but this time against the individual parameters identified as significantly correlated.\nEdaphic Properties\n\nCodetmp_comm <- data.frame(t(its18_select_mc_norm_split_no_ac$data_loaded))\ntmp_env <- data.frame(its18_select_mc_norm_split_no_ac$edaphic)\ntmp_env[,1:8] <- NULL\n\nedaphic_bioenv <- bioenv(wisconsin(tmp_comm), tmp_env, \n                         method = \"spearman\", index = \"bray\", \n                         upto = ncol(tmp_env), metric = \"euclidean\")\n\nbioenv_list <- edaphic_bioenv$models[[edaphic_bioenv$whichbest]]$best\nbioenv_best <- bioenvdist(edaphic_bioenv, which = \"best\")\n\nfor (i in bioenv_list) {\n       tmp_dp <- data.frame(edaphic_bioenv$x)\n       tmp_md <- as.matrix(vegdist(tmp_dp[[i]], \n                                   method = \"euclidean\", binary = FALSE, \n                                   diag = TRUE, upper = TRUE, na.rm = FALSE))\n       tmp_man <- mantel(bioenv_best, tmp_md, \n                         permutations = 999, method = \"spearman\")\n       tmp_md_name <- names(tmp_dp)[[i]]\n       tmp_name <- purrr::map_chr(tmp_md_name, ~ paste0(., \"_bioenv_mantel_test\"))\n       assign(tmp_name, tmp_man)\n       rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_bioenv_mantel_test\")\n\nedaphic_bioenv_ind_mantel <- list(AST = AST_bioenv_mantel_test)\nrm(list = ls(pattern = \"_bioenv_mantel_test\"))\n\n\n\n\n\nCall:\nbioenv(comm = wisconsin(tmp_comm), env = tmp_env, method = \"spearman\",      index = \"bray\", upto = ncol(tmp_env), metric = \"euclidean\") \n\nSubset of environmental variables with best correlation to community data.\n\nCorrelations:    spearman \nDissimilarities: bray \nMetric:          euclidean \n\nBest model has 1 parameters (max. 15 allowed):\nAST\nwith correlation  0.65665 \n\n\n\nShow the results of individual edaphic metadata Mantel tests\n\n\n$AST\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r:     1 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.192 0.242 0.272 0.339 \nPermutation: free\nNumber of permutations: 999\n\n\nbioenv found the following edaphic properties significantly correlated with the community data: AST\nSoil Functional Response\n\nCodetmp_comm <- data.frame(t(its18_select_mc_norm_split_no_ac$data_loaded))\ntmp_env <- data.frame(its18_select_mc_norm_split_no_ac$soil_funct)\ntmp_env[,1:8] <- NULL\n\nsoil_funct_bioenv <- bioenv(wisconsin(tmp_comm), tmp_env, \n                         method = \"spearman\", index = \"bray\", \n                         upto = ncol(tmp_env), metric = \"euclidean\")\n\nbioenv_list <- soil_funct_bioenv$models[[soil_funct_bioenv$whichbest]]$best\nbioenv_best <- bioenvdist(soil_funct_bioenv, which = \"best\")\n\nfor (i in bioenv_list) {\n       tmp_dp <- data.frame(soil_funct_bioenv$x)\n       tmp_md <- as.matrix(vegdist(tmp_dp[[i]], \n                                   method = \"euclidean\", binary = FALSE, \n                                   diag = TRUE, upper = TRUE, na.rm = FALSE))\n       tmp_man <- mantel(bioenv_best, tmp_md, \n                         permutations = 999, method = \"spearman\")\n       tmp_md_name <- names(tmp_dp)[[i]]\n       tmp_name <- purrr::map_chr(tmp_md_name, ~ paste0(., \"_bioenv_mantel_test\"))\n       assign(tmp_name, tmp_man)\n       rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_bioenv_mantel_test\")\n\nsoil_funct_bioenv_ind_mantel <- list(enzNP = enzNP_bioenv_mantel_test, \n                                     PX_ase = PX_ase_bioenv_mantel_test,\n                                     XY_ase = XY_ase_bioenv_mantel_test)\nrm(list = ls(pattern = \"_bioenv_mantel_test\"))\n\n\n\n\n\nCall:\nbioenv(comm = wisconsin(tmp_comm), env = tmp_env, method = \"spearman\",      index = \"bray\", upto = ncol(tmp_env), metric = \"euclidean\") \n\nSubset of environmental variables with best correlation to community data.\n\nCorrelations:    spearman \nDissimilarities: bray \nMetric:          euclidean \n\nBest model has 3 parameters (max. 11 allowed):\nXY_ase PX_ase enzNP\nwith correlation  0.7491622 \n\n\n\nShow the results of individual functional response metadata Mantel tests\n\n\n$enzNP\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.5529 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.202 0.277 0.330 0.417 \nPermutation: free\nNumber of permutations: 999\n\n\n$PX_ase\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.6854 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.209 0.282 0.361 0.412 \nPermutation: free\nNumber of permutations: 999\n\n\n$XY_ase\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.5054 \n      Significance: 0.002 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.202 0.261 0.308 0.359 \nPermutation: free\nNumber of permutations: 999\n\n\nbioenv found the following soil functions significantly correlated with the community data: enzNP, PX_ase, XY_ase\nTemperature Adaptation\n\nCodetmp_comm <- data.frame(t(its18_select_mc_norm_split_no_ac$data_loaded))\ntmp_env <- data.frame(its18_select_mc_norm_split_no_ac$temp_adapt)\ntmp_env[,1:8] <- NULL\n\ntemp_adapt_bioenv <- bioenv(wisconsin(tmp_comm), tmp_env, \n                         method = \"spearman\", index = \"bray\", \n                         upto = ncol(tmp_env), metric = \"euclidean\")\n\nbioenv_list <- temp_adapt_bioenv$models[[temp_adapt_bioenv$whichbest]]$best\nbioenv_best <- bioenvdist(temp_adapt_bioenv, which = \"best\")\n\nfor (i in bioenv_list) {\n       tmp_dp <- data.frame(temp_adapt_bioenv$x)\n       tmp_md <- as.matrix(vegdist(tmp_dp[[i]], \n                                   method = \"euclidean\", binary = FALSE, \n                                   diag = TRUE, upper = TRUE, na.rm = FALSE))\n       tmp_man <- mantel(bioenv_best, tmp_md, \n                         permutations = 999, method = \"spearman\")\n       tmp_md_name <- names(tmp_dp)[[i]]\n       tmp_name <- purrr::map_chr(tmp_md_name, ~ paste0(., \"_bioenv_mantel_test\"))\n       assign(tmp_name, tmp_man)\n       rm(list = ls(pattern = \"tmp_\"))\n}\nobjects(pattern = \"_bioenv_mantel_test\")\n\ntemp_adapt_bioenv_ind_mantel <- list(XY_Q10 = XY_Q10_bioenv_mantel_test,\n                                     Tmin = Tmin_bioenv_mantel_test)\nrm(list = ls(pattern = \"_bioenv_mantel_test\"))\n\n\n\n\n\nCall:\nbioenv(comm = wisconsin(tmp_comm), env = tmp_env, method = \"spearman\",      index = \"bray\", upto = ncol(tmp_env), metric = \"euclidean\") \n\nSubset of environmental variables with best correlation to community data.\n\nCorrelations:    spearman \nDissimilarities: bray \nMetric:          euclidean \n\nBest model has 2 parameters (max. 12 allowed):\nXY_Q10 Tmin\nwith correlation  0.5637527 \n\n\n\nShow the results of individual temperature adaptation metadata Mantel tests\n\n\n$XY_Q10\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.7258 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.134 0.200 0.241 0.316 \nPermutation: free\nNumber of permutations: 999\n\n\n$Tmin\n\nMantel statistic based on Spearman's rank correlation rho \n\nCall:\nmantel(xdis = bioenv_best, ydis = tmp_md, method = \"spearman\",      permutations = 999) \n\nMantel statistic r: 0.616 \n      Significance: 0.001 \n\nUpper quantiles of permutations (null model):\n  90%   95% 97.5%   99% \n0.189 0.245 0.297 0.360 \nPermutation: free\nNumber of permutations: 999\n\n\nbioenv found the following temperature adaptations significantly correlated with the community data: XY_Q10, Tmin"
  },
  {
    "objectID": "metadata.html#distance-based-redundancy-1",
    "href": "metadata.html#distance-based-redundancy-1",
    "title": "8. Metadata Analysis",
    "section": "Distance-based Redundancy",
    "text": "Distance-based Redundancy\nNow we turn our attention to distance-based redundancy analysis (dbRDA), an ordination method similar to Redundancy Analysis (rda) but it allows non-Euclidean dissimilarity indices, such as Manhattan or Bray–Curtis distance. For this, we use capscale from the vegan package. capscale is a constrained versions of metric scaling (principal coordinates analysis), which are based on the Euclidean distance but can be used, and are more useful, with other dissimilarity measures. The functions can also perform unconstrained principal coordinates analysis, optionally using extended dissimilarities.\nFor each of the three metadata subsets, we perform the following steps:\n\nRun rankindex to compare metadata and community dissimilarity indices for gradient detection. This will help us select the best dissimilarity metric to use.\nRun capscale for distance-based redundancy analysis.\nRun envfit to fit environmental parameters onto the ordination. This function basically calculates correlation scores between the metadata parameters and the ordination axes.\nSelect metadata parameters significant for bioenv (see above) and/or envfit analyses.\nRun envfit on ASVs.\nPlot the ordination and vector overlays.\n\nEdaphic Properties\n\nCodetmp_md <- its18_select_mc_norm_split_no_ac$edaphic\ntmp_md$TREAT_T <- as.character(tmp_md$TREAT_T)\ntmp_comm <- data.frame(t(its18_select_mc_norm_split_no_ac$data_loaded))\nedaphic_rank <- rankindex(tmp_md[, 8:ncol(tmp_md)], tmp_comm, \n          indices = c(\"euc\", \"man\", \"gow\",\"bra\", \"kul\"), \n          stepacross = FALSE, method = \"spearman\")\n\n\n\n\n        euc         man         gow         bra         kul \n 0.15398526  0.29983940 -0.02940098  0.29983940  0.29983940 \n\n\nLet’s run capscale using Bray-Curtis. Note, we have 15 metadata parameters in this group but, for some reason, capscale only works with 13 parameters. This may have to do with degrees of freedom?\n\nStarting properties: AST, H2O, N, P, Al, Ca, Fe, K, Mg, Mn, Na, TEB, ECEC, pH, NH4, NO3, PO4, DOC, DON, DOCN.\nAutocorrelated removed: TEB, DON, Na, Al, Ca.\nRemove for capscale: Mg, Mn, Na, Al, Fe, K\n\n\n\n\n\nedaphic_cap <- capscale(tmp_comm ~ AST + H2O + N + P + ECEC + pH + \n                                   NH4 + NO3 + PO4 + DOC + DOCN, \n                        tmp_md, dist = \"bray\")\ncolnames(tmp_md)\n\n\n\nCall: capscale(formula = tmp_comm ~ AST + H2O + N + P + ECEC + pH + NH4\n+ NO3 + PO4 + DOC + DOCN, data = tmp_md, distance = \"bray\")\n\n              Inertia Proportion Rank\nTotal         3.88624    1.00000     \nConstrained   3.66503    0.94308   11\nUnconstrained 0.22120    0.05692    1\nInertia is squared Bray distance \nSpecies scores projected from 'tmp_comm' \n\nEigenvalues for constrained axes:\n  CAP1   CAP2   CAP3   CAP4   CAP5   CAP6   CAP7   CAP8   CAP9  CAP10  CAP11 \n0.9655 0.5791 0.4327 0.3671 0.2987 0.2608 0.2301 0.1867 0.1339 0.1135 0.0970 \n\nEigenvalues for unconstrained axes:\n  MDS1 \n0.2212 \n\n\nNow we can look at the variance against each principal component.\n\n\n\n\n\nAnd then make some quick and dirty plots. This will also come in handy later when we need to parse out data a better plot visualization. The ggplot function autoplot stores these data in a more accessible way than the raw results from capscale\n\nbase::plot(edaphic_cap) \n\n\n\ntmp_auto_plt <- ggplot2::autoplot(edaphic_cap, arrows = TRUE)\ntmp_auto_plt\n\n\n\n\n\n\n\nNext, we need to grab capscale scores for the samples and create a data frame of the first two dimensions. We will also need to add some of the sample details to the data frame. For this we use the vegan function scores which gets species or site scores from the ordination.\n\nCodetmp_samp_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"sites\")\ntmp_samp_scores[,1] <- NULL\ntmp_samp_scores <- tmp_samp_scores %>% dplyr::rename(SampleID = Label)\n\ntmp_md_sub <- tmp_md[, 1:4]\ntmp_md_sub <- tmp_md_sub %>% tibble::rownames_to_column(\"SampleID\")\nedaphic_plot_data <- dplyr::left_join(tmp_md_sub, tmp_samp_scores, by = \"SampleID\")\n\n\nNow we have a new data frame that contains sample details and capscale values.\n\n\n          SampleID PLOT TREAT TREAT_T PAIR        CAP1        CAP2\n1  P10_D00_010_C0E  P10     C       0    E -0.67150210 -1.07183211\n2  P02_D00_010_C0A  P02     C       0    A -0.91911652 -0.41647565\n3  P04_D00_010_C0B  P04     C       0    B -0.95654588  0.64087450\n4  P06_D00_010_C0C  P06     C       0    C -0.87992057  0.68082267\n5  P08_D00_010_C0D  P08     C       0    D -0.07766291 -0.98166349\n6  P01_D00_010_W3A  P01     W       3    A -0.79165161 -0.43340347\n7  P03_D00_010_W3B  P03     W       3    B  0.24722812  0.81805851\n8  P07_D00_010_W3D  P07     W       3    D  0.20151965  1.07639049\n9  P09_D00_010_W3E  P09     W       3    E  0.58233467  0.46562279\n10 P01_D00_010_W8A  P01     W       8    A  0.92158009 -0.06825914\n11 P03_D00_010_W8B  P03     W       8    B  0.94079184 -0.52231363\n12 P05_D00_010_W8C  P05     W       8    C  0.87642984 -0.89155825\n13 P07_D00_010_W8D  P07     W       8    D  0.52651539  0.70373678\n\n\nWe can then do the same with the metadata vectors. Here though we only need the scores and parameter name.\n\nCodeedaphic_md_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"biplot\")\nedaphic_md_scores[,1] <- NULL\nedaphic_md_scores <- edaphic_md_scores %>% \n  dplyr::mutate(parameters = Label, .before = CAP1) %>%\n  tibble::column_to_rownames(\"Label\")\n\n\n\n\n     parameters         CAP1         CAP2\nAST         AST  0.834527196  0.117196419\nH2O         H2O -0.596509128  0.006131247\nN             N  0.245428273  0.506082992\nP             P  0.097845581  0.205793005\nECEC       ECEC -0.304860202  0.538475609\npH           pH -0.081297235 -0.092037201\nNH4         NH4 -0.297079795  0.210528906\nNO3         NO3 -0.264254841  0.225782469\nPO4         PO4 -0.583698114  0.354036263\nDOC         DOC -0.884261492  0.025360275\nDOCN       DOCN  0.007258364 -0.462404778\n\n\nLet’s run some quick correlations of metadata with ordination axes to see which parameters are significant. For this we use the vegan function envfit.\n\nCodetmp_samp_scores_sub <- edaphic_plot_data[, 6:7]\ntmp_samp_scores_sub <- as.matrix(tmp_samp_scores_sub)\ntmp_param_list <- edaphic_md_scores$parameters\n\ntmp_md_sub <- subset(tmp_md,  select =  tmp_param_list)\n\nenvfit_edaphic_md <- envfit(tmp_samp_scores_sub, tmp_md_sub,\n                 perm = 1000, choices = c(1, 2))\n\n\n\n\n\n***VECTORS\n\n         CAP1     CAP2     r2  Pr(>r)  \nAST   0.99126  0.13189 0.4849 0.03696 *\nH2O  -0.99989  0.01511 0.2434 0.25275  \nN     0.43981  0.89809 0.2124 0.27473  \nP     0.43273  0.90152 0.0349 0.86014  \nECEC -0.50151  0.86515 0.2595 0.21778  \npH   -0.66815 -0.74403 0.0102 0.94705  \nNH4  -0.81970  0.57279 0.0906 0.60140  \nNO3  -0.76538  0.64357 0.0824 0.64635  \nPO4  -0.85783  0.51393 0.3187 0.15185  \nDOC  -0.99945  0.03317 0.5354 0.02797 *\nDOCN  0.02108 -0.99978 0.1437 0.47552  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodeedaphic_md_signif_hits <- base::subset(envfit_edaphic_md$vectors$pvals, \n                                  c(envfit_edaphic_md$vectors$pvals \n                                  < 0.05 & envfit_edaphic_md$vectors$r > 0.4))\nedaphic_md_signif_hits <- data.frame(edaphic_md_signif_hits)\nedaphic_md_signif_hits <- rownames(edaphic_md_signif_hits)\nedaphic_md_signif <- edaphic_md_scores[edaphic_md_scores$parameters %in% edaphic_md_signif_hits,]\nedaphic_md_signif$parameters\n\n\nenvfit found that AST, DOC were significantly correlated.\nNow let’s see if the same parameters are significant for the envfit and bioenv analyses.\n\n\n[1] \"Significant parameters from bioenv analysis.\"\n[1] \"AST\"\n_____________________________________\n[1] \"Significant parameters from envfit analysis.\"\n[1] \"AST\" \"DOC\"\n_____________________________________\n[1] \"Found in bioenv but not envfit.\"\ncharacter(0)\n_____________________________________\n[1] \"Found in envfit but not bioenv.\"\n[1] \"DOC\"\n_____________________________________\n[1] \"Found in envfit and bioenv.\"\n[1] \"AST\" \"DOC\"\n\n\n\nCodenew_edaphic_md_signif_hits <- edaphic_sig_diff\n#new_edaphic_md_signif_hits <- append(edaphic_md_signif_hits, edaphic_sig_diff)\nedaphic_md_signif_all <- edaphic_md_scores[edaphic_md_scores$parameters %in% new_edaphic_md_signif_hits,]\n\n\nCheck. Next, we run envfit for the ASVs.\n\nCodeenvfit_edaphic_asv <- envfit(tmp_samp_scores_sub, \n                             tmp_comm[, order(colSums(-tmp_comm))][, 1:10],\n                             perm = 1000, choices = c(1, 2))\nedaphic_asv_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"species\")\nedaphic_asv_scores <- edaphic_asv_scores %>% \n  dplyr::mutate(parameters = Label, .before = CAP1) %>%\n  tibble::column_to_rownames(\"Label\")\nedaphic_asv_scores[,1] <- NULL\n\n\n\n\n\n***VECTORS\n\n          CAP1     CAP2     r2   Pr(>r)   \nASV1  -0.61940 -0.78508 0.3539 0.010989 * \nASV2   0.73098 -0.68240 0.5575 0.003996 **\nASV4  -0.33832 -0.94103 0.4713 0.010989 * \nASV3  -0.89855  0.43888 0.4897 0.002997 **\nASV15  0.95614  0.29292 0.2269 0.192807   \nASV8  -0.87871 -0.47735 0.1267 0.795205   \nASV9   0.73176  0.68156 0.1410 0.468531   \nASV24  0.45502  0.89048 0.1661 0.459540   \nASV12  0.12678  0.99193 0.7513 0.001998 **\nASV13 -0.42157 -0.90680 0.4286 0.060939 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodeedaphic_asv_signif_hits <- base::subset(envfit_edaphic_asv$vectors$pvals, \n                                  c(envfit_edaphic_asv$vectors$pvals \n                                  < 0.05 & envfit_edaphic_asv$vectors$r > 0.5))\nedaphic_asv_signif_hits <- data.frame(edaphic_asv_signif_hits)\nedaphic_asv_signif_hits <- rownames(edaphic_asv_signif_hits)\nedaphic_asv_signif <- edaphic_asv_scores[edaphic_asv_scores$parameters %in% edaphic_asv_signif_hits,]\n\n\n\n\n      parameters       CAP1       CAP2 variable_type\nASV12      ASV12 0.03291551  0.2041479           ASV\nASV2        ASV2 0.48655008 -0.3456690           ASV\n\n\n\nCodeedaphic_md_signif_all$variable_type <- \"metadata\"\nedaphic_asv_signif$variable_type <- \"ASV\"\nedaphic_bioplot_data <- rbind(edaphic_md_signif_all, edaphic_asv_signif)\n\n\nThe last thing to do is categorize parameters scores and ASV scores into different variable types for plotting.\n\nCodeedaphic_bioplot_data_md <- subset(edaphic_bioplot_data, \n                                  edaphic_bioplot_data$variable_type == \"metadata\")\nedaphic_bioplot_data_asv <- subset(edaphic_bioplot_data, \n                                   edaphic_bioplot_data$variable_type == \"ASV\")\n\n\n\nShow the code for the plot\n\n\n\n\n\n\nSoil Functional Response\n\nCodetmp_md <- its18_select_mc_norm_split_no_ac$soil_funct\ntmp_md$TREAT_T <- as.character(tmp_md$TREAT_T)\ntmp_comm <- data.frame(t(its18_select_mc_norm_split_no_ac$data_loaded))\nsoil_funct_rank <- rankindex(tmp_md[, 8:ncol(tmp_md)], tmp_comm, \n          indices = c(\"euc\", \"man\", \"gow\", \"bra\", \"kul\"), \n          stepacross = FALSE, method = \"spearman\")\n\n\n\n\n        euc         man         gow         bra         kul \n 0.06025620  0.38790324 -0.06200129  0.38790324  0.38790324 \n\n\nLet’s run capscale using Bray-Curtis Note, we have 11 metadata parameters in this group but, for some reason, capscale only works with 13 parameters. This may have to do with degrees of freedom?\n\nStarting properties: micC, micN, micP, micCN, micCP, micNP, AG_ase, BG_ase, BP_ase, CE_ase, P_ase, N_ase, S_ase, XY_ase, LP_ase, PX_ase, CO2, enzCN, enzCP, enzNP\nAutocorrelated removed: micN, micNP, enzCN, enzCP, BP_ase, CE_ase, LP_ase, N_ase, P_ase\nRemove for capscale: NONE\n\n\n\n\n\nsoil_funct_cap <- capscale(tmp_comm ~  micC + micP + micCN + micCP + AG_ase + BG_ase + \n                                       S_ase + XY_ase + PX_ase + CO2 + enzNP, \n                           tmp_md, dist = \"bray\")\n\n\n\nCall: capscale(formula = tmp_comm ~ micC + micP + micCN + micCP +\nAG_ase + BG_ase + S_ase + XY_ase + PX_ase + CO2 + enzNP, data = tmp_md,\ndistance = \"bray\")\n\n              Inertia Proportion Rank\nTotal         3.88624    1.00000     \nConstrained   3.59902    0.92609   11\nUnconstrained 0.28722    0.07391    1\nInertia is squared Bray distance \nSpecies scores projected from 'tmp_comm' \n\nEigenvalues for constrained axes:\n  CAP1   CAP2   CAP3   CAP4   CAP5   CAP6   CAP7   CAP8   CAP9  CAP10  CAP11 \n0.9666 0.5891 0.3939 0.3377 0.2686 0.2635 0.2306 0.1940 0.1389 0.1143 0.1018 \n\nEigenvalues for unconstrained axes:\n   MDS1 \n0.28722 \n\n\nNow we can look at the variance against each principal component.\n\n\n\n\n\nAnd then make some quick and dirty plots. This will also come in handy later when we need to parse out data a better plot visualization. The ggplot function autoplot stores these data in a more accessible way than the raw results from capscale\n\nbase::plot(soil_funct_cap) \n\n\n\ntmp_auto_plt <- autoplot(soil_funct_cap, arrows = TRUE)\ntmp_auto_plt\n\n\n\n\n\n\n\nNext, we need to grab capscale scores for the samples and create a data frame of the first two dimensions. We will also need to add some of the sample details to the data frame. For this we use the vegan function scores which gets species or site scores from the ordination.\n\nCodetmp_samp_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"sites\")\ntmp_samp_scores[,1] <- NULL\ntmp_samp_scores <- tmp_samp_scores %>% dplyr::rename(SampleID = Label)\n\ntmp_md_sub <- tmp_md[, 1:4]\ntmp_md_sub <- tmp_md_sub %>% tibble::rownames_to_column(\"SampleID\")\nsoil_funct_plot_data <- dplyr::left_join(tmp_md_sub, tmp_samp_scores, \n                                         by = \"SampleID\")\n\n\nNow we have a new data frame that contains sample details and capscale values.\n\n\n          SampleID PLOT TREAT TREAT_T PAIR        CAP1       CAP2\n1  P10_D00_010_C0E  P10     C       0    E -0.69489162 -0.6675049\n2  P02_D00_010_C0A  P02     C       0    A -0.91653485 -0.6229848\n3  P04_D00_010_C0B  P04     C       0    B -0.95221897  0.3986135\n4  P06_D00_010_C0C  P06     C       0    C -0.85713972  0.6268677\n5  P08_D00_010_C0D  P08     C       0    D -0.09115803 -1.0802252\n6  P01_D00_010_W3A  P01     W       3    A -0.80868973 -0.1643386\n7  P03_D00_010_W3B  P03     W       3    B  0.27067347  0.9200087\n8  P07_D00_010_W3D  P07     W       3    D  0.20427135  1.0264780\n9  P09_D00_010_W3E  P09     W       3    E  0.57963547  0.4440199\n10 P01_D00_010_W8A  P01     W       8    A  0.91663981 -0.1382072\n11 P03_D00_010_W8B  P03     W       8    B  0.92199435 -0.6783194\n12 P05_D00_010_W8C  P05     W       8    C  0.86406950 -0.9575454\n13 P07_D00_010_W8D  P07     W       8    D  0.56334898  0.8931377\n\n\nWe can then do the same with the metadata vectors. Here though we only need the scores and parameter name.\n\nCodesoil_funct_md_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"biplot\")\nsoil_funct_md_scores[,1] <- NULL\nsoil_funct_md_scores <- soil_funct_md_scores %>% \n  dplyr::mutate(parameters = Label, .before = CAP1) %>% \n  tibble::column_to_rownames(\"Label\")\n\n\n\n\n       parameters       CAP1         CAP2\nmicC         micC -0.2359175  0.400384343\nmicP         micP -0.4872005  0.846844390\nmicCN       micCN  0.5085406  0.215263003\nmicCP       micCP  0.4299937 -0.786734788\nAG_ase     AG_ase -0.3381241  0.763769448\nBG_ase     BG_ase  0.2150249  0.021210923\nS_ase       S_ase  0.1648172  0.678698244\nXY_ase     XY_ase  0.6119362  0.164529372\nPX_ase     PX_ase  0.8280069 -0.009182309\nCO2           CO2  0.5373302 -0.181241128\nenzNP       enzNP -0.8493447  0.170758052\n\n\nLet’s run some quick correlations of metadata with ordination axes to see which parameters are significant. For this we use the vegan function envfit.\n\nCodetmp_samp_scores_sub <- soil_funct_plot_data[, 6:7]\ntmp_samp_scores_sub <- as.matrix(tmp_samp_scores_sub)\ntmp_param_list <- soil_funct_md_scores$parameters\n\ntmp_md_sub <- subset(tmp_md,  select =  tmp_param_list)\n\nenvfit_soil_funct_md <- envfit(tmp_samp_scores_sub, tmp_md_sub,\n                 perm = 1000, choices = c(1, 2))\n\n\n\n\n\n***VECTORS\n\n           CAP1     CAP2     r2   Pr(>r)   \nmicC   -0.50911  0.86070 0.1567 0.442557   \nmicP   -0.50009  0.86597 0.6925 0.001998 **\nmicCN   0.92125  0.38897 0.2225 0.261738   \nmicCP   0.48092 -0.87676 0.5831 0.015984 * \nAG_ase -0.40574  0.91399 0.5060 0.036963 * \nBG_ase  0.99507  0.09917 0.0341 0.832168   \nS_ase   0.23862  0.97111 0.3542 0.099900 . \nXY_ase  0.96571  0.25962 0.2931 0.170829   \nPX_ase  0.99996 -0.00944 0.5002 0.034965 * \nCO2     0.94850 -0.31679 0.2342 0.250749   \nenzNP  -0.98088  0.19460 0.5471 0.013986 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodesoil_funct_md_signif_hits <- base::subset(envfit_soil_funct_md$vectors$pvals, \n                                  c(envfit_soil_funct_md$vectors$pvals \n                                  < 0.05 & envfit_soil_funct_md$vectors$r > 0.4))\nsoil_funct_md_signif_hits <- data.frame(soil_funct_md_signif_hits)\nsoil_funct_md_signif_hits <- rownames(soil_funct_md_signif_hits)\nsoil_funct_md_signif <- soil_funct_md_scores[soil_funct_md_scores$parameters %in% \n                                               soil_funct_md_signif_hits,]\nsoil_funct_md_signif$parameters\n\n\nenvfit found that micP, micCP, AG_ase, PX_ase, enzNP were significantly correlated.\nNow let’s see if the same parameters are significant for the envfit and bioenv analyses.\n\n\n[1] \"Significant parameters from bioenv analysis.\"\n[1] \"enzNP\"  \"PX_ase\" \"XY_ase\"\n_____________________________________\n[1] \"Significant parameters from envfit analysis.\"\n[1] \"micP\"   \"micCP\"  \"AG_ase\" \"PX_ase\" \"enzNP\" \n_____________________________________\n[1] \"Found in bioenv but not envfit.\"\n[1] \"XY_ase\"\n_____________________________________\n[1] \"Found in envfit but not bioenv.\"\n[1] \"micP\"   \"micCP\"  \"AG_ase\"\n_____________________________________\n[1] \"Found in envfit and bioenv.\"\n[1] \"micP\"   \"micCP\"  \"AG_ase\" \"PX_ase\" \"enzNP\"  \"XY_ase\"\n\n\n\nCode#new_soil_funct_md_signif_hits <- append(soil_funct_md_signif_hits, soil_funct_sig_diff)\nnew_soil_funct_md_signif_hits <- soil_funct_sig_diff\nsoil_funct_md_signif_all <- soil_funct_md_scores[soil_funct_md_scores$parameters %in% \n                                                   new_soil_funct_md_signif_hits,]\n\n\nCheck. Next, we run envfit for the ASVs.\n\nCodeenvfit_soil_funct_asv <- envfit(tmp_samp_scores_sub, \n                                tmp_comm[, order(colSums(-tmp_comm))][, 1:10], \n                                perm = 1000, choices = c(1, 2))\nsoil_funct_asv_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"species\")\nsoil_funct_asv_scores <- soil_funct_asv_scores %>% \n  dplyr::mutate(parameters = Label, .before = CAP1) %>% \n  tibble::column_to_rownames(\"Label\")\nsoil_funct_asv_scores[,1] <- NULL\n\n\n\n\n\n***VECTORS\n\n          CAP1     CAP2     r2   Pr(>r)    \nASV1  -0.79963 -0.60050 0.2256 0.245754    \nASV2   0.68178 -0.73156 0.6129 0.000999 ***\nASV4  -0.33208 -0.94325 0.5433 0.000999 ***\nASV3  -0.92213  0.38687 0.4516 0.010989 *  \nASV15  0.95847  0.28518 0.2287 0.159840    \nASV8  -0.97867 -0.20544 0.1064 0.930070    \nASV9   0.76380  0.64546 0.1326 0.481518    \nASV24  0.40983  0.91216 0.2438 0.065934 .  \nASV12  0.14788  0.98901 0.7872 0.000999 ***\nASV13 -0.42711 -0.90420 0.4546 0.044955 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodesoil_funct_asv_signif_hits <- base::subset(envfit_soil_funct_asv$vectors$pvals, \n                                  c(envfit_soil_funct_asv$vectors$pvals \n                                  < 0.05 & envfit_soil_funct_asv$vectors$r > 0.5))\nsoil_funct_asv_signif_hits <- data.frame(soil_funct_asv_signif_hits)\nsoil_funct_asv_signif_hits <- rownames(soil_funct_asv_signif_hits)\nsoil_funct_asv_signif <- soil_funct_asv_scores[soil_funct_asv_scores$parameters %in% \n                                                 soil_funct_asv_signif_hits,]\n\n\n\n\n      parameters        CAP1       CAP2 variable_type\nASV4        ASV4 -0.10677580 -0.2996742           ASV\nASV12      ASV12  0.04088024  0.2662493           ASV\nASV2        ASV2  0.46539405 -0.4834130           ASV\n\n\n\nCodesoil_funct_md_signif_all$variable_type <- \"metadata\"\nsoil_funct_asv_signif$variable_type <- \"ASV\"\nsoil_funct_bioplot_data <- rbind(soil_funct_md_signif_all, soil_funct_asv_signif)\n\n\nThe last thing to do is categorize parameters scores and ASV scores into different variable types for plotting.\n\nCodesoil_funct_bioplot_data_md <- subset(soil_funct_bioplot_data, \n                                  soil_funct_bioplot_data$variable_type == \"metadata\")\nsoil_funct_bioplot_data_asv <- subset(soil_funct_bioplot_data, \n                                   soil_funct_bioplot_data$variable_type == \"ASV\")\n\n\n\nShow the code for the plot\n\nsoil_funct_cap_vals <- data.frame(soil_funct_cap$CCA$eig[1:2])\nsoil_funct_cap1 <- signif((soil_funct_cap_vals[1,] * 100), digits=3)\nsoil_funct_cap2 <- signif((soil_funct_cap_vals[2,] * 100), digits=3)\n\ncpa1_lab <- paste(\"CAP1\", \" (\", soil_funct_cap1, \"%)\", sep = \"\")\ncpa2_lab <- paste(\"CAP2\", \" (\", soil_funct_cap2, \"%)\", sep = \"\")\n\nswel_col <- c(\"#2271B2\", \"#71B222\", \"#B22271\")\n\nsoil_funct_plot <- ggplot(soil_funct_plot_data) +\n  geom_point(mapping = aes(x = CAP1, y = CAP2, shape = TREAT,\n                           colour = TREAT_T), size = 4) +\n  scale_colour_manual(values = swel_col) +\n#  geom_text(data = soil_funct_plot_data, aes(x = CAP1, y = CAP2, #UNCOMMENT to add sample labels\n#                              label = SampleID), size = 3) + \n  geom_segment(aes(x = 0, y = 0, xend = CAP1, yend = CAP2),\n               data = soil_funct_bioplot_data_md, linetype = \"solid\",\n               arrow = arrow(length = unit(0.3, \"cm\")), size = 0.4, \n               color = \"#191919\") +\n  geom_text(data = soil_funct_bioplot_data_md, \n            aes(x = CAP1, y = CAP2, label = parameters), size = 3,\n            nudge_x = 0.1, nudge_y = 0.05) +\n  ## USE this code to overlay ASV vestors\n  #geom_segment(aes(x = 0, y = 0, xend = CAP1, yend = CAP2),\n  #             data = soil_funct_bioplot_data_asv, linetype = \"solid\",\n  #             arrow = arrow(length = unit(0.3, \"cm\")), size = 0.2,\n  #             color = \"#676767\") +\n  #geom_text(data = soil_funct_bioplot_data_asv, \n  #          aes(x = CAP1, y = CAP2, label = parameters), size = 2.5,\n  #          nudge_x = 0.05, nudge_y = 0.05) +\n  theme_classic(base_size = 12) + \n  labs(title = \"Capscale Analysis\",\n       subtitle = \"Soil Functional Response\", \n       x = cpa1_lab, \n       y = cpa2_lab)\nsoil_funct_plot <- soil_funct_plot + coord_fixed() + theme(aspect.ratio=1)\nsoil_funct_plot\npng(\"files/metadata/figures/its18_soil_funct_capscale.png\",\n    height = 16, width = 20, units = 'cm', res = 600, bg = \"white\")\nsoil_funct_plot\ninvisible(dev.off())\npdf(\"files/metadata/figures/its18_soil_funct_capscale.pdf\",\n    height = 5, width = 6)\nsoil_funct_plot\ndev.off()\n\n\n\n\nTemperature Adaptation\n\nCodetmp_md <- its18_select_mc_norm_split_no_ac$temp_adapt\ntmp_md$TREAT_T <- as.character(tmp_md$TREAT_T)\ntmp_comm <- data.frame(t(its18_select_mc_norm_split_no_ac$data_loaded))\ntemp_adapt_rank <- rankindex(tmp_md[, 8:ncol(tmp_md)], tmp_comm, \n          indices = c(\"euc\", \"man\", \"gow\", \"bra\", \"kul\"), \n          stepacross = FALSE, method = \"spearman\")\n\n\n\n\n       euc        man        gow        bra        kul \n0.05132842 0.39465598 0.27457353 0.39465598 0.39465598 \n\n\nLet’s run capscale using Bray-Curtis. Note, we have 12 metadata parameters in this group but, for some reason, capscale only works with 13 parameters. This may have to do with degrees of freedom?\n\nStarting properties: AG_Q10, BG_Q10, BP_Q10, CE_Q10, P_Q10, N_Q10, S_Q10, XY_Q10, LP_Q10, PX_Q10, CUEcn, CUEcp, NUE, PUE, Tmin, SI\nAutocorrelated removed: NUE, PUE, P_Q10, SI\n\nRemove for capscale: S_Q10\n\n\n\n\n\ntemp_adapt_cap <- capscale(tmp_comm ~  AG_Q10 + BG_Q10 + BP_Q10 + CE_Q10 + \n                                       N_Q10 + XY_Q10 + LP_Q10 + PX_Q10 + \n                                       CUEcn + CUEcp + Tmin, \n                           tmp_md, dist = \"bray\")\n\n\n\nCall: capscale(formula = tmp_comm ~ AG_Q10 + BG_Q10 + BP_Q10 + CE_Q10 +\nN_Q10 + XY_Q10 + LP_Q10 + PX_Q10 + CUEcn + CUEcp + Tmin, data = tmp_md,\ndistance = \"bray\")\n\n              Inertia Proportion Rank\nTotal         3.88624    1.00000     \nConstrained   3.64416    0.93771   11\nUnconstrained 0.24208    0.06229    1\nInertia is squared Bray distance \nSpecies scores projected from 'tmp_comm' \n\nEigenvalues for constrained axes:\n  CAP1   CAP2   CAP3   CAP4   CAP5   CAP6   CAP7   CAP8   CAP9  CAP10  CAP11 \n0.9367 0.5976 0.4447 0.3685 0.3136 0.2644 0.2093 0.1488 0.1377 0.1235 0.0994 \n\nEigenvalues for unconstrained axes:\n   MDS1 \n0.24208 \n\n\nNow we can look at the variance against each principal component.\n\n\n\n\n\nAnd then make some quick and dirty plots. This will also come in handy later when we need to parse out data a better plot visualization. The ggplot function autoplot stores these data in a more accessible way than the raw results from capscale\n\nbase::plot(temp_adapt_cap) \n\n\n\ntmp_auto_plt <- autoplot(temp_adapt_cap, arrows = TRUE)\ntmp_auto_plt\n\n\n\n\n\n\n\nNext, we need to grab capscale scores for the samples and create a data frame of the first two dimensions. We will also need to add some of the sample details to the data frame. For this we use the vegan function scores which gets species or site scores from the ordination.\n\nCodetmp_samp_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"sites\")\ntmp_samp_scores[,1] <- NULL\ntmp_samp_scores <- tmp_samp_scores %>% dplyr::rename(SampleID = Label)\n\ntmp_md_sub <- tmp_md[, 1:4]\ntmp_md_sub <- tmp_md_sub %>% tibble::rownames_to_column(\"SampleID\")\ntemp_adapt_plot_data <- dplyr::left_join(tmp_md_sub, tmp_samp_scores, \n                                         by = \"SampleID\")\n\n\nNow we have a new data frame that contains sample details and capscale values.\n\n\n          SampleID PLOT TREAT TREAT_T PAIR        CAP1       CAP2\n1  P10_D00_010_C0E  P10     C       0    E -0.70909404  0.8884734\n2  P02_D00_010_C0A  P02     C       0    A -0.88932428  0.4678685\n3  P04_D00_010_C0B  P04     C       0    B -0.94720017 -0.5168746\n4  P06_D00_010_C0C  P06     C       0    C -0.94117253 -0.7283971\n5  P08_D00_010_C0D  P08     C       0    D -0.09697067  1.0013016\n6  P01_D00_010_W3A  P01     W       3    A -0.79138504  0.3434287\n7  P03_D00_010_W3B  P03     W       3    B  0.28447425 -0.8800769\n8  P07_D00_010_W3D  P07     W       3    D  0.18992920 -0.9828048\n9  P09_D00_010_W3E  P09     W       3    E  0.57704340 -0.4016378\n10 P01_D00_010_W8A  P01     W       8    A  0.98434765  0.1432237\n11 P03_D00_010_W8B  P03     W       8    B  0.88625427  0.6330378\n12 P05_D00_010_W8C  P05     W       8    C  0.84253827  0.9056297\n13 P07_D00_010_W8D  P07     W       8    D  0.61055970 -0.8731722\n\n\nWe can then do the same with the metadata vectors. Here though we only need the scores and parameter name.\n\nCodetemp_adapt_md_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"biplot\")\ntemp_adapt_md_scores[,1] <- NULL\ntemp_adapt_md_scores <- temp_adapt_md_scores %>% \n  dplyr::mutate(parameters = Label, .before = CAP1) %>% \n  tibble::column_to_rownames(\"Label\")\n\n\n\n\n       parameters         CAP1        CAP2\nAG_Q10     AG_Q10  0.138111424 -0.09067317\nBG_Q10     BG_Q10  0.529871661  0.44030269\nBP_Q10     BP_Q10  0.213674949  0.18420563\nCE_Q10     CE_Q10 -0.470374849 -0.07935444\nN_Q10       N_Q10  0.002856862  0.28186886\nXY_Q10     XY_Q10  0.826073837  0.18643092\nLP_Q10     LP_Q10 -0.428296135 -0.07664534\nPX_Q10     PX_Q10 -0.361920412  0.05606159\nCUEcn       CUEcn -0.146194994 -0.40205479\nCUEcp       CUEcp  0.318510258  0.66752667\nTmin         Tmin  0.740618311 -0.05737920\n\n\nLet’s run some quick correlations of metadata with ordination axes to see which parameters are significant. For this we use the vegan function envfit.\n\nCodetmp_samp_scores_sub <- temp_adapt_plot_data[, 6:7]\ntmp_samp_scores_sub <- as.matrix(tmp_samp_scores_sub)\ntmp_param_list <- temp_adapt_md_scores$parameters\n\ntmp_md_sub <- subset(tmp_md,  select =  tmp_param_list)\n\nenvfit_temp_adapt_md <- envfit(tmp_samp_scores_sub, tmp_md_sub,\n                 perm = 1000, choices = c(1, 2))\n\n\n\n\n\n***VECTORS\n\n           CAP1     CAP2     r2  Pr(>r)   \nAG_Q10  0.82904 -0.55918 0.0237 0.89710   \nBG_Q10  0.76182  0.64778 0.4109 0.07792 . \nBP_Q10  0.74983  0.66163 0.0689 0.71528   \nCE_Q10 -0.98593 -0.16714 0.1956 0.34865   \nN_Q10   0.00657  0.99998 0.0700 0.71029   \nXY_Q10  0.97498  0.22227 0.6166 0.00999 **\nLP_Q10 -0.98418 -0.17719 0.1627 0.44955   \nPX_Q10 -0.98713  0.15993 0.1155 0.54246   \nCUEcn  -0.33206 -0.94326 0.1605 0.40460   \nCUEcp   0.42027  0.90740 0.4786 0.03497 * \nTmin    0.99659 -0.08248 0.4749 0.02797 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodetemp_adapt_md_signif_hits <- base::subset(envfit_temp_adapt_md$vectors$pvals, \n                                  c(envfit_temp_adapt_md$vectors$pvals \n                                  < 0.05 & envfit_temp_adapt_md$vectors$r > 0.4))\ntemp_adapt_md_signif_hits <- data.frame(temp_adapt_md_signif_hits)\ntemp_adapt_md_signif_hits <- rownames(temp_adapt_md_signif_hits)\ntemp_adapt_md_signif <- temp_adapt_md_scores[temp_adapt_md_scores$parameters %in% \n                                               temp_adapt_md_signif_hits,]\n\n\nenvfit found that XY_Q10, CUEcp, Tmin were significantly correlated.\nNow let’s see if the same parameters are significant for the envfit and bioenv analyses.\n\n\n[1] \"Significant parameters from bioenv analysis.\"\n[1] \"XY_Q10\" \"Tmin\"  \n_____________________________________\n[1] \"Significant parameters from envfit analysis.\"\n[1] \"XY_Q10\" \"CUEcp\"  \"Tmin\"  \n_____________________________________\n[1] \"Found in bioenv but not envfit.\"\ncharacter(0)\n_____________________________________\n[1] \"Found in envfit but not bioenv.\"\n[1] \"CUEcp\"\n_____________________________________\n[1] \"Found in envfit and  bioenv.\"\n[1] \"XY_Q10\" \"CUEcp\"  \"Tmin\"  \n\n\n\nCode#new_temp_adapt_md_signif_hits <- base::append(temp_adapt_md_signif_hits, temp_adapt_sig_diff)\nnew_temp_adapt_md_signif_hits <- temp_adapt_sig_diff[1:4]\ntemp_adapt_md_signif_all <- temp_adapt_md_scores[temp_adapt_md_scores$parameters %in% \n                                                   new_temp_adapt_md_signif_hits,]\n\n\nCheck. Next, we run envfit for the ASVs.\n\nCodeenvfit_temp_adapt_asv <- envfit(tmp_samp_scores_sub, \n                                tmp_comm[, order(colSums(-tmp_comm))][, 1:10],\n                                perm = 1000, choices = c(1, 2))\n\ntemp_adapt_asv_scores <- dplyr::filter(tmp_auto_plt$plot_env$obj, Score == \"species\")\n\ntemp_adapt_asv_scores <- temp_adapt_asv_scores %>% \n                         dplyr::mutate(parameters = Label, .before = CAP1) %>%\n                         tibble::column_to_rownames(\"Label\")\ntemp_adapt_asv_scores[,1] <- NULL\n\n\n\n\n\n***VECTORS\n\n          CAP1     CAP2     r2   Pr(>r)    \nASV1  -0.68969  0.72410 0.2975 0.051948 .  \nASV2   0.68393  0.72955 0.5592 0.003996 ** \nASV4  -0.34226  0.93961 0.4935 0.005994 ** \nASV3  -0.89727 -0.44148 0.5062 0.000999 ***\nASV15  0.96528 -0.26123 0.2497 0.122877    \nASV8  -0.90985  0.41495 0.1146 0.923077    \nASV9   0.76209 -0.64748 0.1338 0.474525    \nASV24  0.44129 -0.89736 0.2375 0.095904 .  \nASV12  0.14568 -0.98933 0.7737 0.002997 ** \nASV13 -0.41856  0.90819 0.4357 0.056943 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\nCodetemp_adapt_asv_signif_hits <- base::subset(envfit_temp_adapt_asv$vectors$pvals, \n                                  c(envfit_temp_adapt_asv$vectors$pvals \n                                  < 0.05 & envfit_temp_adapt_asv$vectors$r > 0.5))\ntemp_adapt_asv_signif_hits <- data.frame(temp_adapt_asv_signif_hits)\ntemp_adapt_asv_signif_hits <- rownames(temp_adapt_asv_signif_hits)\ntemp_adapt_asv_signif <- temp_adapt_asv_scores[temp_adapt_asv_scores$parameters %in% \n                                                 temp_adapt_asv_signif_hits,]\n\n\n\n\n      parameters        CAP1       CAP2 variable_type\nASV12      ASV12  0.03533894 -0.2327656           ASV\nASV2        ASV2  0.38413654  0.3977765           ASV\nASV3        ASV3 -0.45977392 -0.1843674           ASV\n\n\n\nCodetemp_adapt_md_signif_all$variable_type <- \"metadata\"\ntemp_adapt_asv_signif$variable_type <- \"ASV\"\ntemp_adapt_bioplot_data <- rbind(temp_adapt_md_signif_all, temp_adapt_asv_signif)\n\n\nThe last thing to do is categorize parameters scores and ASV scores into different variable types for plotting.\n\nCodetemp_adapt_bioplot_data_md <- subset(temp_adapt_bioplot_data, \n                                  temp_adapt_bioplot_data$variable_type == \"metadata\")\ntemp_adapt_bioplot_data_asv <- subset(temp_adapt_bioplot_data, \n                                   temp_adapt_bioplot_data$variable_type == \"ASV\")\n\n\n\nShow the code for the plot\n\ntemp_adapt_cap_vals <- data.frame(temp_adapt_cap$CCA$eig[1:2])\ntemp_adapt_cap1 <- signif((temp_adapt_cap_vals[1,] * 100), digits=3)\ntemp_adapt_cap2 <- signif((temp_adapt_cap_vals[2,] * 100), digits=3)\n\ncpa1_lab <- paste(\"CAP1\", \" (\", temp_adapt_cap1, \"%)\", sep = \"\")\ncpa2_lab <- paste(\"CAP2\", \" (\", temp_adapt_cap2, \"%)\", sep = \"\")\n\nswel_col <- c(\"#2271B2\", \"#71B222\", \"#B22271\")\n\ntemp_adapt_plot <- ggplot(temp_adapt_plot_data) +\n  geom_point(mapping = aes(x = CAP1, y = CAP2, shape = TREAT,\n                           colour = TREAT_T), size = 4) +\n  scale_colour_manual(values = swel_col) +\n#  geom_text(data = temp_adapt_plot_data, aes(x = CAP1, y = CAP2, #UNCOMMENT to add sample labels\n#                              label = SampleID), size = 3) + \n  geom_segment(aes(x = 0, y = 0, xend = CAP1, yend = CAP2),\n               data = temp_adapt_bioplot_data_md, linetype = \"solid\",\n               arrow = arrow(length = unit(0.3, \"cm\")), size = 0.4,\n               color = \"#191919\", inherit.aes = FALSE) +\n  geom_text(data = temp_adapt_bioplot_data_md, \n            aes(x = CAP1, y = CAP2, label = parameters), size = 3,\n            nudge_x = 0.1, nudge_y = 0.05) +\n  # uSe to include ASV vectors\n  #geom_segment(aes(x = 0, y = 0, xend = CAP1, yend = CAP2),\n  #             data = temp_adapt_bioplot_data_asv, linetype = \"solid\",\n  #             arrow = arrow(length = unit(0.3, \"cm\")), size = 0.2,\n  #             color = \"#676767\") +\n  #geom_text(data = temp_adapt_bioplot_data_asv, \n  #          aes(x = CAP1, y = CAP2, label = parameters), size = 2.5,\n  #          nudge_x = 0.05, nudge_y = 0.05) +\n  theme_classic(base_size = 12) + \n  labs(title = \"Capscale Analysis\",\n       subtitle = \"Temperature Adaptation\",\n       x = cpa1_lab, \n       y = cpa2_lab)\ntemp_adapt_plot <- temp_adapt_plot + coord_fixed() + theme(aspect.ratio=1)\ntemp_adapt_plot\npng(\"files/metadata/figures/its18_temp_adapt_capscale.png\",\n    height = 16, width = 20, units = 'cm', res = 600, bg = \"white\")\ntemp_adapt_plot\ninvisible(dev.off())\npdf(\"files/metadata/figures/its18_temp_adapt_capscale.pdf\",\n    height = 5, width = 6)\ntemp_adapt_plot\ndev.off()"
  },
  {
    "objectID": "metadata.html#capscale-plots-1",
    "href": "metadata.html#capscale-plots-1",
    "title": "8. Metadata Analysis",
    "section": "Capscale Plots",
    "text": "Capscale Plots\n\n\n\nEdaphic properties\nSoil Functional Response\nTemperature Adaptation\n\n\n\n\n\n\n\n\n(ITS) Figure 7 | Distance-based Redundancy Analysis (db-RDA) of PIME filtered data based on Bray-Curtis dissimilarity showing the relationships between community composition change versus edaphic properties.\n\n\n\n\n\n\n\n(ITS) Figure 8 | Distance-based Redundancy Analysis (db-RDA) of PIME filtered data based on Bray-Curtis dissimilarity showing the relationships between community composition change versus microbial functional response.\n\n\n\n\n\n\n\n(ITS) Figure 9 | Distance-based Redundancy Analysis (db-RDA) of PIME filtered data based on Bray-Curtis dissimilarity showing the relationships between community composition change versus temperature adaptation."
  },
  {
    "objectID": "metadata.html#last-updated-on",
    "href": "metadata.html#last-updated-on",
    "title": "8. Metadata Analysis",
    "section": "Last updated on",
    "text": "Last updated on\n\n\n[1] \"2022-06-21 13:23:18 EST\""
  },
  {
    "objectID": "taxa.html",
    "href": "taxa.html",
    "title": "4. Taxonomic Diversity",
    "section": "",
    "text": "Click here for setup information.\n\nknitr::opts_chunk$set(echo = TRUE, eval = FALSE)\nset.seed(119)\n#library(conflicted)\n#pacman::p_depends(phangorn, local = TRUE)  \n#pacman::p_depends_reverse(phangorn, local = TRUE)  \n\nlibrary(phyloseq); packageVersion(\"phyloseq\")\nlibrary(Biostrings); packageVersion(\"Biostrings\")\npacman::p_load(tidyverse, patchwork,\n               agricolae, labdsv, naniar, \n               microbiome, microeco, file2meco, cowplot,\n               GUniFrac, ggalluvial, ggdendro, igraph, reactable,\n               pheatmap, microbiomeMarker,\n               captioner, downloadthis,\n               install = FALSE, update = FALSE)\noptions(scipen=999)\nknitr::opts_current$get(c(\n  \"cache\",\n  \"cache.path\",\n  \"cache.rebuild\",\n  \"dependson\",\n  \"autodep\"\n))"
  },
  {
    "objectID": "taxa.html#workflow-input",
    "href": "taxa.html#workflow-input",
    "title": "4. Taxonomic Diversity",
    "section": "Workflow Input",
    "text": "Workflow Input\nAll files needed to run this workflow can be downloaded from figshare.\n\n\nIn this workflow, we use the microeco to look at the taxonomic distribution of microbial communities."
  },
  {
    "objectID": "taxa.html#s-rrna",
    "href": "taxa.html#s-rrna",
    "title": "4. Taxonomic Diversity",
    "section": "16s rRNA",
    "text": "16s rRNA\n\n\n\n\n\n\nHere we compare the taxonomic breakdown of the Full (unfiltered), Arbitrary filtered, PERfect filtered, and PIME filtered data sets, split by temperature treatment.\nBefore we plot the data, we want to separate out the Proteobacteria classes so we can plot these along with other phyla. To accomplish this we perform the following steps:\n\nGet all Class-level Proteobacteria names\n\n\nCodessu18_data_sets <- c(\"ssu18_ps_work\",  \"ssu18_ps_filt\",\n                     \"ssu18_ps_perfect\", \"ssu18_ps_pime\")\n\nfor (i in ssu18_data_sets) {\n     tmp_name <- purrr::map_chr(i, ~paste0(., \"_proteo\"))\n     tmp_get <- get(i)\n     tmp_df <- subset_taxa(tmp_get, Phylum == \"Proteobacteria\")\n     assign(tmp_name, tmp_df)\n     print(tmp_name)\n     tmp_get_taxa <- get_taxa_unique(tmp_df,\n                                     taxonomic.rank = rank_names(tmp_df)[3],\n                                     errorIfNULL=TRUE)\n     print(tmp_get_taxa)\n     rm(list = ls(pattern = \"tmp_\"))\n     rm(list = ls(pattern = \"_proteo\"))\n}\n\n\n\nReplace Phylum Proteobacteria with the Class name.\n\n\nCodefor (j in ssu18_data_sets) {\n  tmp_name <- purrr::map_chr(j, ~paste0(., \"_proteo_clean\"))\n  tmp_get <- get(j)\n  tmp_clean <- data.frame(tax_table(tmp_get))\n\n   for (i in 1:nrow(tmp_clean)){\n       if (tmp_clean[i,2] == \"Proteobacteria\" & tmp_clean[i,3] == \"Alphaproteobacteria\"){\n           phylum <- base::paste(\"Alphaproteobacteria\")\n           tmp_clean[i, 2] <- phylum\n   }   else if (tmp_clean[i,2] == \"Proteobacteria\" & tmp_clean[i,3] == \"Gammaproteobacteria\"){\n           phylum <- base::paste(\"Gammaproteobacteria\")\n           tmp_clean[i, 2] <- phylum\n   }   else if (tmp_clean[i,2] == \"Proteobacteria\" & tmp_clean[i,3] == \"Zetaproteobacteria\"){\n              phylum <- base::paste(\"Zetaproteobacteria\")\n           tmp_clean[i, 2] <- phylum\n   }   else if (tmp_clean[i,2] == \"Proteobacteria\" & tmp_clean[i,3] == \"p_Proteobacteria\"){\n           phylum <- base::paste(\"p_Proteobacteria\")\n           tmp_clean[i, 2] <- phylum\n       }\n     }\n  tax_table(tmp_get) <- as.matrix(tmp_clean)\n  rank_names(tmp_get)\n  assign(tmp_name, tmp_get)\n  print(c(tmp_name, tmp_get))\n  print(length(get_taxa_unique(tmp_get,\n                               taxonomic.rank = rank_names(tmp_get)[2],\n                               errorIfNULL=TRUE)))\n  tmp_path <- file.path(\"files/taxa/rdata/\")\n  saveRDS(tmp_get, paste(tmp_path, j, \"_clean.rds\", sep = \"\"))\n  rm(list = ls(pattern = \"tmp_\"))\n}\nrm(class, order, phylum)\nobjects(pattern=\"_proteo_clean\")\n\n\n\nIn order to use microeco, we need to add the rank designation as a prefix to each taxa. For example, Actinobacteriota is changed to p__Actinobacteriota.\n\n\nCodefor (i in ssu18_data_sets) {\n  tmp_get <- get(purrr::map_chr(i, ~paste0(., \"_proteo_clean\")))\n  #tmp_get <- get(i)\n  #tmp_path <- file.path(\"files/alpha/rdata/\")\n  #tmp_read <- readRDS(paste(tmp_path, i, \".rds\", sep = \"\"))\n  tmp_sam_data <- sample_data(tmp_get)\n  tmp_tax_data <- data.frame(tax_table(tmp_get))\n  tmp_tax_data$Phylum <- gsub(\"p_Proteobacteria\", \"Proteobacteria\", tmp_tax_data$Phylum)\n\n  #tmp_tax_data[,c(1:6)]\n  tmp_tax_data$ASV_ID <- NULL # Some have, some do not\n  tmp_tax_data$ASV_SEQ <- NULL\n  \n  tmp_tax_data[] <- data.frame(lapply(tmp_tax_data, gsub, \n                                      pattern = \"^[k | p | c | o | f]_.*\", \n                                      replacement = \"\", fixed = FALSE))\n  tmp_tax_data$Kingdom <- paste(\"k__\", tmp_tax_data$Kingdom, sep = \"\")\n  tmp_tax_data$Phylum <- paste(\"p__\", tmp_tax_data$Phylum, sep = \"\")\n  tmp_tax_data$Class <- paste(\"c__\", tmp_tax_data$Class, sep = \"\")\n  tmp_tax_data$Order <- paste(\"o__\", tmp_tax_data$Order, sep = \"\")\n  tmp_tax_data$Family <- paste(\"f__\", tmp_tax_data$Family, sep = \"\")\n  tmp_tax_data$Genus <- paste(\"g__\", tmp_tax_data$Genus, sep = \"\")\n  tmp_tax_data <- as.matrix(tmp_tax_data)\n  \n  tmp_ps <- phyloseq(otu_table(tmp_get),\n                     phy_tree(tmp_get),\n                     tax_table(tmp_tax_data),\n                     tmp_sam_data)\n  assign(i, tmp_ps)\n  rm(list = ls(pattern = \"tmp_\"))\n}\nrm(list = ls(pattern = \"_proteo_clean\"))\n\n\n\nNext, we need to covert each phyloseq object to a microtable class. The microtable class is the basic data structure for the microeco package and designed to store basic information from all the downstream analyses (e.g, alpha diversity, beta diversity, etc.). We use the file2meco to read the phyloseq object and convert into a microtable object. We can add _me as a suffix to each object to distiguish it from its’ phyloseq counterpart.\n\n\nCodefor (i in ssu18_data_sets) {\n    tmp_get <- get(i)\n    tmp_otu_table <- data.frame(t(otu_table(tmp_get)))\n    tmp_sample_info <- data.frame(sample_data(tmp_get))\n    tmp_taxonomy_table <- data.frame(tax_table(tmp_get))\n    tmp_phylo_tree <- phy_tree(tmp_get)\n    tmp_taxonomy_table %<>% tidy_taxonomy\n    tmp_dataset <- microtable$new(sample_table = tmp_sample_info, \n                                  otu_table = tmp_otu_table, \n                                  tax_table = tmp_taxonomy_table, \n                                  phylo_tree = tmp_phylo_tree)\n    tmp_dataset$tidy_dataset()\n    print(tmp_dataset)\n    tmp_dataset$tax_table %<>% base::subset(Kingdom == \"k__Archaea\" | Kingdom == \"k__Bacteria\")\n    print(tmp_dataset)\n    tmp_dataset$filter_pollution(taxa = c(\"mitochondria\", \"chloroplast\"))\n    print(tmp_dataset)\n    tmp_dataset$tidy_dataset()\n    print(tmp_dataset)\n    tmp_name <- purrr::map_chr(i, ~paste0(., \"_me\")) \n    assign(tmp_name, tmp_dataset)\n    rm(list = ls(pattern = \"tmp_\"))\n}  \nobjects()\n\n\nHere is an example of what a new microeco object looks like when you call it.\n\nssu18_ps_work_me\n\nmicrotable class:\nsample_table have 15 rows and 6 columns\notu_table have 20173 rows and 15 columns\ntax_table have 20173 rows and 6 columns\nphylo_tree have 20173 tips\nTaxa abundance: calculated for Kingdom,Phylum,Class,Order,Family,Genus \n\n\n\n\n\n\nNow, we calculate the taxa abundance at each taxonomic rank using cal_abund(). This function return a list called taxa_abund containing several data frame of the abundance information at each taxonomic rank. The list is stored in the microtable object automatically. It’s worth noting that the cal_abund() function can be used to solve some complex cases, such as supporting both the relative and absolute abundance calculation and selecting the partial taxonomic columns. More information can be found in the description of the file2meco package. In the same loop we also create a trans_abund class, which is used to transform taxonomic abundance data for plotting.\n\n\nCodefor (i in ssu18_data_sets) {\n  tmp_me <- get(purrr::map_chr(i, ~paste0(., \"_me\")))\n  tmp_me$cal_abund()\n  tmp_me_abund <- trans_abund$new(dataset = tmp_me, taxrank = \"Phylum\", ntaxa = 12)\n  tmp_me_abund$abund_data$Abundance <- tmp_me_abund$abund_data$Abundance / 100\n  tmp_me_abund_gr <- trans_abund$new(dataset = tmp_me, taxrank = \"Phylum\", ntaxa = 12, \n                                  groupmean = \"TEMP\")\n  tmp_me_abund_gr$abund_data$Abundance <- tmp_me_abund_gr$abund_data$Abundance / 100\n  tmp_name <- purrr::map_chr(i, ~paste0(., \"_me_abund\")) \n  assign(tmp_name, tmp_me_abund)  \n  tmp_name_gr <- purrr::map_chr(i, ~paste0(., \"_me_abund_group\")) \n  assign(tmp_name_gr, tmp_me_abund_gr)  \n  rm(list = ls(pattern = \"tmp_\"))\n}\nobjects()\n\n\nThe result is stored in object$taxa_abund ...\n\nI prefer to specify the order of taxa in these kinds of plots. We can look the top ntaxa (defined above) by accessing the use_taxanames character vector of each microtable object.\n\n\n\n\nHum. Similar but not the same. This means we need to define the order for each separately. Once we do that, we will override the use_taxanames character vectors with the reordered vectors.\n\nCodessu18_ps_work_tax_ord <- c(\"Alphaproteobacteria\", \"Gammaproteobacteria\", \"Acidobacteriota\", \n                           \"Actinobacteriota\", \"Bacteroidota\", \"Firmicutes\", \"Myxococcota\", \n                           \"Verrucomicrobiota\", \"Chloroflexi\", \"Planctomycetota\", \n                           \"Methylomirabilota\", \"Crenarchaeota\")\nssu18_ps_filt_tax_ord <- c(\"Alphaproteobacteria\", \"Gammaproteobacteria\", \"Acidobacteriota\", \n                           \"Actinobacteriota\", \"Bacteroidota\", \"Firmicutes\", \"Myxococcota\", \n                           \"Verrucomicrobiota\", \"RCP2-54\", \"Planctomycetota\", \n                           \"Methylomirabilota\", \"Crenarchaeota\")\nssu18_ps_perfect_tax_ord <- c(\"Alphaproteobacteria\", \"Gammaproteobacteria\", \"Acidobacteriota\", \n                              \"Actinobacteriota\", \"Bacteroidota\", \"Firmicutes\", \"Myxococcota\", \n                              \"Verrucomicrobiota\", \"RCP2-54\", \"Planctomycetota\", \n                              \"Methylomirabilota\", \"Crenarchaeota\")\nssu18_ps_pime_tax_ord <- c(\"Alphaproteobacteria\", \"Gammaproteobacteria\", \"Acidobacteriota\", \n                           \"Actinobacteriota\", \"Bacteroidota\", \"Firmicutes\", \"Myxococcota\", \n                           \"Verrucomicrobiota\", \"RCP2-54\", \"Nitrospirota\", \n                           \"Methylomirabilota\", \"Crenarchaeota\")\n\n\n\nAnd one more little step before plotting. Here we a) specify a custom color palette and b) specify the sample order.\n\n\nCodetop_level <- \"Phylum\"\nssu18_colvec.tax <- c(\"#00463C\",\"#FFD5FD\",\"#00A51C\",\"#C80B2A\",\"#00C7F9\",\"#FFA035\",\n                      \"#ED0DFD\",\"#0063E5\",\"#5FFFDE\",\"#C00B6F\",\"#00A090\",\"#FF95BA\")\nssu18_samp_order <- c(\"P02_D00_010_C0A\", \"P04_D00_010_C0B\", \"P06_D00_010_C0C\", \"P08_D00_010_C0D\", \"P10_D00_010_C0E\", \n                    \"P01_D00_010_W3A\", \"P03_D00_010_W3B\", \"P05_D00_010_W3C\", \"P07_D00_010_W3D\", \"P09_D00_010_W3E\", \n                    \"P01_D00_010_W8A\", \"P03_D00_010_W8B\", \"P05_D00_010_W8C\", \"P07_D00_010_W8D\", \"P09_D00_010_W8E\")\n\n\n\nNow we can generate plots (in a loop) for each faceted data set.\n\n\nCodefor (i in ssu18_data_sets) {\n  tmp_abund <- get(purrr::map_chr(i, ~paste0(., \"_me_abund\")))\n  tmp_tax_order <- get(purrr::map_chr(i, ~paste0(., \"_tax_ord\")))\n  tmp_abund$use_taxanames <- tmp_tax_order\n  tmp_facet_plot <- tmp_abund$plot_bar(use_colors = ssu18_colvec.tax, \n                                       others_color = \"#323232\", \n                                       facet = \"TEMP\", \n                                       xtext_keep = TRUE, \n                                       xtext_type_hor = FALSE, \n                                       legend_text_italic = FALSE, \n                                       xtext_size = 6, \n                                       facet_color = \"#cccccc\", \n                                       order_x = ssu18_samp_order)\n  tmp_facet_name <- purrr::map_chr(i, ~paste0(., \"_facet_plot\")) \n  assign(tmp_facet_name, tmp_facet_plot)  \n  rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nThen add a little formatting to the faceted plots.\n\nCodeset_to_plot <- c(\"ssu18_ps_work_facet_plot\", \"ssu18_ps_filt_facet_plot\", \"ssu18_ps_perfect_facet_plot\", \"ssu18_ps_pime_facet_plot\")\nfor (i in  set_to_plot) {\n    tmp_get <- get(i)\n    tmp_get <- tmp_get +\n    theme_cowplot() +\n    guides(fill = guide_legend(title = top_level, reverse = FALSE,\n                               keywidth = 0.7, keyheight = 0.7)) +\n    ylab(NULL) + xlab(\"Sample\") +\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_rect(fill = \"transparent\", colour = NA),\n        plot.background = element_rect(fill = \"transparent\", colour = NA),\n        panel.border = element_rect(fill = NA, color = \"black\"),\n        legend.text = element_text(size = 7),\n        legend.title = element_text(size = 10),\n        legend.position = \"right\",\n        axis.text.y = element_text(size = 8),\n        axis.text.x = element_text(size = 6, angle = 90),\n        strip.text = element_text(size = 8, angle = 0),\n        axis.title = element_text(size = 10)) + ylab(NULL) +\n      scale_y_continuous()\n  assign(i, tmp_get)\n  rm(list = ls(pattern = \"tmp_\")) \n}\n\n\nAnd now plots for the groupmeans sets. We can use the same taxa order since that should not have changed.\n\nCodeset_to_plot <- c(\"ssu18_ps_work_group_plot\", \"ssu18_ps_filt_group_plot\", \"ssu18_ps_perfect_group_plot\", \"ssu18_ps_pime_group_plot\")\nfor (i in ssu18_data_sets) {\n  tmp_abund <- get(purrr::map_chr(i, ~paste0(., \"_me_abund_group\")))\n  tmp_tax_order <- get(purrr::map_chr(i, ~paste0(., \"_tax_ord\")))\n  tmp_abund$use_taxanames <- tmp_tax_order\n  tmp_group_plot <- tmp_abund$plot_bar(use_colors = ssu18_colvec.tax, \n                                       others_color = \"#323232\", \n                                       xtext_keep = TRUE, \n                                       xtext_type_hor = TRUE, \n                                       legend_text_italic = FALSE, \n                                       xtext_size = 10, \n                                       facet_color = \"#cccccc\")\n  tmp_group_name <- purrr::map_chr(i, ~paste0(., \"_group_plot\")) \n  assign(tmp_group_name, tmp_group_plot)  \n  rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nLet’s also add a little formatting to the groupmean plots.\n\nCodefor (i in  set_to_plot) {\n     tmp_get <- get(i)\n     tmp_get <- tmp_get +\n       theme_cowplot() +\n       ylab(\"Relative Abundance (% total reads)\") + xlab(\"Temperature\") +\n       theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n             panel.background = element_rect(fill = \"transparent\", colour = NA),\n             plot.background = element_rect(fill = \"transparent\", colour = NA),\n             panel.border = element_rect(fill = NA, color = \"black\"),\n             legend.position = \"none\",\n             axis.text = element_text(size = 8),\n             axis.title = element_text(size = 10)) + \n       scale_y_continuous()\n\n     assign(i, tmp_get)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\nFinally we use the patchwork package to combine the two plots and customize the look.\n\n\nCode## single index that acts as an index for referencing elements (variables) in a list\n##  solution modified from this SO answerhttps://stackoverflow.com/a/54451460\nvar_list <- list(\n  var1 = ssu18_data_sets,\n  var2 = c(\"FULL\", \"Arbitrary filtered\", \"PERfect filtered\", \"PIME filtered\")\n)\nfor (j in  1:length(var_list$var1)) {\n     tmp_plot_final_name <- purrr::map_chr(var_list$var1[j], ~paste0(., \"_\", top_level, \"_plot_final\"))\n     tmp_set_type <- var_list$var2[j]\n     tmp_p_plot <- get(purrr::map_chr(var_list$var1[j], ~paste0(., \"_group_plot\")))\n     tmp_m_plot <- get(purrr::map_chr(var_list$var1[j], ~paste0(., \"_facet_plot\")))\n     tmp_plot_final <- tmp_p_plot + tmp_m_plot\n     tmp_plot_final <- tmp_plot_final +\n         plot_annotation(tag_levels = \"a\", \n                         title = paste(\"Top\", top_level, \"of\", tmp_set_type, \"data set\"), \n                         #subtitle = 'Top taxa of non-filtered data',\n                         caption = \"a) grouped by temperature.,\n                         b) All samples, faceted by temperature.\") + \n       plot_layout(widths = c(1, 2)) &\n       theme(plot.title = element_text(size = 13),\n             plot.subtitle = element_text(size = 10), \n             plot.tag = element_text(size = 12), \n             axis.title = element_text(size = 10), \n             axis.text = element_text(size = 8)) \n  assign(tmp_plot_final_name, tmp_plot_final)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\n\n\n\n\n\n\n\nIt is important to note that while most of the dominant taxa are the same across the FULL, Arbitrary, PIME, and PERfect data sets, there a few key difference:\n\n\nPlanctomycetota was not among the dominant taxa in the PIME data set. Nitrospirota was substituted.\nChloroflexi was not among the dominant taxa in either the Arbitrary, PERfect, or PIME data sets. RCP2-54 was substituted in both cases.\n\n\n\n\nFULL (unfiltered)\nArbitrary filtered\nPERFECT filtered\nPIME filtered\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 1 | Dominant taxonomic groups from the FULL (unfiltered) 16S rRNA data set. A) Grouped by temperature and B) all samples faceted by temperature treatment.\n\n\n\n\n\n\n\n(16S rRNA) Figure 2 | Dominant taxonomic groups from the Arbitrary filtered 16S rRNA data set. A) Grouped by temperature and B) all samples faceted by temperature treatment.\n\n\n\n\n\n\n\n(16S rRNA) Figure 3 | Dominant taxonomic groups from the PERfect filtered 16S rRNA data set. A) Grouped by temperature and B) all samples faceted by temperature treatment.\n\n\n\n\n\n\n\n(16S rRNA) Figure 4 | Dominant taxonomic groups from the PIME filtered 16S rRNA data set. A) Grouped by temperature and B) all samples faceted by temperature treatment."
  },
  {
    "objectID": "taxa.html#its",
    "href": "taxa.html#its",
    "title": "4. Taxonomic Diversity",
    "section": "ITS",
    "text": "ITS\n\n\n\n\n\n\nHere we compare the taxonomic breakdown of the FULL (unfiltered), PIME filtered, and PERfect filtered data sets, split by temperature treatment for the ITS data. The workflow is basically the same as the 16S rRNA data. Click the box below to see the entire workflow.\n\n\n\nDetailed workflow for ITS taxonomic analysis.\n\nChoose the number of taxa to display and the taxonomic level. Aggregate the rest into “Other”.\n\n\ntop_hits <- 14\ntop_level <- \"Order\"\n\nAs above, in order to use microeco, we need to add the rank designation as a prefix to each taxa. For example, Basidiomycota is changed to p__Basidiomycota.\n\nfor (i in its18_data_sets) {\n  tmp_get <- get(i)\n  tmp_sam_data <- sample_data(tmp_get)\n  tmp_tax_data <- data.frame(tax_table(tmp_get))\n  tmp_tax_data$ASV_ID <- NULL # Some have, some do not\n  tmp_tax_data$ASV_SEQ <- NULL\n  \n#  tmp_tax_data[] <- data.frame(lapply(tmp_tax_data, gsub, \n#                                      pattern = \"^[k | p | c | o | f]_.*\", \n#                                      replacement = \"\", fixed = FALSE))\n  tmp_tax_data$Kingdom <- paste(\"k__\", tmp_tax_data$Kingdom, sep = \"\")\n  tmp_tax_data$Phylum <- paste(\"p__\", tmp_tax_data$Phylum, sep = \"\")\n  tmp_tax_data$Class <- paste(\"c__\", tmp_tax_data$Class, sep = \"\")\n  tmp_tax_data$Order <- paste(\"o__\", tmp_tax_data$Order, sep = \"\")\n  tmp_tax_data$Family <- paste(\"f__\", tmp_tax_data$Family, sep = \"\")\n  tmp_tax_data$Genus <- paste(\"g__\", tmp_tax_data$Genus, sep = \"\")\n  tmp_tax_data <- as.matrix(tmp_tax_data)\n  \n  tmp_ps <- phyloseq(otu_table(tmp_get),\n                     tax_table(tmp_tax_data),\n                     tmp_sam_data)\n  assign(i, tmp_ps)\n  rm(list = ls(pattern = \"tmp_\"))\n}\ndata.frame(tax_table(its18_ps_pime))\n\n\nfor (i in its18_data_sets) {\n    tmp_get <- get(i)\n    tmp_otu_table <- data.frame(t(otu_table(tmp_get)))\n    tmp_sample_info <- data.frame(sample_data(tmp_get))\n    tmp_taxonomy_table <- data.frame(tax_table(tmp_get))\n    #tmp_taxonomy_table %<>% tidy_taxonomy\n    tmp_dataset <- microtable$new(sample_table = tmp_sample_info, \n                                  otu_table = tmp_otu_table, \n                                  tax_table = tmp_taxonomy_table)\n    tmp_dataset$tidy_dataset()\n    print(tmp_dataset)\n    tmp_dataset$tax_table %<>% base::subset(Kingdom == \"k__Fungi\")\n    print(tmp_dataset)\n    tmp_dataset$tidy_dataset()\n    print(tmp_dataset)\n    tmp_name <- purrr::map_chr(i, ~paste0(., \"_me\")) \n    assign(tmp_name, tmp_dataset)\n    rm(list = ls(pattern = \"tmp_\"))\n}  \nobjects()\n\nHere is an example of what a new microeco object looks like when you call it.\n\nits18_ps_work_me\n\nmicrotable class:\nsample_table have 13 rows and 6 columns\notu_table have 3355 rows and 13 columns\ntax_table have 3355 rows and 6 columns\nTaxa abundance: calculated for Kingdom,Phylum,Class,Order,Family,Genus \n\n\n\n\n\n\nNow, we calculate the taxa abundance at each taxonomic rank using cal_abund(). This function return a list called taxa_abund containing several data frame of the abundance information at each taxonomic rank. The list is stored in the microtable object automatically. It’s worth noting that the cal_abund() function can be used to solve some complex cases, such as supporting both the relative and absolute abundance calculation and selecting the partial taxonomic columns. More information can be found in the description of the file2meco package. In the same loop we also create a trans_abund class, which is used to transform taxonomic abundance data for plotting.\n\n\nfor (i in its18_data_sets) {\n  tmp_me <- get(purrr::map_chr(i, ~paste0(., \"_me\")))\n  tmp_me$cal_abund()\n  tmp_me_abund <- trans_abund$new(dataset = tmp_me, taxrank = top_level, ntaxa = top_hits)\n  tmp_me_abund$abund_data$Abundance <- tmp_me_abund$abund_data$Abundance / 100\n  tmp_me_abund_gr <- trans_abund$new(dataset = tmp_me, taxrank = top_level, ntaxa = top_hits, \n                                  groupmean = \"TEMP\")\n  tmp_me_abund_gr$abund_data$Abundance <- tmp_me_abund_gr$abund_data$Abundance / 100\n  tmp_name <- purrr::map_chr(i, ~paste0(., \"_me_abund\")) \n  assign(tmp_name, tmp_me_abund)  \n  tmp_name_gr <- purrr::map_chr(i, ~paste0(., \"_me_abund_group\")) \n  assign(tmp_name_gr, tmp_me_abund_gr)  \n  rm(list = ls(pattern = \"tmp_\"))\n}\n\nThe result is stored in object$taxa_abund ...\n\nI prefer to specify the order of taxa in these kinds of plots. We can look the top ntaxa (defined above) by accessing the use_taxanames character vector of each microtable object.\n\n\n\n\nHum. Similar but not the same. This means we need to define the order for each separately. Once we do that, we will override the use_taxanames character vectors with the reordered vectors.\n\nCodeits18_ps_work_tax_ord <- rev(c(\n  \"k_Fungi\",  \"p_Ascomycota\",  \"c_Agaricomycetes\", \"Agaricales\",  \n  \"Archaeorhizomycetales\",  \"Capnodiales\",  \"Eurotiales\",  \"Geastrales\",  \n  \"Glomerales\",  \"Helotiales\", \"Hypocreales\",  \"Saccharomycetales\",  \n  \"Trichosporonales\",  \"Xylariales\"))   \n\nits18_ps_filt_tax_ord <- rev(c(\n  \"k_Fungi\",  \"p_Ascomycota\",  \"Polyporales\", \"Agaricales\",  \n  \"Archaeorhizomycetales\",  \"Capnodiales\",  \"Eurotiales\",  \"Geastrales\",  \n  \"Glomerales\",  \"Mortierellales\", \"Hypocreales\",  \"Saccharomycetales\",  \n  \"Trichosporonales\",  \"Xylariales\"))   \n\nits18_ps_perfect_tax_ord <- rev(c(\n  \"k_Fungi\",  \"p_Ascomycota\",  \"c_Agaricomycetes\", \"Agaricales\",  \n  \"Archaeorhizomycetales\",  \"Capnodiales\",  \"Eurotiales\",  \"Geastrales\",  \n  \"Glomerales\",  \"Helotiales\",   \"Hypocreales\",  \"Saccharomycetales\", \n  \"Trichosporonales\",  \"Pezizales\"))   \nits18_ps_pime_tax_ord <- rev(c(\n  \"k_Fungi\",  \"p_Ascomycota\",  \"Polyporales\",  \"Agaricales\",  \n  \"Archaeorhizomycetales\",  \"Capnodiales\",  \"Eurotiales\",  \"Geastrales\", \n  \"Glomerales\",  \"Mortierellales\",  \"Hypocreales\",  \"Saccharomycetales\",  \n  \"Trichosporonales\",  \"Pezizales\"))   \n\n\n\nAnd one more little step before plotting. Here we a) specify a custom color palette and b) specify the sample order.\n\n\nits18_colvec.tax <- rev(c(\"#323232\", \"#004949\", \"#924900\", \"#490092\", \n                      \"#6db6ff\", \"#920000\", \"#ffb6db\", \"#24ff24\", \n                      \"#006ddb\", \"#db6d00\", \"#b66dff\", \"#ffff6d\", \n                      \"#009292\", \"#b6dbff\", \"#ff6db6\"))\nits18_samp_order <- c(\"P02_D00_010_C0A\", \"P04_D00_010_C0B\", \"P06_D00_010_C0C\", \"P08_D00_010_C0D\", \"P10_D00_010_C0E\", \n                    \"P01_D00_010_W3A\", \"P03_D00_010_W3B\", \"P07_D00_010_W3D\", \"P09_D00_010_W3E\", \n                    \"P01_D00_010_W8A\", \"P03_D00_010_W8B\", \"P05_D00_010_W8C\", \"P07_D00_010_W8D\")\n\n\nNow we can generate plots (in a loop) for each faceted data set.\n\n\nfor (i in its18_data_sets) {\n  tmp_abund <- get(purrr::map_chr(i, ~paste0(., \"_me_abund\")))\n  tmp_tax_order <- get(purrr::map_chr(i, ~paste0(., \"_tax_ord\")))\n  tmp_abund$use_taxanames <- tmp_tax_order\n  tmp_facet_plot <- tmp_abund$plot_bar(use_colors = its18_colvec.tax, \n                                       others_color = \"#323232\", \n                                       facet = \"TEMP\", \n                                       xtext_keep = TRUE, \n                                       xtext_type_hor = FALSE, \n                                       legend_text_italic = FALSE, \n                                       xtext_size = 6, \n                                       facet_color = \"#cccccc\", \n                                       order_x = its18_samp_order)\n  tmp_facet_name <- purrr::map_chr(i, ~paste0(., \"_facet_plot\")) \n  assign(tmp_facet_name, tmp_facet_plot)  \n  rm(list = ls(pattern = \"tmp_\"))\n}\n\nThen add a little formatting to the faceted plots.\n\nset_to_plot <- c(\"its18_ps_work_facet_plot\", \"its18_ps_filt_facet_plot\", \"its18_ps_perfect_facet_plot\", \"its18_ps_pime_facet_plot\")\nfor (i in  set_to_plot) {\n    tmp_get <- get(i)\n    tmp_get <- tmp_get +\n    theme_cowplot() +\n    guides(fill = guide_legend(title = top_level, reverse = FALSE,\n                               keywidth = 0.7, keyheight = 0.7)) +\n    ylab(NULL) + xlab(\"Sample\") +\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n        panel.background = element_rect(fill = \"transparent\", colour = NA),\n        plot.background = element_rect(fill = \"transparent\", colour = NA),\n        panel.border = element_rect(fill = NA, color = \"black\"),\n        legend.text = element_text(size = 7),\n        legend.title = element_text(size = 10),\n        legend.position = \"right\",\n        axis.text.y = element_text(size = 8),\n        axis.text.x = element_text(size = 6, angle = 90),\n        strip.text = element_text(size = 8, angle = 0),\n        axis.title = element_text(size = 10)) + ylab(NULL) +\n      scale_y_continuous()\n  assign(i, tmp_get)\n  rm(list = ls(pattern = \"tmp_\")) \n}\n\nAnd now plots for the groupmeans sets. We can use the same taxa order since that should not have changed.\n\nset_to_plot <- c(\"its18_ps_work_group_plot\", \"its18_ps_filt_group_plot\", \"its18_ps_perfect_group_plot\", \"its18_ps_pime_group_plot\")\nfor (i in its18_data_sets) {\n  tmp_abund <- get(purrr::map_chr(i, ~paste0(., \"_me_abund_group\")))\n  tmp_tax_order <- get(purrr::map_chr(i, ~paste0(., \"_tax_ord\")))\n  tmp_abund$use_taxanames <- tmp_tax_order\n  tmp_group_plot <- tmp_abund$plot_bar(use_colors = its18_colvec.tax, \n                                       others_color = \"#323232\", \n                                       xtext_keep = TRUE, \n                                       xtext_type_hor = TRUE, \n                                       legend_text_italic = FALSE, \n                                       xtext_size = 10, \n                                       facet_color = \"#cccccc\")\n  tmp_group_name <- purrr::map_chr(i, ~paste0(., \"_group_plot\")) \n  assign(tmp_group_name, tmp_group_plot)  \n  rm(list = ls(pattern = \"tmp_\"))\n}\n\nLet’s also add a little formatting to the groupmean plots.\n\nfor (i in  set_to_plot) {\n     tmp_get <- get(i)\n     tmp_get <- tmp_get +\n       theme_cowplot() +\n       ylab(\"Relative Abundance (% total reads)\") + xlab(\"Temperature\") +\n       theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),\n             panel.background = element_rect(fill = \"transparent\", colour = NA),\n             plot.background = element_rect(fill = \"transparent\", colour = NA),\n             panel.border = element_rect(fill = NA, color = \"black\"),\n             legend.position = \"none\",\n             axis.text = element_text(size = 8),\n             axis.title = element_text(size = 10)) + \n       scale_y_continuous()\n\n     assign(i, tmp_get)\n     rm(list = ls(pattern = \"tmp_\"))\n}\n\n\nFinally we use the patchwork package to combine the two plots and customize the look.\n\n\n## single index that acts as an index for referencing elements (variables) in a list\n##  solution modified from this SO answerhttps://stackoverflow.com/a/54451460\nvar_list <- list(\n  var1 = its18_data_sets,\n  var2 = c(\"FULL\", \"Arbitrary filtered\", \"PERfect filtered\", \"PIME filtered\")\n)\nfor (j in  1:length(var_list$var1)) {\n     tmp_plot_final_name <- purrr::map_chr(var_list$var1[j], ~paste0(., \"_\", top_level, \"_plot_final\"))\n     tmp_set_type <- var_list$var2[j]\n     tmp_p_plot <- get(purrr::map_chr(var_list$var1[j], ~paste0(., \"_group_plot\")))\n     tmp_m_plot <- get(purrr::map_chr(var_list$var1[j], ~paste0(., \"_facet_plot\")))\n     tmp_plot_final <- tmp_p_plot + tmp_m_plot\n     tmp_plot_final <- tmp_plot_final +\n         plot_annotation(tag_levels = \"a\", \n                         title = paste(\"Top\", top_level, \"of\", tmp_set_type, \"data set\"), \n                         #subtitle = 'Top taxa of non-filtered data',\n                         caption = \"a) grouped by temperature.,\n                         b) All samples, faceted by temperature.\") + \n       plot_layout(widths = c(1, 2)) &\n       theme(plot.title = element_text(size = 13),\n             plot.subtitle = element_text(size = 10), \n             plot.tag = element_text(size = 12), \n             axis.title = element_text(size = 10), \n             axis.text = element_text(size = 8)) \n  assign(tmp_plot_final_name, tmp_plot_final)\n  rm(list = ls(pattern = \"tmp_\"))\n}\n\n\n\n\n\n\n\n\n\nIt is important to note that while most of the dominant taxa are the same across the FULL, Arbitrary, PIME, and PERfect data sets, there a few key difference:\n\n\nc_Agaricomycetes & Helotiales were not among the dominant taxa in the Arbitrary or PIME filtered data sets. Polyporales & Mortierellales were substituted, respectively.\nXylariales was not among the dominant taxa in either the PIME or PERfect data sets. Pezizales was substituted in both cases.\n\n\n\n\nFULL (unfiltered)\nArbitrary filtered\nPERFECT filtered\nPIME filtered\n\n\n\n\n\n\n\n\n(16S rRNA) Figure 5 | Dominant taxonomic groups from the FULL (unfiltered) ITS data set. A) Grouped by temperature and B) all samples faceted by temperature treatment.\n\n\n\n\n\n\n\n(16S rRNA) Figure 6 | Dominant taxonomic groups from the Arbitrary filtered ITS data set. A) Grouped by temperature and B) all samples faceted by temperature treatment.\n\n\n\n\n\n\n\n(16S rRNA) Figure 7 | Dominant taxonomic groups from the PERfect filtered ITS data set. A) Grouped by temperature and B) all samples faceted by temperature treatment.\n\n\n\n\n\n\n\n(16S rRNA) Figure 8 | Dominant taxonomic groups from the PIME filtered ITS data set. A) Grouped by temperature and B) all samples faceted by temperature treatment."
  },
  {
    "objectID": "som.html#supplmentary-tables",
    "href": "som.html#supplmentary-tables",
    "title": "Electronic Supplementary Material",
    "section": "Supplmentary Tables",
    "text": "Supplmentary Tables\nSupplmentary Table 1 | Publicly available data and data products.Supplmentary Table 2 | Sample Details.Supplmentary Table 3 | Primer sequences for 16S rRNA & ITS gene amplification.Supplmentary Table 4 | Tracking read changes through DADA2 workflow (16S rRNA).Supplmentary Table 5 | Tracking read changes through DADA2 workflow (ITS).Supplmentary Table 6 | Results of different filtering approaches (16S rRNA).Supplmentary Table 7 | Results of different filtering approaches (ITS).Supplmentary Table 8 | Hill numbers (16S rRNA) for samples in the unfiltered & filtered data sets.Supplmentary Table 9 | Summary of significant tests (16S rRNA) for unfiltered & filtered datas sets.Supplmentary Table 10 | Hill numbers (ITS) for samples in the unfiltered & filtered data sets.Supplmentary Table 11 | Summary of significant tests (ITS) for the unfiltered & filtered data sets.Supplmentary Table 12 | Results of beta dispersion & permutation test for homogeneity of multivariate dispersions (16S rRNA).Supplmentary Table 13 | Results of beta dispersion & permutation test for homogeneity of multivariate dispersions (ITS).Supplmentary Table 14 | Summary of beta diversity significant tests for the unfiltered & filtered data sets.Supplmentary Table 15 | Mantel tests for 16S rRNA & ITS data compared to each of the three metadata groups."
  },
  {
    "objectID": "som.html#supplmentary-figures",
    "href": "som.html#supplmentary-figures",
    "title": "Electronic Supplementary Material",
    "section": "Supplmentary Figures",
    "text": "Supplmentary Figures\nSupplmentary Figure 1 | Top bacterial/archaeal phyla (unfiltered data).Supplmentary Figure 2 | Top fungal orders (unfiltered data).Supplmentary Figure 3 | Alpha diversity (16S rRNA) of the unfiltered & filtered datas sets.Supplmentary Figure 4 | Alpha diversity (ITS) of the unfiltered & filtered datas sets.Supplmentary Figure 5 | Autocorrelation plots (16S rRNA).Supplmentary Figure 6 | Autocorrelation plots (ITS).Supplmentary Figure 7 | Acidobacteriota family plots.Supplmentary Figure 8 | Actinobacteriota family plots.Supplmentary Figure 9 | Alphaproteobacteria family plots.Supplmentary Figure 10 | Gammaproteobacteria family plots.Supplmentary Figure 11 | Bacteroidota family plots.Supplmentary Figure 12 | Firmicutes family plots.Supplmentary Figure 13 | Myxococcota family plots.Supplmentary Figure 14 | Verrucomicrobiota family plots"
  },
  {
    "objectID": "som.html#supplmentary-datasets",
    "href": "som.html#supplmentary-datasets",
    "title": "Electronic Supplementary Material",
    "section": "Supplmentary Datasets",
    "text": "Supplmentary Datasets\nSupplmentary Dataset 1 | Output from the 16S rRNA DADA2 workflow containing information for each ASVs.Supplmentary Dataset 2 | Output from the ITS DADA2 workflow containing information for each ASVs.Supplmentary Dataset 3 | Complete metadata information collected in this study.Supplmentary Dataset 4 | Differentially abundant (DA) ASVs from the 16S rRNA data identified using ISA.Supplmentary Dataset 5 | Differentially abundant (DA) ASVs from the 16S rRNA data identified using LEfSe.Supplmentary Dataset 6 | Differentially abundant (DA) ASVs from the ITS data identified using ISA.Supplmentary Dataset 7 | Differentially abundant (DA) ASVs from the ITS data identified using LEfSe."
  },
  {
    "objectID": "som.html#sample-naming",
    "href": "som.html#sample-naming",
    "title": "Electronic Supplementary Material",
    "section": "Sample naming",
    "text": "Sample naming\nSamples were named by combining the plot number (P01–P10) with the treatment (C = control, W = warming), the temperature (0 = no warming, 3 = +3°C warming, and 8 = +8°C warming), and the plot pairing designation (A—E). For example, P07_W3D is the sample from plot #7 that was warmed by +3°C. This sample is part of group D which contains P07_W8D (warmed by +8°C) and P08_C0D (the control sample for this group) (Supplementary Table 2).\n\n\n\nSupplementary Table 2 | Sample Details.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "som.html#processing-microbial-community-data",
    "href": "som.html#processing-microbial-community-data",
    "title": "Electronic Supplementary Material",
    "section": "Processing microbial community data",
    "text": "Processing microbial community data\nDNA was extracted using the DNeasy Powersoil kit (Qiagen) and bacterial and fungal communities were amplified using a two-stage PCR protocol where the locus specific primers used for PCR1 included the Illumina sequencing primer sequence on their 5’ ends. For bacteria, we amplified the V4 hypervariable region of the 16S rRNA gene using the 515F–806R (Caporaso et al. 2011) primer pair (Supplementary Table 3). For fungi, we amplified the first internal transcribed spacer (ITS1) region of the rRNA operon, using the primers ITS1F (Gardes and Bruns 1993) and ITS2 (White et al. 1990) (Supplementary Table 3).\n\n\n\nSupplementary Table 3 | Primer sequences for 16S rRNA & ITS gene amplification.\n\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nWe used Platinum 2X Mastermix (Thermo) in PCR reactions with a final volume of 12.5µl with 25 cycles using a 50°C annealing temperature for both loci. PCR2 used 2µl of PCR1 as template and added on remaining Illumina adaptors and index sequences. PCR2 products were cleaned and normalized using PCR Normalization plates (CharmBiotech, USA) and the pooled libraries concentrated using AMPure beads (Beckman Coulter, USA). Libraries were sequenced on an Illumina MiSeq with 250bp paired end reads.\nReads in the 16S rRNA and ITS data sets were trimmed of forward and reverse primers using cutadapt (Martin 2011) (v1.18) following an initial filtering step that removed reads with ambiguous bases. Primer sequences with more than 12% error rate (–error-rate = 0.12) were discarded. Reads were then processed using DADA2 (Callahan et al. 2016) (v1.16.0) within the R environment (Team, n.d.) (v4.1.0). Reads were dropped from the data set if they had three or more expected errors (maxEE = 2), at least one base with very low quality (truncQ = 2), or at least one position with an unspecified nucleotide (maxN = 0). Based on visual inspection of quality plots, only the forward reads from the 16S rRNA data were retained, while both forward and reverse reads were retained in the ITS data set. Remaining reads were dereplicated before inferring amplicon sequence variants (ASVs).\nWe used ASVs over traditional OTUs because ASVs provide single nucleotide resolution, thus providing more detailed resolution when examining treatment effects. Paired-end reads (ITS only) were merged and read pairs that did not match exactly across at least 12 base pairs (minOverlap = 12) were discarded. For the 16S rRNA data we retained amplicons between 230 and 235 base pairs and for the ITS data we retained amplicons between 100 and 450 base pairs. Reads were then screened for chimeras (method = consensus). Taxonomy for the 16S rRNA data set was assigned to each ASV using the naive Bayesian classifier (Wang et al. 2007) against the Silva reference database (Quast et al. 2012) (Silva_nr_v138_train_set version 138). For taxonomic classification of the ITS data set, we used the naive Bayesian classifier (Wang et al. 2007) against the UNITE (Nilsson et al. 2019) database, specifically the UNITE general FASTA release for Fungi (v. 04.02.2020) (Abarenkov et al. 2020). Supplementary Dataset1 and Supplementary Dataset2 contain the ASV table, taxonomic assignments, and unique sequences for the 16S rRNA and ITS data sets, respectively. The complete DADA2 workflow is available here: https://sweltr.github.io/high-temp/dada2.html.\nPrior to community analysis of the 16S rRNA data set, ASVs classified as chloroplasts, mitochondria, or Eukaryota, or ASVs that remained unclassified (i.e., NA) at the kingdom level, were removed from the data set using the phyloseq package (McMurdie and Holmes 2013) (v1.36.0) in the R environment (Team, n.d.). No such curation was performed on the ITS data set since all ASVs could be classified to kingdom level (Fungi). The complete data set preparation workflow is available here: https://sweltr.github.io/high-temp/data-prep.html."
  },
  {
    "objectID": "som.html#filtering",
    "href": "som.html#filtering",
    "title": "Electronic Supplementary Material",
    "section": "Filtering",
    "text": "Filtering\nWe applied three complementary methods of prevalence filtering to both the 16S rRNA and ITS data sets. The complete filtering workflow is available here: https://sweltr.github.io/high-temp/filtering.html.\ni) Sample-wise filtering with arbitrary functions. Here we used the genefilter_sample function from the phyloseq package (McMurdie and Holmes 2013) (v1.36.0) where ASVs represented by fewer than 5 reads and/or present in less that 20% of samples, were removed.\nii) PERFect (PERmutation Filtering test for microbiome data) (Smirnova, Huzurbazar, and Jafari 2019) (v0.2.4) filtering. Here we used the function PERFect_sim with the alpha parameter set to 0.05 for the 16S rRNA data and 0.1 for the ITS data.\niii) PIME (Prevalence Interval for Microbiome Evaluation) (Roesch et al. 2020) (v0.1.0) filtering. Here we first rarefied all samples to even depths (per the developer’s recommendation) and then split the data sets by predictor variable (temperature treatment) using the pime.split.by.variable function. Next, we calculated the prevalence intervals with the function pime.prevalence and then used the function pime.best.prevalence to calculate the best prevalence. The best prevalence interval was selected when the out-of-bag (OOB) error rate first reached zero or close to zero. The most prevalent ASVs (at the best prevalence interval) were retained from each split. Splits were then merged to obtain the final PIME filtered data set."
  },
  {
    "objectID": "som.html#alpha-diversity-estimates",
    "href": "som.html#alpha-diversity-estimates",
    "title": "Electronic Supplementary Material",
    "section": "Alpha diversity estimates",
    "text": "Alpha diversity estimates\nTo account for presence of rare sequence variants caused by sequencing errors (or other technical artifacts), we used Hill numbers (Alberdi and Gilbert 2019a) for estimates of alpha diversity. Hill numbers allow the weight put on rare versus abundant sequence variants to be scaled while providing intuitive comparisons of diversity levels using effective number of ASVs as a measuring unit. This approach allows for balancing the over representation of rare ASVs that might be inflated due to sequencing errors. To calculate Hill numbers, we used the R package hilldiv (Alberdi and Gilbert 2019b). We calculated three metrics that put less or more weight on common ASVs: (i) Observed richness, where q-value = 0; (ii) Shannon exponential, which weighs ASVs by their frequency, where q-value = 1; and (iii) Simpson multiplicative inverse, which over weighs abundant ASVs, where q-value = 2.\nWe present all three metrics of alpha diversity, while acknowledging that each metric of alpha diversity is based on (measured) relative abundance and can be subject to bias due to variation in extraction efficiency and sequencing depth, especially when detecting rare taxa (or ASVs) (Bálint et al. 2016). However, we also recognize that each metric provides complementary information, by varying in their relative sensitivity towards rare and common species (Roswell, Dushoff, and Winfree 2021). We therefore interpret alpha diversity metrics in terms of changes in diversity due to changes in rarer ASVs (observed richness) and due to changes in more proportionally abundant ASVs (Shannon and inverse Simpson).\nNext, we assessed whether the alpha diversity estimates were normally distributed using both the Shapiro-Wilk Normality test (Shapiro and Wilk 1965) and the Bartlett Test of Homogeneity of Variances (Bartlett 1937). If the p-values from both tests were not significant (p > 0.05), we accepted the null hypothesis that the results were normally distributed. If one or both of the the p-values were significant (p < 0.05), we rejected the null hypothesis. For parametric data we tested for significance across treatments using ANOVA followed by Tukey post-hoc test. For non-parametric data we tested for significance across treatments using Kruskal-Wallis followed by Dunn test with Benjamini-Hochberg correction. All tests were performed using the vegan package (Oksanen et al. 2012) in R (Team, n.d.). The complete alpha diversity workflow is available here: https://sweltr.github.io/high-temp/alpha.html."
  },
  {
    "objectID": "som.html#beta-diversity-estimates",
    "href": "som.html#beta-diversity-estimates",
    "title": "Electronic Supplementary Material",
    "section": "Beta diversity estimates",
    "text": "Beta diversity estimates\nTo test for significance between temperature treatments, we performed the following steps for the 16S rRNA and ITS data sets. First, we transformed the sample counts to relative abundance. We then generated distance matrices using the phyloseq (McMurdie and Holmes 2013) function phyloseq::distance. For the 16S rRNA data, we used unweighted and weighted UniFrac (Chen et al. 2012). For the ITS data, we used Jensen-Shannon Divergence (Fuglede and Topsoe 2004) and Bray-Curtis dissimilarity (Bray and Curtis 1957). Next, we calculated beta dispersion using the betadisper function from the vegan package (Oksanen et al. 2012). Then we used the function permutest to run a Permutation test for homogeneity of multivariate dispersions (Legendre, Oksanen, and ter-Braak 2011). If beta dispersion tests were not significant, we ran a PERMANOVA (Legendre and Anderson 1999) using adonis (PERMANOVA assumes equal dispersion), otherwise we used Analysis of Similarity (Clarke 1993) (anosim), both available in the vegan package (Oksanen et al. 2012). Ordination plots were generated for each distance matrix using Principal Coordinate Analysis (Gower 1966) (PCoA). The complete beta diversity workflow is available here: https://sweltr.github.io/high-temp/beta.html."
  },
  {
    "objectID": "som.html#diffentially-abundant-asvs",
    "href": "som.html#diffentially-abundant-asvs",
    "title": "Electronic Supplementary Material",
    "section": "Diffentially abundant ASVs",
    "text": "Diffentially abundant ASVs\nDiffentially abundant ASVs across temperature treatments (PIME filtered data sets) were identified using (i) the labdsv package (Roberts and Roberts 2016) (v.2.0-1)—to run Dufrene-Legendre Indicator Species Analysis (ISA)—and (ii) the microbiomeMarker package (Cao 2020) (v.0.0.1.9000) to run linear discriminant analysis (LDA) effect size (LEfSe) (Segata et al. 2011). ISA calculates the indicator value (fidelity and relative abundance) of ASVs in treatment groups. For the 16S rRNA and ITS data, we set the p-value cutoff to 0.5. For all other parameters the default values were used. For the LEfSe analysis we used pre-sample normalization of the sum of the values to 1e+06 (norm = \"CPM\"), set the LDA score cutoff to 2 (lda_cutoff = 2), set the p-value cutoff of Wilcoxon test to 0.05 (wilcoxon_cutoff = 0.05), and the p-value cutoff of Kruskal-Wallis test to 0.05 (kw_cutoff = 0.05). The complete diffentially abundant workflow is available here: https://sweltr.github.io/high-temp/da.html."
  },
  {
    "objectID": "som.html#multivariate-analysis",
    "href": "som.html#multivariate-analysis",
    "title": "Electronic Supplementary Material",
    "section": "Multivariate analysis",
    "text": "Multivariate analysis\nHere we compare the environmental metadata (Supplementary Dataset3) with both the PIME filtered 16S rRNA and the ITS community data. The complete multivariate workflow is available here: https://sweltr.github.io/high-temp/metadata.html. Each workflow contained the same major steps:\n\nMetadata normality tests: We used Shapiro-Wilk Normality Test (Shapiro and Wilk 1965) to test whether each metadata parameter was normally distributed.\nNormalize parameters: Use the R package bestNormalize (Peterson and Cavanaugh 2019; Peterson 2021) and default parameters to find and execute the best normalizing transformation for non-parametric metadata parameters identified in step #1. The function tested the following normalizing transformations: arcsinh, Box-Cox, Yeo-Johnson, Ordered Quantile (ORQ) normalization, log transformation, square-root, and exponential. Once the non-parametric parameters were transformed, we reran the normality tests.\n\nSplit metadata parameters into groups: Next we split the metadata parameters into three groups: a) environmental and edaphic properties; b) microbial functional responses; and c) temperature adaptation properties.\n\nEnvironmental and edaphic properties: AST, H2O, N, P, Al, Ca, Fe, K, Mg, Mn, Na, TEB, ECEC, pH, NH4, NO3, PO4, DOC, DON, DOCN.\nMicrobial functional responses: micC, micN, micP, micCN, micCP, micNP, AGase, BGase, BPase, CEase, Pase, Nase, Sase, XYase, LPase, PXase, CO2, enzCN, enzCP, enzNP.\nTemperature adaptation: AGQ10, BGQ10, BPQ10, CEQ10, PQ10, NQ10, SQ10, XYQ10, LPQ10, PXQ10, CUEcn, CUEcp, NUE, PUE, Tmin, SI.\n\n\n\nThe following parameters were collected but not used in the analysis: minPO4, minNH4, minNO3, minTIN.\n\nAutocorrelation tests: Then we tested all possible pair-wise comparisons of the normalized metadata (from step #2) for each group (step #3) to identify potential autocorrelated parameters.\nRemove autocorrelated parameters from each group. Based on the results of step #4, we removed autocorrelated parameters.\nDissimilarity correlation tests: Here we used Mantel Tests to determine if any metadata groups were significantly correlated with community data. We generated Bray-Curtis (Bray and Curtis 1957) distance matrices for the community data and Euclidean distance matrices for each metadata group. Then we performed Mantel tests (Mantel 1967; Legendre and Legendre 2012) for all comparisons using the function mantel from the vegan (Oksanen et al. 2012) package.\nBest subset of variables: Then we determined which metadata parameters (from each group) were the most strongly correlated with the community data. For this we used the bioenv function from the vegan (Oksanen et al. 2012) package where method = \"spearman\", index = \"bray\", and metric = \"euclidean\".\n\nDistance-based Redundancy Analysis (dbRDA): Finally, we performed ordination analysis of samples plus metadata vector overlays using capscale, also from the vegan package. For each of the three metadata subsets, we perform the following steps:\n\nRun rankindex (Faith, Minchin, and Belbin 1987) in the vegan package to compare metadata and community dissimilarity indices for gradient detection. This aids in the selection of the best dissimilarity metric to use. Here we tested the following metrics: Euclidean, Manhattan, Gower, Bray–Curtis, and Kulczynski.\nRun capscale in the vegan package for distance-based redundancy analysis.\nRun envfit to fit environmental parameters onto the ordination. This function basically calculates correlation scores between the metadata parameters and the ordination axes.\nSelect metadata parameters significant for bioenv (see above) and/or envfit analyses.\nPlot the ordination and vector overlays."
  },
  {
    "objectID": "som.html#tracking-reads-through-dada2-workflow",
    "href": "som.html#tracking-reads-through-dada2-workflow",
    "title": "Electronic Supplementary Material",
    "section": "Tracking reads through DADA2 workflow",
    "text": "Tracking reads through DADA2 workflow\n16S rRNA data\nThe processed and curated 16S rRNA data set contained 937,761 high-quality reads, with a range of 25,151–86,600 reads per sample (mean 62,443). Modelling and error correcting amplicon errors inferred 20,332 ASVs, 19% of which were doubleton ASVs. After removing reads classified as mitochondria, chloroplast, or Eukaryota, the data set contained 20,173 ASVs and 936,640 reads—–with a range of 25,088–-86,600 reads (mean 62,443) and 813–-3065 (mean 2063) ASVs per sample (Supplementary Table 4).\n\n\n\nSupplementary Table 4 | Tracking read changes through DADA2 workflow (16S rRNA).\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n\n\n Column descriptions. 1raw: number of initial reads; 2pre filt: after removing reads with ambiguous bases; 3cut: after removing primers; 4filter: after filtering; 5denoiseF: forward reads after denoising; 6nonchim: after removing chimeras; 7final: final read totals; 8asvs: total ASVs. \nITS data\nThe processed and curated ITS data set contained 491,143 high-quality reads, with a range of 80–64,636 reads per sample (mean 32,743). Modelling and error correcting amplicon errors inferred 3357 ASVs, 2.1% of which were doubleton ASVs. After removing 2 samples with low read counts (< 500 reads), the data set contained 3357 ASVs and 490,767 reads—–with a range of 9172–-64,636 reads (mean 37,751) and 335–1017 (mean 757) ASVs per sample (Supplementary Table 5).\n\n\n\nSupplementary Table 5 | Tracking read changes through DADA2 workflow (ITS).\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n\n\n Column descriptions. 1raw: number of initial reads; 2pre filt: after removing reads with ambiguous bases; 3cut: after removing primers; 4filter: after filtering; 5denoiseF: forward reads after denoising; 6denoiseR: reverse reads after denoising; 7merge: after merging paired-end reads 8nonchim: after removing chimeras; 9final: final read totals; 10asvs: total ASVs. *These samples were removed from final analysis."
  },
  {
    "objectID": "som.html#filtering-1",
    "href": "som.html#filtering-1",
    "title": "Electronic Supplementary Material",
    "section": "Filtering",
    "text": "Filtering\n16S rRNA data\nThe curated 16S rRNA data set contained 936,640 reads and 20,173 ASVs. Applying arbitrary filtering—removing ASVs represented by fewer than 5 reads and/or present in less than 20% of samples—reduced the number of reads by 22% and the number of ASVs by 90%. Similarly, PERFect filtering reduced the number of reads by 22% and the number of ASVs by 92% (Supplementary Table 6).\nRarefying the FULL data set to even read depths (25,088 reads/sample) prior to PIME filtering removed 2741 ASVs, reducing the number of reads by 60% and the number of ASVs by 14%. Baseline noise detection indicated the out-of-bag (OOB) error rate was 0.67. Splitting the rarefied data by predictor variable (i.e., temperature treatment) resulted in a total of 7883 ASVs for the Control group, 7173 ASVs for the +3°C group, and 6027 ASVs for the +8°C group. The Best Prevalence interval, calculated using the function pime.best.prevalence, was Prevalence = 50%, the first interval where the OOB error rate dropped to 0%. At this interval (compared to the FULL data set), the number of reads was reduced by 75% and the number of ASVs by 95% (Supplementary Table 6).\n\n\n\nSupplementary Table 6 | Results of different filtering approaches (16S rRNA).\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nITS data\nThe curated ITS data set contained 490,767 reads and 3355 ASVs. Applying arbitrary filtering—removing ASVs represented by fewer than 5 reads and/or present in less that 20% of samples—reduced the number of reads by 21% and the number of ASVs by 76%. PERFect filtering reduced the number of reads by 26% and the number of ASVs by 91% (Supplementary Table 7).\nRarefying the FULL data set to even read depths (9172 reads/sample) prior to PIME filtering removed 298 ASVs, reducing the number of reads by 76% and the number of ASVs by 9%. Baseline noise detection indicated the out-of-bag (OOB) error rate was 0.39. Splitting the rarefied data by predictor variable (i.e., temperature treatment) yielded a total of 1932 ASVs for the Control group, 1682 ASVs for the +3°C group, and 1306 ASVs for the +8°C group. The Best Prevalence interval, calculated using the function pime.best.prevalence, was Prevalence = 55%, the first interval where the OOB error rate dropped to 0%. At this interval (compared to the FULL data set), the number of reads was reduced by 86% and the number of ASVs by 87% (Supplementary Table 7).\n\n\n\nSupplementary Table 7 | Results of different filtering approaches (ITS).\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "som.html#taxonomic-diversity-of-microbial-communities",
    "href": "som.html#taxonomic-diversity-of-microbial-communities",
    "title": "Electronic Supplementary Material",
    "section": "Taxonomic diversity of microbial communities",
    "text": "Taxonomic diversity of microbial communities\nBacterial diversity was largely comprised of Proteobacteria (Alpha, Gamma), Acidobacteriota, Actinobacteriota, Bacteroidota, Firmicutes, Myxococcota, Verrucomicrobiota, Chloroflexi, and Planctomycetota. Methylomirabilota and Crenarchaeota were the dominanmt phyla of Archaeal diversity (Supplementary Figure 1). For a breakdown of dominant bacterial phyla by family, see . Fungal diversity was largely comprised of Ascomycota, Basidiomycota, and Glomeromycota (Supplementary Figure 2). The complete taxonomic workflow is available here: https://sweltr.github.io/high-temp/taxa.html."
  },
  {
    "objectID": "som.html#alpha-diversity-of-microbial-communities",
    "href": "som.html#alpha-diversity-of-microbial-communities",
    "title": "Electronic Supplementary Material",
    "section": "Alpha diversity of microbial communities",
    "text": "Alpha diversity of microbial communities\nShapiro-Wilk Normality and Bartlett tests indicated all data was normally distributed except for Shannon exponential estimates of the 16S rRNA PIME filtered data. Differences in alpha diversity assessed using analysis of variance (ANOVA) followed by Tukey HSD post hoc tests (normally distributed data) or Kruskal-Wallis followed by Dunn test with Benjamini-Hochberg correction (non-normally distributed data).\n16S rRNA data\nSupplementary Table 8 contains the results of alpha diversity estimates for the unfiltered data set plus the arbitrary, PERFect, and PIME filtered data sets. In Supplementary Table 9 we report the results of the Shapiro-Wilk Normality and Bartlett tests for each Hill number as well as the results of post-hoc analysis. Alpha diversity plots comparing the different filtering methods for each Hill number can be found in Supplementary Figure 3.\n\n\n\nSupplementary Table 8 | Hill numbers (16S rRNA) for samples in the unfiltered & filtered data sets.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n FULL = unfiltered data set; FILT = arbitrary filtering where nreads > 5 and prevalence > 20%; PERFect = PERFect filtering; PIME = PIME filtering \n\n\n\n\n\n\nSupplementary Table 9 | Summary of significant tests (16S rRNA) for unfiltered & filtered datas sets.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n Column descriptions. 1metric: Hill number; 2data set: FULL = unfiltered data set; FILT = arbitrary filtering where nreads > 5 and prevalence > 20%; PERFect = PERFect filtering; PIME = PIME filtering; 3pval_shap: p-value of Shapiro-Wilk Normality test; 4pval_bart: p-value of Bartlett Test of Homogeneity of Variances; 5method: Selected significance test; 6posthoc method: Selected posthoc test; 7posthoc pval: Posthoc p-value \n\n\n\n\n\n\n\n\n\nITS data\nSupplementary Table 10 contains the results of alpha diversity estimates for the unfiltered data set plus the arbitrary, PERFect, and PIME filtered data sets. In Supplementary Table 11 we report the results of the Shapiro-Wilk Normality and Bartlett tests for each Hill number as well as the results of post-hoc analysis. Alpha diversity plots comparing the different filtering methods for each Hill number can be found in Supplementary Figure 4.\n\n\n\nSupplementary Table 10 | Hill numbers (ITS) for samples in the unfiltered & filtered data sets.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n FULL = unfiltered data set; FILT = arbitrary filtering where nreads > 5 and prevalence > 20%; PERFect = PERFect filtering; PIME = PIME filtering \n\n\n\n\n\n\nSupplementary Table 11 | Summary of significant tests (ITS) for the unfiltered & filtered data sets.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n Column descriptions. 1metric: Hill number; 2data set: FULL = unfiltered data set; FILT = arbitrary filtering where nreads > 5 and prevalence > 20%; PERFect = PERFect filtering; PIME = PIME filtering; 3pval_shap: p-value of Shapiro-Wilk Normality test; 4pval_bart: p-value of Bartlett Test of Homogeneity of Variances; 5method: Selected significance test; 6posthoc method: Selected posthoc test; 7posthoc pval: Posthoc p-value"
  },
  {
    "objectID": "som.html#beta-diversity-of-microbial-communities",
    "href": "som.html#beta-diversity-of-microbial-communities",
    "title": "Electronic Supplementary Material",
    "section": "Beta diversity of microbial communities",
    "text": "Beta diversity of microbial communities\nTo test for significance between treatment groups, we calculated the beta dispersion (using the betadisper function, vegan package) for each 16S rRNA distance matrix (unweighted and weighted UniFrac) and each ITS distance matrix (Jensen-Shannon and Bray-Curtis). We then used the function permutest to run a Permutation Test for Homogeneity of multivariate dispersions against the results of each beta dispersion test (Supplementary Table 12, Supplementary Table 13). If the results were not significant (i.e., p-value > 0.05) we ran a PERMANOVA using adonis (PERMANOVA assumes equal dispersion), otherwise we used Analysis of Similarity (ANOSIM). Supplementary Table 14 contains the results of the significance tests.\n\n\n\nSupplementary Table 12 | Results of beta dispersion & permutation test for homogeneity of multivariate dispersions (16S rRNA).\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Table 13 | Results of beta dispersion & permutation test for homogeneity of multivariate dispersions (ITS).\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Table 14 | Summary of beta diversity significant tests for the unfiltered & filtered data sets.\n\n\n\n Download data as csv file"
  },
  {
    "objectID": "som.html#differentially-abundant-asvs",
    "href": "som.html#differentially-abundant-asvs",
    "title": "Electronic Supplementary Material",
    "section": "Differentially abundant ASVs",
    "text": "Differentially abundant ASVs\nIndicator Species Analysis (ISA) of the 16S rRNA data set identified 251 differentially abundant (DA) ASVs. Of those, 154 ASVs were enriched in the Control samples, 82 in the +3°C treatment, and 15 in the +8°C treatment (Supplementary Dataset4). Linear discriminant analysis (LDA) effect size (LEfSe) identified 676 DA ASVs with an LDA score > 2.0 and a p-value < 0.05. Of those, 355 ASVs were enriched in the Control samples, 227 in the +3°C treatment, and 94 in the +8°C treatment (Supplementary Dataset5).\nISA of the ITS data set identified 203 DA ASVs. Of those, 54 ASVs were enriched in the Control samples, 95 in the +3°C treatment, and 54 in the +8°C treatment (Supplementary Dataset6). LEfSe identified 228 DA ASVs with an LDA score > 2.0 and a p-value < 0.05. Of those, 52 ASVs were enriched in the Control samples, 107 in the +3°C treatment, and 69 in the +8°C treatment (Supplementary Dataset7)."
  },
  {
    "objectID": "som.html#multivariate-analysis-1",
    "href": "som.html#multivariate-analysis-1",
    "title": "Electronic Supplementary Material",
    "section": "Multivariate analysis",
    "text": "Multivariate analysis\nNormality tests & parameter normalization\nWe used Shapiro-Wilk Normality Test (Shapiro and Wilk 1965) to determine which of the 61 metadata parameters were or were not normally distributed. For the 16S rRNA data we needed to transform 25 metadata parameters (p-value < 0.05) and for the ITS data, 21 metadata parameters needed transformation (p-value < 0.05). Please see the project website for the specific parameters that were transformed and the method of transformation used in each case (https://sweltr.github.io/high-temp/metadata.html). For both community data sets, bestNormalize was unable to find a suitable transformation for Al and Fe. This is likely because there was very little variation in these parameters and/or there were too few significant digits.\nRemoving autocorrelated parameters\nBased on autocorrelation tests between the metadata and community data (Supplementary Figure 5, Supplementary Figure 6), we removed the following parameters:\nEnvironmental and edaphic properties: TEB, DON, Na, Al, Ca.\nMicrobial functional responses: micN, micNP, enzCN, enzCP, BPase, CEase, LPase, Nase, Pase.\nTemperature adaptation: NUE, PUE, SI.\n\n\n\nIn addition, we removed PQ10 (temperature adaptation) from the ITS analysis based on the autocorrelation tests.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDissimilarity correlation tests\nWe used Mantel Tests to determine if any metadata groups were significantly correlated with 16S rRNA or ITS community data (Supplementary Table 15).\n\n\n\nSupplementary Table 15 | Mantel tests for 16S rRNA & ITS data compared to each of the three metadata groups. Significant differences denoted by p-values < 0.05.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nBest subset of variables\nThe bioenv function found the following metadata parameters (normalized with autocorrelated data removed) significantly correlated with community data (results of Mantel tests shown in parentheses).\nEnvironmental and edaphic properties\n16S rRNA: AST (r = 1.0, p = 0.001).ITS: AST (r = 1.0, p = 0.001).\nMicrobial functional responses\n16S rRNA: AGase (r = 0.559, p = 0.001), enzNP (r = 0.462, p = 0.006), Sase (r = 0.614, p = 0.001), PXase (r = 0.612, p = 0.001), XYase (r = 0.456, p = 0.002).ITS: enzNP (r = 0.553, p = 0.001), PXase (r = 0.685, p = 0.001), XYase (r = 0.505, p = 0.002).\nTemperature adaptation\n16S rRNA: CUEcp (r = 0.325, p = 0.013), LPQ10 (r = 0.377, p = 0.005), PQ10 (r = 0.518, p = 0.001), SQ10 (r = 0.440, p = 0.001), and Tmin (r = 0.404, p = 0.005).ITS: XYQ10 (r = 0.726, p = 0.001), Tmin (r = 0.616, p = 0.001).\nDistance-based Redundancy Analysis (dbRDA)\nIn all cases (i.e., both community data sets against each of the three metadata subsets), rankindex (Faith, Minchin, and Belbin 1987) indicated that Bray-Curtis was best dissimilarity metric to use. Based on these results, we set dist = \"bray\" for each dbRDA analysis using capscale. Due to issue pertaining to degrees of freedom, we needed to remove some metadata parameters from specific groups. From the 16S rRNA analysis, we removed Mg and Mn (environmental and edaphic properties). From the ITS analysis, we removed Mg, Mn, Na, Al, Fe, and K (environmental and edaphic properties) and SQ10 (temperature adaptation). Next, we used the vegan function envfit to fit environmental parameters onto the ordination. This function calculates correlation scores between metadata parameters and ordination axes. envfit found the following parameters were significantly correlated with community data (Goodness of fit statistic/squared correlation coefficient and empirical p-values for each variable shown in parentheses).\nEnvironmental and edaphic properties\n16S rRNA: AST (r2 =0.829, p = 0.001), H2O (r2 =0.519, p = 0.010), DOC (r2 =0.446, p = 0.024).ITS: AST (r2 =0.485, p = 0.037), DOC (r2 =0.535, p = 0.028).\nMicrobial functional responses\n16S rRNA: AGase (r2 = 0.444, p = 0.026), BGase (r2 = 0.560, p = 0.007), Sase (r2 = 0.737, p = 0.002), XYase (r2 = 0.519, p = 0.009), PXase (r2 = 0.764, p = 0.001), CO2 (r2 = 0.504, p = 0.013), enzNP (r2 = 0.624, p = 0.004).ITS: micP (r2 = 0.693, p = 0.002), micCP (r2 = 0.583, p = 0.016), AGase (r2 = 0.506, p = 0.037), PXase (r2 = 0.500, p = 0.035), enzNP (r2 = 0.547, p = 0.014).\nTemperature adaptation\n16S rRNA: SQ10 (r2 = 0.496, p = 0.015), XYQ10 (r2 = 0.373, p = 0.049), LPQ10 (r2 = 0.413, p = 0.041), Tmin (r2 = 0.446, p = 0.030).ITS: XYQ10 (r2 = 0.617, p = 0.010), CUEcp (r2 = 0.479, p = 0.035), Tmin (r2 = 0.475, p = 0.028)."
  },
  {
    "objectID": "som.html#appendix-1-supplementary-datasets",
    "href": "som.html#appendix-1-supplementary-datasets",
    "title": "Electronic Supplementary Material",
    "section": "Appendix 1: Supplementary Datasets",
    "text": "Appendix 1: Supplementary Datasets\nFor this study, Supplementary Datasets are text files that were too large to include in the Supplementary Material. The individual files can be downloaded from the journal’s website. Below are descriptions for each Supplementary Data item.\nSupplementary Dataset1\nDescription: Output from the 16S rRNA DADA2 workflow before manual curation. Table is a tab delimited text file containing information for 20,332 ASVs. The first column is the unique ASV ID, followed by the read counts for each sample, ASV taxonomic lineage (Kingdom to Genus), and finally the unique ASV sequence.\n\n\n\n\n\n\n Download the zipped file here\n\n\n\n\nSupplementary Dataset2\nDescription: Output from the ITS DADA2 workflow before manual curation. Table is a tab delimited text file containing information for 3357 ASVs. The first column is the unique ASV ID, followed by the read counts for each sample, ASV taxonomic lineage (Kingdom to Genus), and finally the unique ASV sequence.\n\n\n\n\n\n\n Download the zipped file here\n\n\n\n\nSupplementary Dataset3\n\n\n\nDescription: Complete metadata information collected in this study. Tab delimited text file containing data for 61 metadata parameters (before normalization) associated with each sample. The first column is the sample ID, followed plot number (1–10), treatment (control or warm), temperature (0°C, +3°C, +8°C), plot pair ID (A–E), and collection season (W = rainy season). Subsequent columns contain values for all metadata parameters.\nSupplementary Dataset 3 | Complete metadata information collected in this study.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nSupplementary Dataset4\nDescription: Differentially abundant (DA) ASVs from the 16S rRNA data identified using Indicator Species Analysis (ISA) against the PIME filtered data set. Tab delimited text file of all 251 DA ASVs between temperature treatments.\n\n\n\nSupplementary Dataset 4 | Differentially abundant (DA) ASVs from the 16S rRNA data identified using ISA.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nDescription of table headers:\n\n\nASV_ID ASV name.\n\ngroup Sample group ASV is enriched in.\n\nindval Indicator value from Dufrene-Legendre Indicator Species Analysis.\n\npval p-value from Dufrene-Legendre Indicator Species Analysis.\n\nfreq Total number of samples where ASV was detected.\n\nfreq_C0 Total number of Control samples where ASV was detected.\n\nfreq_W3 Total number of +3°C samples where ASV was detected.\n\nfreq_W8 Total number of +8°C samples where ASV was detected.\n\nreads_total Total reads in data set.\n\nreads_C0 Total reads in Control samples.\n\nreads_W3 Total reads in +3°C samples.\n\nreads_W8 Total reads in +8°C samples.\n\nThe remaining columns contain lineage information for each ASV followed by its’ unique sequence.\nSupplementary Dataset5\nDescription: Differentially abundant (DA) ASVs from the 16S rRNA data identified using linear discriminant analysis (LDA) effect size (LEfSe) against the PIME filtered data set. Tab delimited text file of all 676 DA ASVs between temperature treatments.\n\n\n\nSupplementary Dataset 5 | Differentially abundant (DA) ASVs from the 16S rRNA data identified using LEfSe. \n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nDescription of table headers:\n\n\nASV_ID ASV name.\n\ngroup Sample group ASV is enriched in.\n\nlda Linear discriminant analysis (LDA) scores.\n\npval p-value from LEfSe analysis.\n\nfreq Total number of samples where ASV was detected.\n\nfreq_C0 Total number of Control samples where ASV was detected.\n\nfreq_W3 Total number of +3°C samples where ASV was detected.\n\nfreq_W8 Total number of +8°C samples where ASV was detected.\n\nreads_total Total reads in data set.\n\nreads_C0 Total reads in Control samples.\n\nreads_W3 Total reads in +3°C samples.\n\nreads_W8 Total reads in +8°C samples.\n\nThe remaining columns contain lineage information for each ASV followed by its’ unique sequence.\nSupplementary Dataset6\nDescription: Differentially abundant (DA) ASVs from the ITS data identified using Indicator Species Analysis (ISA) against the PIME filtered data set. Tab delimited text file of all 203 DA ASVs between temperature treatments.\n\n\n\nSupplementary Dataset 6 | Differentially abundant (DA) ASVs from the ITS data identified using ISA.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nDescription of table headers:\n\n\nASV_ID ASV name.\n\ngroup Sample group ASV is enriched in.\n\nindval Indicator value from Dufrene-Legendre Indicator Species Analysis.\n\npval p-value from Dufrene-Legendre Indicator Species Analysis.\n\nfreq Total number of samples where ASV was detected.\n\nfreq_C0 Total number of Control samples where ASV was detected.\n\nfreq_W3 Total number of +3°C samples where ASV was detected.\n\nfreq_W8 Total number of +8°C samples where ASV was detected.\n\nreads_total Total reads in data set.\n\nreads_C0 Total reads in Control samples.\n\nreads_W3 Total reads in +3°C samples.\n\nreads_W8 Total reads in +8°C samples.\n\nThe remaining columns contain lineage information for each ASV followed by its’ unique sequence.\nSupplementary Dataset7\nDescription: Differentially abundant (DA) ASVs from the ITS data identified using linear discriminant analysis (LDA) effect size (LEfSe) against the PIME filtered data set. Tab delimited text file of all 228 DA ASVs between temperature treatments.\n\n\n\nSupplementary Dataset 7 | Differentially abundant (DA) ASVs from the ITS data identified using LEfSe.\n\n\n\n Download data as csv file\n\n\n\n\n\n\n\n\n\n\nDescription of table headers:\n\n\nASV_ID ASV name.\n\ngroup Sample group ASV is enriched in.\n\nlda Linear discriminant analysis (LDA) scores.\n\npval p-value from LEfSe analysis.\n\nfreq Total number of samples where ASV was detected.\n\nfreq_C0 Total number of Control samples where ASV was detected.\n\nfreq_W3 Total number of +3°C samples where ASV was detected.\n\nfreq_W8 Total number of +8°C samples where ASV was detected.\n\nreads_total Total reads in data set.\n\nreads_C0 Total reads in Control samples.\n\nreads_W3 Total reads in +3°C samples.\n\nreads_W8 Total reads in +8°C samples.\n\nThe remaining columns contain lineage information for each ASV followed by its’ unique sequence."
  },
  {
    "objectID": "som.html#appendix-2-family-level-bacterial-charts",
    "href": "som.html#appendix-2-family-level-bacterial-charts",
    "title": "Electronic Supplementary Material",
    "section": "Appendix 2: Family-level bacterial charts",
    "text": "Appendix 2: Family-level bacterial charts\nTop twelve (12) families of abundant bacterial phyla. Remaining taxa are grouped in Other. In cases where ASVs could not be classified to family level, abundance data was calculated for the next highest taxonomic rank, denoted by the prefix rank abbreviation plus underscore (e.g., c_ is Class). As above, relative abundance of taxa based on the full, unfiltered data set. Left plots show taxa collapsed by temperature treatment while right plots show individual samples faceted by temperature treatment. Taxa are ordered first by rank and then alphabetically. The same color palette displayed in the same order was used for each plot."
  }
]