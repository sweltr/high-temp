---
title: "4a Filtering: PIME (Prevalence Interval for Microbiome Evaluation) "
description: |
  Reproducible workflow for ... In this workflow, ....
author:
#  - name: Jarrod J Scott
#    url: https://example.com/norajones
#    affiliation: Spacely Sprockets
#    affiliation_nrl: https://example.com/spacelysprokets
bibliography: assets/cite.bib
---

<details markdown="1">
<summary>Click here for setup information.</summary>

```{r setup}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
set.seed(119)

#library(conflicted)
#pacman::p_depends(PERFect, local = TRUE)  
#pacman::p_depends_reverse(PERFect, local = TRUE)  
library(phyloseq); packageVersion("phyloseq")
library(Biostrings); packageVersion("Biostrings")
pacman::p_load(tidyverse, metacoder, hilldiv, patchwork, ampvis2, 
               agricolae, labdsv, naniar, codefolder, PERFect, pime,
               pairwiseAdonis, microbiome, seqRFLP, DT, microbiomeMarker, 
               install = FALSE, update = FALSE)

options(scipen=999)
knitr::opts_current$get(c(
  "cache",
  "cache.path",
  "cache.rebuild",
  "dependson",
  "autodep"
))
```
</details>

# Summary of Results

```{r, include=FALSE, eval=TRUE}
## Load to build page only #4
remove(list = ls())
load("page_build/pime-summary_high_temp_pime_wf.rdata")
ssu18_asv_pime_sum[,2] <- list(NULL)
ssu18_asv_pime_sum <- ssu18_asv_pime_sum %>% dplyr::rename("ASV reads" = "total reads") %>%
                                             dplyr::rename("no. ASVs" = "total asvs")
ssu18_otu_pime_sum[,2:3] <- list(NULL)
ssu18_otu_pime_sum <- ssu18_otu_pime_sum %>% dplyr::rename("OTU reads" = "total reads") %>%
                                             dplyr::rename("no. OTUs" = "total otus")
ssu18_pime_sum <- dplyr::left_join(ssu18_asv_pime_sum, ssu18_otu_pime_sum)
ssu18_pime_sum <- ssu18_pime_sum[, c(1:3,5,4,6)]

its18_asv_pime_sum[,2] <- list(NULL)
its18_asv_pime_sum <- its18_asv_pime_sum %>% dplyr::rename("ASV reads" = "total reads") %>%
                                             dplyr::rename("no. ASVs" = "total asvs")
its18_otu_pime_sum[,2:3] <- list(NULL)
its18_otu_pime_sum <- its18_otu_pime_sum %>% dplyr::rename("OTU reads" = "total reads")%>%
                                             dplyr::rename("no. OTUs" = "total otus")
its18_pime_sum <- dplyr::left_join(its18_asv_pime_sum, its18_otu_pime_sum)
its18_pime_sum <- its18_pime_sum[, c(1:3,5,4,6)]
```

In Part A of this workflow, we used PIME [(Prevalence Interval for Microbiome Evaluation)](https://github.com/microEcology/pime) [@roesch2020pime] to filter the FULL 16S rRNA (ASV & OTU) and ITS (ASV & OTU) data sets. In [Part B](perfect.html) we used PERFect [(PERmutation Filtering test for microbiome data)](https://github.com/katiasmirn/PERFect) [@smirnova2019perfect] to filter thye data sets. 

The first step in the PIME process is to rarefy the data and then proceed with the filtering. Here is a summary of the results. Tables show comparisons of ASV vs. OTU data sets. Tables are duplicated in the relevant sections below.

## 16S rRNA ASV vs. OTU

```{r, echo=FALSE, layout="l-body", eval=TRUE}
knitr::kable(ssu18_pime_sum)
```

## ITS ASV vs. OTU

```{r, echo=FALSE, layout="l-body", eval=TRUE}
knitr::kable(its18_pime_sum)
```

# Prerequisites

This workflow contains PIME filtering of the 2018 16S rRNA data set. In order to run the workflow, you either need to first run the  [DADA2 Workflow](dada2.html), the [Data Preparation workflow](data-prep.html), and the [OTU clustering workflow](otu.html). See the [Data Availability](data-availability.html) page for complete details.

# 16s rRNA ASV Data Set

```{r, include=FALSE, eval=TRUE}
## Load to build page only #3
remove(list = ls())
load("page_build/pime_ssu18_asv_wf.rdata")
```


```{r, include=FALSE}
## LOad only after workflow is complete #2
remove(list = ls())
load("files/pime/rdata/pime_ssu18_asv.rdata")
ssu18_amp_data <- readRDS("files/data-prep/rdata/ssu18_amp_data.rds")
ssu18_samp_data <- readRDS("files/data-prep/rdata/ssu18_samp_data.rds")

ssu18_samp_data[, 7:9] <- NULL
rm(list = ls(pattern = "its18_"))
```


```{r, include=FALSE}
## Initial Load for PIME  ANALYSIS #1
remove(list = ls())
set.seed(119)
ssu18_ps_work <- readRDS("files/data-prep/rdata/ssu18_ps_work.rds")
ssu18_ps_work_o <- readRDS("files/data-prep/rdata/ssu18_ps_work_o.rds")
ssu18_samp_data <- readRDS("files/data-prep/rdata/ssu18_samp_data.rds")
ssu18_amp_data <- readRDS("files/data-prep/rdata/ssu18_amp_data.rds")
ssu18_ps_data <- readRDS("files/data-prep/rdata/ssu18_ps_amp.rds")
ssu18_ps_data_tree <- rtree(ntaxa(ssu18_ps_data), rooted = TRUE,
                           tip.label = taxa_names(ssu18_ps_data))
ssu18_ps_data <- merge_phyloseq(ssu18_ps_data, sample_data, ssu18_ps_data_tree)
```

We will use PIME (Prevalence Interval for Microbiome Evaluation) to create a filtered data set.

## Setup

First, choose a phyloseq object and a sample data frame

```{r}
ssu18_pime_ds <- ssu18_ps_work
ssu18_which_pime <- "ssu18_pime_ds"
ssu18_pime_ds@phy_tree <- NULL
ssu18_pime_ds
```

```{r, echo=FALSE, eval=TRUE}
tmp_ps <- ssu18_ps_work
tmp_ps@phy_tree <- NULL
tmp_ps
```

```{r}
ssu18_pime_sample_d <- data.frame(rowSums(otu_table(ssu18_pime_ds_full)))
ssu18_pime_sample_d <- ssu18_pime_sample_d %>% dplyr::rename(total_reads = 1)

ssu18_pime_ds <- rarefy_even_depth(ssu18_pime_ds_full,
                               sample.size = min(ssu18_pime_sample_d$total_reads),
                               trimOTUs = TRUE, replace = FALSE,
                               rngseed = 119)
```

```
2741 OTUs were removed because they are no longer 
present in any sample after random subsampling
```

```{r, echo=FALSE, eval=TRUE}
ssu18_pime_ds
```

The first step in PIME is to define if the microbial community presents a high relative abundance of taxa with low prevalence, which is considered as noise in PIME analysis. This is calculated by random forests analysis and is the baseline noise detection.

```{r}
ssu18_pime.oob.error <- pime.oob.error(ssu18_pime_ds, "TEMP")
```

```{r, echo=FALSE, eval=TRUE}
ssu18_pime.oob.error
```

## Split by Predictor Variable

```{r}
data.frame(sample_data(ssu18_pime_ds))
ssu18_per_variable_obj <- pime.split.by.variable(ssu18_pime_ds, "TEMP")
ssu18_per_variable_obj
```

```{r, echo=FALSE, eval=TRUE}
ssu18_per_variable_obj
```

## Calculate Prevalence Intervals

Using the output of `pime.split.by.variable`, we calculate the prevalence intervals with the function `pime.prevalence`. This function estimates the highest prevalence possible (no empty ASV table), calculates prevalence for taxa, starting at 5 maximum prevalence possible (no empty ASV table or dropping samples). After prevalence calculation, each prevalence interval are merged.

```{r}
ssu18_prevalences <- pime.prevalence(ssu18_per_variable_obj)
ssu18_prevalences
```

<details markdown="1">
<summary>Detailed results for all prevalences intervals  </summary>

```{r, echo=FALSE, eval=TRUE}
ssu18_prevalences
```
</details>

## Calculate Best Prevalence

Finally, we use the function `pime.best.prevalence` to calculate the best prevalence. The function uses randomForest to build random forests trees for samples classification and variable importance computation. It performs classifications for each prevalence interval returned by `pime.prevalence`. Variable importance is calculated, returning the Mean Decrease Accuracy (MDA), Mean Decrease Impurity (MDI), overall and by sample group, and taxonomy for each ASV. PIME keeps the top 30 variables with highest MDA each prevalence level.

```{r}
set.seed(1911)
ssu18_best.prev <- pime.best.prevalence(ssu18_prevalences, "TEMP")
```

```{r, echo=FALSE, eval=TRUE}
ssu18_best.prev$`OOB error`
```

```{r}
ssu18_what_is_best <- ssu18_best.prev$`OOB error`
ssu18_what_is_best[, c(2:4)] <- sapply(ssu18_what_is_best[, c(2:4)], as.numeric)
ssu18_what_is_best <- ssu18_what_is_best %>%
  dplyr::rename("OOB_error_rate" = "OOB error rate (%)")
ssu18_what_is_best$Interval <- str_replace_all(ssu18_what_is_best$Interval, "%", "")
ssu18_best <- with(ssu18_what_is_best, Interval[which.min(OOB_error_rate)])
ssu18_best <- paste("`", ssu18_best, "`", sep = "")
ssu18_prev_choice <- paste("ssu18_best.prev$`Importance`$", ssu18_best, sep = "")
ssu18_imp_best <- eval(parse(text = (ssu18_prev_choice)))
```

Looks like the lowest OOB error rate (%) that retains the most ASVs is `r min(ssu18_what_is_best$OOB_error_rate)`% from `r ssu18_best`. We will use this interval.

## Best Prevalence Summary

```{r, echo=FALSE}
ssu18_imp_best[, 13:14] <- list(NULL)
ssu18_imp_best <- ssu18_imp_best %>% dplyr::rename("ASV_ID" = "SequenceID")
```

</br>

```{r, layout='l-body-outset', echo=FALSE, eval=TRUE}
## elementId need to be unique https://www.random.org/strings/
ssu18_imp_best_t <- ssu18_imp_best %>%
  dplyr::rename("Control" = "X0", "Warm_3" = "X3", "Warm_8" = "X8")
datatable(ssu18_imp_best_t, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('Table of ASVs (max 30)
                                     from the chosen prevalence
                                     interval.')),
          elementId = "pf0wohnh74w860fku3z5",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = list(c(5, 10, -1), c("5", "10", "All"))
            )
          ) %>%
  DT::formatStyle(columns = colnames(ssu18_imp_best_t), fontSize = '80%') %>%
  DT::formatRound(columns = c("Control", "Warm_3", "Warm_8",
                              "MeanDecreaseAccuracy",
                              "MeanDecreaseGini" ), digits = 4)
```

Here is a list of the top 30 ASVs.

```{r, eval=TRUE, echo=FALSE}
ssu18_imp_best_t$ASV_ID
```

Now we need to create a phyloseq object of ASVs at this cutoff (`r ssu18_best`).

```{r}
ssu18_best_val <- str_replace_all(ssu18_best, "Prevalence ", "")
ssu18_best_val <- paste("ssu18_prevalences$", ssu18_best_val, sep = "")
ssu18_prevalence_best <- eval(parse(text = (ssu18_best_val)))
saveRDS(ssu18_prevalence_best, "files/pime/rdata/ssu18_prevalence_best.rds")
```

And look at a summary of the data.

```{r, echo=FALSE, eval=TRUE}
ssu18_prevalence_best
```

```{r, echo=FALSE}
ssu18_min_read_ps <- min(readcount(ssu18_prevalence_best))
ssu18_max_read_ps <- max(readcount(ssu18_prevalence_best))
ssu18_total_reads_ps <- sum(readcount(ssu18_prevalence_best))
ssu18_mean_reads_ps <- round(mean(readcount(ssu18_prevalence_best)), digits = 0)
ssu18_median_reads_ps <- median(readcount(ssu18_prevalence_best))
ssu18_total_asvs_ps <- ntaxa(ssu18_prevalence_best)
ssu18_singleton_ps <- tryCatch(ntaxa(rare(ssu18_prevalence_best,
                                    detection = 1, prevalence = 0)),
                         error=function(err) NA)
ssu18_singleton_ps_perc <- tryCatch(round((100*(ntaxa(rare(ssu18_prevalence_best,
                                                     detection = 1,
                                                     prevalence = 0)) / ntaxa(ps))),
                                    digits = 3),
                              error=function(err) NA)
ssu18_sparsity_ps <- round(length(which(
  abundances(ssu18_prevalence_best) == 0))/length(abundances(ssu18_prevalence_best)),
  digits = 3)
```

| Metric                              | Results                         |
|-------------------------------------|---------------------------------|
| Min. number of reads                | `r ssu18_min_read_ps`           |
| Max. number of reads                | `r ssu18_max_read_ps`           |
| Total number of reads               | `r ssu18_total_reads_ps`        |
| Average number of reads             | `r ssu18_mean_reads_ps`         |
| Mean number of reads                | `r ssu18_mean_reads_ps`         |
| Median number of reads              | `r ssu18_median_reads_ps`       |
| Total ASVS                          | `r ssu18_total_asvs_ps`         |
| Sparsity                            | `r ssu18_sparsity_ps`           |


## Estimate Error in Prediction

Using  the function `pime.error.prediction` we can estimate the error in prediction. For each prevalence interval, this function randomizes the samples labels into arbitrary groupings using `n` random permutations, defined by the `bootstrap` value. For each, randomized and prevalence filtered, data set the OOB error rate is calculated to estimate whether the original differences in groups of samples occur by chance. Results are in a list containing a table and a box plot summarizing the results.

```{r}
ssu18_randomized <- pime.error.prediction(ssu18_pime_ds, "TEMP",
                                          bootstrap = 100, parallel = TRUE,
                                          max.prev = 95)
```

<br/>

```{r, echo=FALSE}
ssu18_oob_error <- ssu18_randomized$`Results table`
```

```{r, echo=FALSE, layout='l-body', echo=FALSE, eval=TRUE}
datatable(ssu18_oob_error, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('Table of ASV from the chosen
                                     prevalence interval.')),
          elementId = "x4rm71xhgtrgjnoldisj",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = list(c(5, 10, -1), c("5", "10", "All"))
            )
          ) %>%
  DT::formatStyle(columns = colnames(ssu18_oob_error), fontSize = '80%') %>%
  DT::formatRound(columns = 1:ncol(ssu18_oob_error), digits = 4)
```

```{r, echo=FALSE}
ssu18_randomized$Plot
```

It is also possible to estimate the variation of OOB error for each prevalence interval filtering. This is done by running the random forests classification for `n` times, determined by the `bootstrap` value. The function will return a box plot figure and a table for each classification error.

```{r}
ssu18_replicated.oob.error <- pime.oob.replicate(ssu18_prevalences, "TEMP",
                                         bootstrap = 100, parallel = TRUE)
```

```{r, echo=FALSE, eval=TRUE, fig.height=5}
ssu18_obb_orig <- ssu18_replicated.oob.error$Plot
ssu18_obb_rand <- ssu18_randomized$Plot

ssu18_obb_orig <- ssu18_obb_orig +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank()) +
  labs(title = "OOB error Original data set")

ssu18_obb_rand <- ssu18_obb_rand  +
  labs(title = "OOB error Randomized data set")
ssu18_obb_orig / ssu18_obb_rand
```

To obtain the confusion matrix from random forests classification use the following:

```{r}
ssu18_prev_confuse <- paste("ssu18_best.prev$`Confusion`$", ssu18_best, sep = "")
eval(parse(text = (ssu18_prev_confuse)))
```

## Save Phyloseq PIME objects

```{r}
ssu18_ps_pime <- ssu18_prevalence_best
ssu18_ps_pime_tree <- rtree(ntaxa(ssu18_ps_pime), rooted = TRUE,
                           tip.label = taxa_names(ssu18_ps_pime))
ssu18_ps_pime <- merge_phyloseq(ssu18_ps_pime,
                               sample_data,
                               ssu18_ps_pime_tree)
saveRDS(ssu18_ps_pime, "files/pime/rdata/ssu18_ps_pime.rds")
```

### Split & save by predictor variable

```{r}
data.frame(sample_data(ssu18_ps_pime))
ssu18_ps_pime_split <- pime.split.by.variable(ssu18_ps_pime, "TEMP")
saveRDS(ssu18_ps_pime_split$`0`, "files/pime/rdata/ssu18_ps_pime_0.rds")
saveRDS(ssu18_ps_pime_split$`3`, "files/pime/rdata/ssu18_ps_pime_3.rds")
saveRDS(ssu18_ps_pime_split$`8`, "files/pime/rdata/ssu18_ps_pime_8.rds")
```

```{r, echo=FALSE, eval=TRUE}
ssu18_ps_pime_split
#ssu18_ps_pime_split$`0`
#ssu18_ps_pime_split$`3`
#ssu18_ps_pime_split$`8`
```

```{r, echo=FALSE}
ssu18_pime_preval.tax <- tax_table(ssu18_ps_pime)
write.table(ssu18_pime_preval.tax,
            file="files/pime/tables/ssu18_asv_PIME_tax_table.txt",
            sep = "\t", quote = FALSE)

ssu18_pime_preval.asv <- otu_table(t(ssu18_ps_pime))
write.table(ssu18_pime_preval.asv,
            file="files/pime/tables/ssu18_asv_PIME_otu_table.txt",
            sep = "\t", quote = FALSE)

ssu18_pime_preval.samp <- sample_data(ssu18_ps_pime)
write.table(ssu18_pime_preval.samp,
            file="files/pime/tables/ssu18_asv_PIME_sample_data.txt",
            sep = "\t", quote = FALSE)
```

## Create Ampvis2 PIME Object

```{r, code_folding=TRUE}
ssu18_otu <- data.frame(t(otu_table(ssu18_ps_pime)))
ssu18_otu[] <- lapply(ssu18_otu, as.numeric)
ssu18_otu <- as.matrix(ssu18_otu)
ssu18_tax <- as.matrix(data.frame(tax_table(ssu18_ps_pime)))
ssu18_samples <- data.frame(sample_data(ssu18_ps_pime))
ssu18_ps_amp <- merge_phyloseq(otu_table(ssu18_otu, taxa_are_rows = TRUE),
                          tax_table(ssu18_tax, ssu18_tax),
                          sample_data(ssu18_samples))
```

```{r, code_folding=TRUE}
ssu18_amp_asv_pime  <- data.frame(otu_table(ssu18_ps_amp))
ssu18_amp_asv_pime <- ssu18_amp_asv_pime %>% tibble::rownames_to_column("OTU")
ssu18_amp_tax_pime  <- data.frame(tax_table(ssu18_ps_amp))
ssu18_amp_tax_pime <- ssu18_amp_tax_pime %>% tibble::rownames_to_column("OTU")
ssu18_amp_tax_pime$ASV_SEQ <- NULL
colnames(ssu18_amp_tax_pime)[colnames(ssu18_amp_tax_pime) == "ASV_ID"] <- "Species"
ssu18_amp_asv_tax_pime <- left_join(ssu18_amp_asv_pime, ssu18_amp_tax_pime, by = "OTU")
ssu18_samp_data_t <- ssu18_samp_data
ssu18_samp_data_t[,7:9] <- NULL
ssu18_amp_data_pime <- amp_load(ssu18_amp_asv_tax_pime, metadata = ssu18_samp_data_t)

ssu18_diversity_pime <-
  amp_alphadiv(
    ssu18_amp_data_pime,
    measure = "observed",
    richness = FALSE)

ssu18_diversity_pime <-
  ssu18_diversity_pime %>%
  dplyr::rename(
    "total_reads" = "Reads",
    "total_asvs" = "ObservedOTUs")

ssu18_diversity_pime$coverage = cut(ssu18_diversity_pime$total_reads,
                          c(0, 5000, 25000, 100000, 200000),
                          labels = c(
                            "low (< 5k)", "medium (5-25k)",
                            "high (25-100k)", "extra_high (> 100k)"))
ssu18_diversity_pime <- ssu18_diversity_pime[order(ssu18_diversity_pime$sample_id), ]
ssu18_amp_pime <- amp_load(ssu18_amp_asv_tax_pime,
                          metadata = ssu18_diversity_pime, tree = ssu18_ps_pime_tree)
ssu18_samp_pime <- ssu18_diversity_pime
ssu18_samp_pime <- ssu18_samp_pime %>% tibble::remove_rownames()
```

```{r, eval=TRUE, echo=FALSE}
ssu18_amp_pime
```

```{r, echo=FALSE}
saveRDS(ssu18_amp_pime, "files/pime/rdata/ssu18_amp_pime.rds")
```

## Summary

Let's take a look at a table of sample information. Any header with the `_p` suffix is the *PIME filtered* data.

<br/>

```{r, echo=FALSE}
## This is for table only
ssu18_dummy_tab <- ssu18_samp_pime
ssu18_dummy_tab <- ssu18_dummy_tab %>%
  dplyr::rename("total_reads_p" = "total_reads") %>%
  dplyr::rename("total_asvs_p" = "total_asvs")
ssu18_dummy_tab$coverage <- NULL
#ssu18_samp_data_tab <- dplyr::left_join(ssu18_samp_data, ssu18_dummy_tab)
#ssu18_samp_data_tab <- ssu18_samp_data_tab[, c(1:7,13,8,14,9,15,10,16,11,17,12)]
```

```{r, echo=FALSE, layout="l-body-outset", eval=TRUE}
## elementId https://www.random.org/strings/
ssu18_samp_data_tab_2 <- ssu18_dummy_tab

datatable(ssu18_samp_data_tab_2, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('Sample summary table.
            Use the buttons to navigate through the table or
            download a copy.')),
          elementId = "k9rn0xxltiqkg0bowglj",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = c(5, 15)
            )
          ) %>%
    DT::formatStyle(columns = colnames(ssu18_samp_data_tab_2),
                    fontSize = '80%')
```

And here is how the data sets changed through the PIME filtering process.

```{r, echo=FALSE, eval=TRUE}
ssu18_ps_pime_0 <- ssu18_ps_pime_split$`0`
ssu18_ps_pime_3 <- ssu18_ps_pime_split$`3`
ssu18_ps_pime_8 <- ssu18_ps_pime_split$`8`
tmp_objects <- data.frame(c("FULL data set", "Rarefied data", "PIME filtered data",
                            "PIME (0C)", "PIME (3C)", "PIME (8C)"))
tmp_samples <- c("ssu18_ps_work", "ssu18_pime_ds", "ssu18_ps_pime",
                 "ssu18_ps_pime_0", "ssu18_ps_pime_3", "ssu18_ps_pime_8")
tmp_no_samp <- c()
for (i in tmp_samples) {
   tmp_get <- nsamples(get(i))
   tmp_no_samp <- c(append(tmp_no_samp, tmp_get))
}
tmp_no_samp <- data.frame(tmp_no_samp)

tmp_rc <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_rc <- c(append(tmp_rc, tmp_get))
}
tmp_rc <- data.frame(tmp_rc)
tmp_asv <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_asv <- c(append(tmp_asv, tmp_get))
}
tmp_asv <- data.frame(tmp_asv)

ssu18_asv_pime_sum <- dplyr::bind_cols(tmp_objects, tmp_samples) %>%
                      dplyr::bind_cols(., tmp_no_samp) %>%
                      dplyr::bind_cols(., tmp_rc) %>%
                      dplyr::bind_cols(., tmp_asv) %>%
  dplyr::rename("Description" = 1, "object name" = 2, "no. samples" = 3,
                "total reads" = 4, "total asvs" = 5)
rm(list = ls(pattern = "tmp_"))
```

<br/>

```{r, echo=FALSE, layout="l-body-outset", eval=TRUE}
knitr::kable(ssu18_asv_pime_sum)
```


```{r, echo=FALSE}
saveRDS(ssu18_samp_pime, "files/pime/rdata/ssu18_samp_pime.rds")
save.image("page_build/pime_ssu18_asv_wf.rdata")
```

```{r include=FALSE, eval=TRUE}
gdata::keep(ssu18_asv_pime_sum, sure = TRUE)
```

# ITS ASV Data Set

```{r, include=FALSE}
## Load for original ANALYSIS ONLY #1
remove(list = ls())
set.seed(119)
its18_ps_work <- readRDS("files/data-prep/rdata/its18_ps_work.rds")
its18_ps_work_o <- readRDS("files/data-prep/rdata/its18_ps_work_o.rds")
its18_samp_data <- readRDS("files/data-prep/rdata/its18_samp_data.rds")
its18_amp_data <- readRDS("files/data-prep/rdata/its18_amp_data.rds")
its18_ps_data <- readRDS("files/data-prep/rdata/its18_ps_amp.rds")
its18_ps_data_tree <- rtree(ntaxa(its18_ps_data), rooted = TRUE,
                           tip.label = taxa_names(its18_ps_data))
its18_ps_data <- merge_phyloseq(its18_ps_data, sample_data, its18_ps_data_tree)
```

```{r, include=FALSE}
## LOad only after workflow is complete #2
remove(list = ls())
load("files/pime/rdata/pime_its18_asv.rdata")
its18_amp_data <- readRDS("files/data-prep/rdata/its18_amp_data.rds")
rm(list = ls(pattern = "ssu18_"))
```

```{r, include=FALSE, eval=TRUE}
## Load to build page ONLY #3
load("page_build/pime_its18_asv_wf.rdata")
```

We will use PIME (Prevalence Interval for Microbiome Evaluation) to create a filtered data set.

## Setup

First, choose a phyloseq object and a sample data frame

```{r}
its18_pime_ds <- its18_ps_work
its18_which_pime <- "its18_pime_ds"
its18_pime_ds@phy_tree <- NULL
```

```{r, echo=FALSE, eval=TRUE}
its18_ps_work
```

```{r}
its18_pime_sample_d <- data.frame(rowSums(otu_table(its18_pime_ds_full)))
its18_pime_sample_d <- its18_pime_sample_d %>% dplyr::rename(total_reads = 1)

its18_pime_ds <- rarefy_even_depth(its18_pime_ds_full,
                               sample.size = min(its18_pime_sample_d$total_reads),
                               trimOTUs = TRUE, replace = FALSE,
                               rngseed = 119)
```

```
298 OTUs were removed because they are no longer 
present in any sample after random subsampling
```

```{r, echo=FALSE, eval=TRUE}
its18_pime_ds
```

The first step in PIME is to define if the microbial community presents a high relative abundance of taxa with low prevalence, which is considered as noise in PIME analysis. This is calculated by random forests analysis and is the baseline noise detection.

```{r}
its18_pime.oob.error <- pime.oob.error(its18_pime_ds, "TEMP")
```

```{r, echo=FALSE, eval=TRUE}
its18_pime.oob.error
```

## Split by Predictor Variable

```{r}
data.frame(sample_data(its18_pime_ds))
its18_per_variable_obj <- pime.split.by.variable(its18_pime_ds, "TEMP")
its18_per_variable_obj
```

```{r, echo=FALSE, eval=TRUE}
its18_per_variable_obj
```

## Calculate Prevalence Intervals

Using the output of `pime.split.by.variable`, we calculate the prevalence intervals with the function `pime.prevalence`. This function estimates the highest prevalence possible (no empty ASV table), calculates prevalence for taxa, starting at 5 maximum prevalence possible (no empty ASV table or dropping samples). After prevalence calculation, each prevalence interval are merged.

```{r}
its18_prevalences <- pime.prevalence(its18_per_variable_obj)
its18_prevalences
```

<details markdown="1">
<summary>Detailed results for all prevalences intervals  </summary>

```{r, echo=FALSE, eval=TRUE}
its18_prevalences
```
</details>

## Calculate Best Prevalence

Finally, we use the function `pime.best.prevalence` to calculate the best prevalence. The function uses randomForest to build random forests trees for samples classification and variable importance computation. It performs classifications for each prevalence interval returned by `pime.prevalence`. Variable importance is calculated, returning the Mean Decrease Accuracy (MDA), Mean Decrease Impurity (MDI), overall and by sample group, and taxonomy for each ASV. PIME keeps the top 30 variables with highest MDA each prevalence level.

```{r}
set.seed(1911)
its18_best.prev <- pime.best.prevalence(its18_prevalences, "TEMP")
```

```{r, echo=FALSE, eval=TRUE}
its18_best.prev$`OOB error`
```

```{r}
its18_what_is_best <- its18_best.prev$`OOB error`
its18_what_is_best[, c(2:4)] <- sapply(its18_what_is_best[, c(2:4)], as.numeric)
its18_what_is_best <- its18_what_is_best %>%
  dplyr::rename("OOB_error_rate" = "OOB error rate (%)")
its18_what_is_best$Interval <- str_replace_all(its18_what_is_best$Interval, "%", "")
its18_best <- with(its18_what_is_best, Interval[which.min(OOB_error_rate)])
its18_best <- paste("`", its18_best, "`", sep = "")
its18_prev_choice <- paste("its18_best.prev$`Importance`$", its18_best, sep = "")
its18_imp_best <- eval(parse(text = (its18_prev_choice)))
```

Looks like the lowest OOB error rate (%) that retains the most ASVs is `r min(its18_what_is_best$OOB_error_rate)`% from `r its18_best`. We will use this interval.

## Best Prevalence Summary

```{r, echo=FALSE}
its18_imp_best[, 13:14] <- list(NULL)
its18_imp_best <- its18_imp_best %>% dplyr::rename("ASV_ID" = "SequenceID")
```

</br>

```{r, layout='l-body-outset', echo=FALSE, eval=TRUE}
## elementId need to be unique https://www.random.org/strings/
its18_imp_best_t <- its18_imp_best %>%
  dplyr::rename("Control" = "X0", "Warm_3" = "X3", "Warm_8" = "X8")

datatable(its18_imp_best_t, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('Table of ASVs (max 30)
                                     from the chosen prevalence
                                     interval.')),
          elementId = "nqv1m1dflzfc7qupdsdp",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = list(c(5, 10, -1), c("5", "10", "All"))
            )
          ) %>%
  DT::formatStyle(columns = colnames(its18_imp_best_t), fontSize = '80%') %>%
  DT::formatRound(columns = c("Control", "Warm_3", "Warm_8",
                              "MeanDecreaseAccuracy",
                              "MeanDecreaseGini" ), digits = 4)
```


Here is a list of the top 30 ASVs.

```{r, eval=TRUE, echo=FALSE}
its18_imp_best_t$ASV_ID
```

Now we need to create a phyloseq object of ASVs at this cutoff (`r its18_best`).

```{r}
its18_best_val <- str_replace_all(its18_best, "Prevalence ", "")
its18_best_val <- paste("its18_prevalences$", its18_best_val, sep = "")
its18_prevalence_best <- eval(parse(text = (its18_best_val)))
saveRDS(its18_prevalence_best, "files/pime/rdata/its18_asv_prevalence_best.rds")
```

And look at a summary of the data.

```{r, echo=FALSE, eval=TRUE}
its18_prevalence_best
```

```{r, echo=FALSE}
its18_min_read_ps <- min(readcount(its18_prevalence_best))
its18_max_read_ps <- max(readcount(its18_prevalence_best))
its18_total_reads_ps <- sum(readcount(its18_prevalence_best))
its18_mean_reads_ps <- round(mean(readcount(its18_prevalence_best)), digits = 0)
its18_median_reads_ps <- median(readcount(its18_prevalence_best))
its18_total_asvs_ps <- ntaxa(its18_prevalence_best)
its18_singleton_ps <- tryCatch(ntaxa(rare(its18_prevalence_best,
                                    detection = 1, prevalence = 0)),
                         error=function(err) NA)
its18_singleton_ps_perc <- tryCatch(round((100*(ntaxa(rare(its18_prevalence_best,
                                                     detection = 1,
                                                     prevalence = 0)) / ntaxa(ps))),
                                    digits = 3),
                              error=function(err) NA)
its18_sparsity_ps <- round(length(which(
  abundances(its18_prevalence_best) == 0))/length(abundances(its18_prevalence_best)),
  digits = 3)
```

| Metric                              | Results                         |
|-------------------------------------|---------------------------------|
| Min. number of reads                | `r its18_min_read_ps`           |
| Max. number of reads                | `r its18_max_read_ps`           |
| Total number of reads               | `r its18_total_reads_ps`        |
| Average number of reads             | `r its18_mean_reads_ps`         |
| Mean number of reads                | `r its18_mean_reads_ps`         |
| Median number of reads              | `r its18_median_reads_ps`       |
| Total ASVS                          | `r its18_total_asvs_ps`         |
| Sparsity                            | `r its18_sparsity_ps`           |


## Estimate Error in Prediction

Using  the function `pime.error.prediction` we can estimate the error in prediction. For each prevalence interval, this function randomizes the samples labels into arbitrary groupings using `n` random permutations, defined by the `bootstrap` value. For each, randomized and prevalence filtered, data set the OOB error rate is calculated to estimate whether the original differences in groups of samples occur by chance. Results are in a list containing a table and a box plot summarizing the results.

```{r}
its18_randomized <- pime.error.prediction(its18_pime_ds, "TEMP",
                                          bootstrap = 100, parallel = TRUE,
                                          max.prev = 95)
```

<br/>

```{r, echo=FALSE}
its18_oob_error <- its18_randomized$`Results table`
```

```{r, echo=FALSE, layout='l-body', echo=FALSE, eval=TRUE}
datatable(its18_oob_error, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('Table of ASV from the chosen
                                     prevalence interval.')),
          elementId = "spzk4b6i230umwig8jaw",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = list(c(5, 10, -1), c("5", "10", "All"))
            )
          ) %>%
  DT::formatStyle(columns = colnames(its18_oob_error), fontSize = '80%') %>%
  DT::formatRound(columns = 1:ncol(its18_oob_error), digits = 4)
```

```{r, echo=FALSE}
its18_randomized$Plot
```

It is also possible to estimate the variation of OOB error for each prevalence interval filtering. This is done by running the random forests classification for `n` times, determined by the `bootstrap` value. The function will return a box plot figure and a table for each classification error.

```{r}
its18_replicated.oob.error <- pime.oob.replicate(its18_prevalences, "TEMP",
                                         bootstrap = 100, parallel = TRUE)
```

```{r, echo=FALSE, eval=TRUE, fig.height=5}
its18_obb_orig <- its18_replicated.oob.error$Plot
its18_obb_rand <- its18_randomized$Plot

its18_obb_orig <- its18_obb_orig +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank()) +
  labs(title = "OOB error Original data set")

its18_obb_rand <- its18_obb_rand  +
  labs(title = "OOB error Randomized data set")
its18_obb_orig / its18_obb_rand
```

To obtain the confusion matrix from random forests classification use the following:

```{r}
its18_prev_confuse <- paste("its18_best.prev$`Confusion`$", its18_best, sep = "")
eval(parse(text = (its18_prev_confuse)))
```

## Save Phyloseq PIME objects

```{r}
its18_ps_pime <- its18_prevalence_best
its18_ps_pime_tree <- rtree(ntaxa(its18_ps_pime), rooted = TRUE,
                           tip.label = taxa_names(its18_ps_pime))
its18_ps_pime <- merge_phyloseq(its18_ps_pime,
                               sample_data,
                               its18_ps_pime_tree)
saveRDS(its18_ps_pime, "files/pime/rdata/its18_ps_pime.rds")
```

### Split & save by predictor variable

```{r}
data.frame(sample_data(its18_ps_pime))
its18_ps_pime_split <- pime.split.by.variable(its18_ps_pime, "TEMP")
saveRDS(its18_ps_pime_split$`0`, "files/pime/rdata/its18_ps_pime_0.rds")
saveRDS(its18_ps_pime_split$`3`, "files/pime/rdata/its18_ps_pime_3.rds")
saveRDS(its18_ps_pime_split$`8`, "files/pime/rdata/its18_ps_pime_8.rds")
```

```{r, echo=FALSE, eval=TRUE}
its18_ps_pime_split
#its18_ps_pime_split$`0`
#its18_ps_pime_split$`4`
#its18_ps_pime_split$`8`
```

```{r, echo=FALSE}
its18_pime_preval.tax <- tax_table(its18_ps_pime)
write.table(its18_pime_preval.tax,
            file="files/pime/tables/its18_asv_PIME_tax_table.txt",
            sep = "\t", quote = FALSE)

its18_pime_preval.asv <- otu_table(t(its18_ps_pime))
write.table(its18_pime_preval.asv,
            file="files/pime/tables/its18_asv_PIME_otu_table.txt",
            sep = "\t", quote = FALSE)

its18_pime_preval.samp <- sample_data(its18_ps_pime)
write.table(its18_pime_preval.samp,
            file="files/pime/tables/its18_asv_PIME_sample_data.txt",
            sep = "\t", quote = FALSE)
```

## Create Ampvis2 PIME Object

```{r, code_folding=TRUE}
its18_otu <- data.frame(t(otu_table(its18_ps_pime)))
its18_otu[] <- lapply(its18_otu, as.numeric)
its18_otu <- as.matrix(its18_otu)
its18_tax <- as.matrix(data.frame(tax_table(its18_ps_pime)))
its18_samples <- data.frame(sample_data(its18_ps_pime))
its18_ps_amp <- merge_phyloseq(otu_table(its18_otu, taxa_are_rows = TRUE),
                          tax_table(its18_tax, its18_tax),
                          sample_data(its18_samples))
```

```{r, code_folding=TRUE}
its18_amp_asv_pime  <- data.frame(otu_table(its18_ps_amp))
its18_amp_asv_pime <- its18_amp_asv_pime %>% tibble::rownames_to_column("OTU")
its18_amp_tax_pime  <- data.frame(tax_table(its18_ps_amp))
its18_amp_tax_pime <- its18_amp_tax_pime %>% tibble::rownames_to_column("OTU")
its18_amp_tax_pime$ASV_SEQ <- NULL
colnames(its18_amp_tax_pime)[colnames(its18_amp_tax_pime) == "ASV_ID"] <- "Species"
its18_amp_asv_tax_pime <- left_join(its18_amp_asv_pime, its18_amp_tax_pime, by = "OTU")
its18_samp_data_t <- its18_samp_data
its18_samp_data_t[,7:12] <- NULL
its18_amp_data_pime <- amp_load(its18_amp_asv_tax_pime, metadata = its18_samp_data_t)

its18_diversity_pime <-
  amp_alphadiv(
    its18_amp_data_pime,
    measure = "observed",
    richness = FALSE)

its18_diversity_pime <-
  its18_diversity_pime %>%
  dplyr::rename(
    "total_reads" = "Reads",
    "total_asvs" = "ObservedOTUs")

its18_diversity_pime$coverage = cut(its18_diversity_pime$total_reads,
                          c(0, 5000, 25000, 100000, 200000),
                          labels = c(
                            "low (< 5k)", "medium (5-25k)",
                            "high (25-100k)", "extra_high (> 100k)"))
its18_diversity_pime <- its18_diversity_pime[order(its18_diversity_pime$sample_id), ]
its18_amp_pime <- amp_load(its18_amp_asv_tax_pime,
                          metadata = its18_diversity_pime, tree = its18_ps_pime_tree)
its18_samp_pime <- its18_diversity_pime
its18_samp_pime <- its18_samp_pime %>% tibble::remove_rownames()
```

```{r, eval=TRUE, echo=FALSE}
its18_amp_pime
```

```{r, echo=FALSE}
saveRDS(its18_amp_pime, "files/pime/rdata/its18_amp_pime.rds")
```

## Summary

Now, let's take a look at a table of sample information. Any header with the `_p` suffix is the *PIME filtered* data.

<br/>

```{r, echo=FALSE}
## elementId https://www.random.org/strings/
## This is for table only
its18_dummy_tab <- its18_samp_pime
its18_dummy_tab <- its18_dummy_tab %>%
  dplyr::rename("total_reads_p" = "total_reads") %>%
  dplyr::rename("total_asvs_p" = "total_asvs")
its18_dummy_tab$coverage <- NULL
#its18_samp_data_tab <- dplyr::left_join(its18_samp_data, its18_dummy_tab)
#its18_samp_data_tab <- its18_samp_data_tab[, c(1:7,13,8,14,9,15,10,16,11,17,12)]
```

```{r, echo=FALSE, layout="l-body-outset", eval=TRUE}
## elementId https://www.random.org/strings/
its18_samp_data_tab_2 <- its18_dummy_tab
datatable(its18_samp_data_tab_2, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('Sample summary table.
            Use the buttons to navigate through the table or
            download a copy.')),
          elementId = "gb5viunhejdj8523gp3c",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = c(5, 15)
            )
          ) %>%
    DT::formatStyle(columns = colnames(its18_samp_data_tab_2),
                    fontSize = '80%')
```

And here is how the data sets changed through the PIME filtering process.

```{r, echo=FALSE, eval=TRUE}
its18_ps_pime_0 <- its18_ps_pime_split$`0`
its18_ps_pime_3 <- its18_ps_pime_split$`3`
its18_ps_pime_8 <- its18_ps_pime_split$`8`
tmp_objects <- data.frame(c("FULL data set", "Rarefied data", "PIME filtered data",
                            "PIME (0C)", "PIME (3C)", "PIME (8C)"))
tmp_samples <- c("its18_ps_work", "its18_pime_ds", "its18_ps_pime",
                 "its18_ps_pime_0", "its18_ps_pime_3", "its18_ps_pime_8")
tmp_no_samp <- c()
for (i in tmp_samples) {
   tmp_get <- nsamples(get(i))
   tmp_no_samp <- c(append(tmp_no_samp, tmp_get))
}
tmp_no_samp <- data.frame(tmp_no_samp)

tmp_rc <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_rc <- c(append(tmp_rc, tmp_get))
}
tmp_rc <- data.frame(tmp_rc)
tmp_asv <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_asv <- c(append(tmp_asv, tmp_get))
}
tmp_asv <- data.frame(tmp_asv)

its18_asv_pime_sum <- dplyr::bind_cols(tmp_objects, tmp_samples) %>%
                      dplyr::bind_cols(., tmp_no_samp) %>%
                      dplyr::bind_cols(., tmp_rc) %>%
                      dplyr::bind_cols(., tmp_asv) %>%
  dplyr::rename("Description" = 1, "object name" = 2, "no. samples" = 3,
                "total reads" = 4, "total asvs" = 5)
rm(list = ls(pattern = "tmp_"))
```

<br/>

```{r, echo=FALSE, layout="l-body-outset", eval=TRUE}
knitr::kable(its18_asv_pime_sum)
```

```{r, echo=FALSE}
saveRDS(its18_samp_pime, "files/pime/rdata/its18_samp_pime.rds")
save.image("page_build/pime_its18_asv_wf.rdata")
```

```{r include=FALSE, eval=TRUE}
gdata::keep(ssu18_asv_pime_sum, its18_asv_pime_sum, sure = TRUE)
```

# 16s rRNA OTU Data Set

```{r, include=FALSE, eval=TRUE}
## Load to build page only #3
load("page_build/pime_ssu18_otu_wf.rdata")
```

```{r, include=FALSE}
## LOad only after workflow is complete #2
remove(list = ls())
load("files/pime/rdata/pime_ssu18_otu.rdata")
ssu18_samp_data <- readRDS("files/data-prep/rdata/ssu18_samp_data.rds")
ssu18_samp_data[, 7:9] <- NULL
rm(list = ls(pattern = "its18_"))
```

```{r include=FALSE}
## Initial Load for PIME  ANALYSIS ONLY #1
remove(list = ls())
set.seed(119)
ssu18_ps_work_otu <- readRDS("files/otu/rdata/ssu18_ps_work_otu.rds")
ssu18_work_amp_otu <- readRDS("files/otu/rdata/ssu18_ps_work_otu_amp.rds")
```

We will use PIME (Prevalence Interval for Microbiome Evaluation) to create a filtered data set.

## Setup

First, choose a phyloseq object and a sample data frame

```{r}
ssu18_pime_ds <- ssu18_ps_work_otu
ssu18_which_pime <- "ssu18_pime_ds"
ssu18_pime_ds@phy_tree <- NULL
ssu18_pime_ds
```

```{r, echo=FALSE, eval=TRUE}
ssu18_ps_work_otu
```

```{r}
ssu18_pime_sample_d <- data.frame(rowSums(otu_table(ssu18_pime_ds)))
ssu18_pime_sample_d <- ssu18_pime_sample_d %>% dplyr::rename(total_reads = 1)

ssu18_pime_ds <- rarefy_even_depth(ssu18_pime_ds,
                             sample.size = min(ssu18_pime_sample_d$total_reads),
                             trimOTUs = TRUE, replace = FALSE, rngseed = 119)
```

```
2397 OTUs were removed because they are no longer 
present in any sample after random subsampling
```

```{r, echo=FALSE, eval=TRUE}
ssu18_pime_ds
```

The first step in PIME is to define if the microbial community presents a high relative abundance of taxa with low prevalence, which is considered as noise in PIME analysis. This is calculated by random forests analysis and is the baseline noise detection.

```{r}
ssu18_pime.oob.error <- pime.oob.error(ssu18_pime_ds, "TEMP")
```

```{r, echo=FALSE, eval=TRUE}
ssu18_pime.oob.error
```

## Split by Predictor Variable

```{r}
data.frame(sample_data(ssu18_pime_ds))
ssu18_per_variable_obj <- pime.split.by.variable(ssu18_pime_ds, "TEMP")
ssu18_per_variable_obj
```

```{r, echo=FALSE, eval=TRUE}
ssu18_per_variable_obj
```

## Calculate Prevalence Intervals

Using the output of `pime.split.by.variable`, we calculate the prevalence intervals with the function `pime.prevalence`. This function estimates the highest prevalence possible (no empty ASV table), calculates prevalence for taxa, starting at 5 maximum prevalence possible (no empty ASV table or dropping samples). After prevalence calculation, each prevalence interval are merged.

```{r}
ssu18_prevalences <- pime.prevalence(ssu18_per_variable_obj)
ssu18_prevalences
```

<details markdown="1">
<summary>Detailed results for all prevalences intervals  </summary>

```{r, echo=FALSE, eval=TRUE}
ssu18_prevalences
```
</details>

## Calculate Best Prevalence

Finally, we use the function `pime.best.prevalence` to calculate the best prevalence. The function uses randomForest to build random forests trees for samples classification and variable importance computation. It performs classifications for each prevalence interval returned by `pime.prevalence`. Variable importance is calculated, returning the Mean Decrease Accuracy (MDA), Mean Decrease Impurity (MDI), overall and by sample group, and taxonomy for each ASV. PIME keeps the top 30 variables with highest MDA each prevalence level.

```{r}
set.seed(1911)
ssu18_best.prev <- pime.best.prevalence(ssu18_prevalences, "TEMP")
```

```{r, echo=FALSE, eval=TRUE}
ssu18_best.prev$`OOB error`
```

```{r}
ssu18_what_is_best <- ssu18_best.prev$`OOB error`
ssu18_what_is_best[, c(2:4)] <- sapply(ssu18_what_is_best[, c(2:4)], as.numeric)
ssu18_what_is_best <- ssu18_what_is_best %>%
  dplyr::rename("OOB_error_rate" = "OOB error rate (%)")
ssu18_what_is_best$Interval <- str_replace_all(ssu18_what_is_best$Interval, "%", "")
ssu18_best <- with(ssu18_what_is_best, Interval[which.min(OOB_error_rate)])
ssu18_best <- paste("`", ssu18_best, "`", sep = "")
ssu18_prev_choice <- paste("ssu18_best.prev$`Importance`$", ssu18_best, sep = "")
ssu18_imp_best <- eval(parse(text = (ssu18_prev_choice)))
```

Looks like the lowest OOB error rate (%) that retains the most ASVs is `r min(ssu18_what_is_best$OOB_error_rate)`% from `r ssu18_best`. We will use this interval.

## Best Prevalence Summary

```{r, echo=FALSE}
ssu18_imp_best[, 13:14] <- list(NULL)
ssu18_imp_best <- ssu18_imp_best %>% dplyr::rename("OTU_ID" = "SequenceID")
```

</br>

```{r, layout='l-body-outset', echo=FALSE, eval=TRUE}
## elementId need to be unique https://www.random.org/strings/
ssu18_imp_best_t <- ssu18_imp_best %>%
  dplyr::rename("Control" = "X0", "Warm_3" = "X3", "Warm_8" = "X8")

datatable(ssu18_imp_best_t, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('Table of OTUs (max 30)
                                     from the chosen prevalence
                                     interval.')),
          elementId = "8snt4r54kcbnxq67qb5l",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = list(c(5, 10, -1), c("5", "10", "All"))
            )
          ) %>%
  DT::formatStyle(columns = colnames(ssu18_imp_best_t), fontSize = '80%') %>%
  DT::formatRound(columns = c("Control", "Warm_3", "Warm_8",
                              "MeanDecreaseAccuracy",
                              "MeanDecreaseGini" ), digits = 4)
```


Here is a list of the top 30 OTUs

```{r, eval=TRUE, echo=FALSE}
ssu18_imp_best_t$ASV_ID
```

Now we need to create a phyloseq object of ASVs at this cutoff (`r ssu18_best`).

```{r}
ssu18_best_val <- str_replace_all(ssu18_best, "Prevalence ", "")
ssu18_best_val <- paste("ssu18_prevalences$", ssu18_best_val, sep = "")
ssu18_prevalence_best <- eval(parse(text = (ssu18_best_val)))
saveRDS(ssu18_prevalence_best, "files/pime/rdata/ssu18_otu_prevalence_best.rds")
```

And look at a summary of the data.

```{r, echo=FALSE, eval=TRUE}
ssu18_prevalence_best
```

```{r, echo=FALSE}
ssu18_min_read_ps <- min(readcount(ssu18_prevalence_best))
ssu18_max_read_ps <- max(readcount(ssu18_prevalence_best))
ssu18_total_reads_ps <- sum(readcount(ssu18_prevalence_best))
ssu18_mean_reads_ps <- round(mean(readcount(ssu18_prevalence_best)), digits = 0)
ssu18_median_reads_ps <- median(readcount(ssu18_prevalence_best))
ssu18_total_asvs_ps <- ntaxa(ssu18_prevalence_best)
ssu18_singleton_ps <- tryCatch(ntaxa(rare(ssu18_prevalence_best,
                                    detection = 1, prevalence = 0)),
                         error=function(err) NA)
ssu18_singleton_ps_perc <- tryCatch(round((100*(ntaxa(rare(ssu18_prevalence_best,
                                                     detection = 1,
                                                     prevalence = 0)) / ntaxa(ps))),
                                    digits = 3),
                              error=function(err) NA)
ssu18_sparsity_ps <- round(length(which(
  abundances(ssu18_prevalence_best) == 0))/length(abundances(ssu18_prevalence_best)),
  digits = 3)
```

| Metric                              | Results                         |
|-------------------------------------|---------------------------------|
| Min. number of reads                | `r ssu18_min_read_ps`           |
| Max. number of reads                | `r ssu18_max_read_ps`           |
| Total number of reads               | `r ssu18_total_reads_ps`        |
| Average number of reads             | `r ssu18_mean_reads_ps`         |
| Mean number of reads                | `r ssu18_mean_reads_ps`         |
| Median number of reads              | `r ssu18_median_reads_ps`       |
| Total ASVS                          | `r ssu18_total_asvs_ps`         |
| Sparsity                            | `r ssu18_sparsity_ps`           |


## Estimate Error in Prediction

Using  the function `pime.error.prediction` we can estimate the error in prediction. For each prevalence interval, this function randomizes the samples labels into arbitrary groupings using `n` random permutations, defined by the `bootstrap` value. For each, randomized and prevalence filtered, data set the OOB error rate is calculated to estimate whether the original differences in groups of samples occur by chance. Results are in a list containing a table and a box plot summarizing the results.

```{r}
ssu18_randomized <- pime.error.prediction(ssu18_pime_ds, "TEMP",
                                          bootstrap = 100, parallel = TRUE,
                                          max.prev = 95)
```

<br/>

```{r, echo=FALSE}
ssu18_oob_error <- ssu18_randomized$`Results table`
```

```{r, echo=FALSE, layout='l-body', echo=FALSE, eval=TRUE}
datatable(ssu18_oob_error, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('Table of
                                     prevalence intervals.')),
          elementId = "bi5yoe1rk74dppcwr6wk",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = list(c(5, 10, -1), c("5", "10", "All"))
            )
          ) %>%
  DT::formatStyle(columns = colnames(ssu18_oob_error), fontSize = '80%') %>%
  DT::formatRound(columns = 1:ncol(ssu18_oob_error), digits = 4)
```

```{r, echo=FALSE}
ssu18_randomized$Plot
```

It is also possible to estimate the variation of OOB error for each prevalence interval filtering. This is done by running the random forests classification for `n` times, determined by the `bootstrap` value. The function will return a box plot figure and a table for each classification error.

```{r}
ssu18_replicated.oob.error <- pime.oob.replicate(ssu18_prevalences, "TEMP",
                                         bootstrap = 100, parallel = TRUE)
```

```{r, echo=FALSE, eval=TRUE, fig.height=5}
ssu18_obb_orig <- ssu18_replicated.oob.error$Plot
ssu18_obb_rand <- ssu18_randomized$Plot

ssu18_obb_orig <- ssu18_obb_orig +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank()) +
  labs(title = "OOB error Original data set")

ssu18_obb_rand <- ssu18_obb_rand  +
  labs(title = "OOB error Randomized data set")
ssu18_obb_orig / ssu18_obb_rand
```

To obtain the confusion matrix from random forests classification use the following:

```{r}
ssu18_prev_confuse <- paste("ssu18_best.prev$`Confusion`$", ssu18_best, sep = "")
eval(parse(text = (ssu18_prev_confuse)))
```

## Save Phyloseq PIME objects

```{r}
ssu18_ps_pime <- ssu18_prevalence_best
ssu18_ps_pime_tree <- rtree(ntaxa(ssu18_ps_pime), rooted = TRUE,
                           tip.label = taxa_names(ssu18_ps_pime))
ssu18_ps_pime <- merge_phyloseq(ssu18_ps_pime,
                               sample_data,
                               ssu18_ps_pime_tree)
saveRDS(ssu18_ps_pime, "files/pime/rdata/ssu18_ps_pime_otu.rds")
```

### Split & save by predictor variable

```{r}
data.frame(sample_data(ssu18_ps_pime))
ssu18_ps_pime_split <- pime.split.by.variable(ssu18_ps_pime, "TEMP")
saveRDS(ssu18_ps_pime_split$`0`, "files/pime/rdata/ssu18_ps_pime_otu_0.rds")
saveRDS(ssu18_ps_pime_split$`3`, "files/pime/rdata/ssu18_ps_pime_otu_3.rds")
saveRDS(ssu18_ps_pime_split$`8`, "files/pime/rdata/ssu18_ps_pime_otu_8.rds")
```

```{r, echo=FALSE, eval=TRUE}
ssu18_ps_pime_split
#ssu18_ps_pime_split$`0`
#ssu18_ps_pime_split$`3`
#ssu18_ps_pime_split$`8`
```

```{r, echo=FALSE}
ssu18_pime_preval.tax <- tax_table(ssu18_ps_pime)
write.table(ssu18_pime_preval.tax,
            file="files/pime/tables/ssu18_otu_PIME_tax_table.txt",
            sep = "\t", quote = FALSE)

ssu18_pime_preval.asv <- otu_table(t(ssu18_ps_pime))
write.table(ssu18_pime_preval.asv,
            file="files/pime/tables/ssu18_otu_PIME_otu_table.txt",
            sep = "\t", quote = FALSE)

ssu18_pime_preval.samp <- sample_data(ssu18_ps_pime)
write.table(ssu18_pime_preval.samp,
            file="files/pime/tables/ssu18_otu_PIME_sample_data.txt",
            sep = "\t", quote = FALSE)
```

## Create Ampvis2 PIME Object

```{r, code_folding=TRUE}
ssu18_otu <- data.frame(t(otu_table(ssu18_ps_pime)))
ssu18_otu[] <- lapply(ssu18_otu, as.numeric)
ssu18_otu <- as.matrix(ssu18_otu)
ssu18_tax <- as.matrix(data.frame(tax_table(ssu18_ps_pime)))
ssu18_samples <- data.frame(sample_data(ssu18_ps_pime))
ssu18_ps_amp <- merge_phyloseq(otu_table(ssu18_otu, taxa_are_rows = TRUE),
                          tax_table(ssu18_tax, ssu18_tax),
                          sample_data(ssu18_samples))
```

```{r, code_folding=TRUE}
ssu18_amp_asv_pime  <- data.frame(otu_table(ssu18_ps_amp))
ssu18_amp_asv_pime <- ssu18_amp_asv_pime %>% tibble::rownames_to_column("OTU")
ssu18_amp_tax_pime  <- data.frame(tax_table(ssu18_ps_amp))
ssu18_amp_tax_pime <- ssu18_amp_tax_pime %>% tibble::rownames_to_column("OTU")
ssu18_amp_tax_pime$ASV_SEQ <- NULL
ssu18_amp_tax_pime$Species <- ssu18_amp_tax_pime$OTU
ssu18_amp_asv_tax_pime <- left_join(ssu18_amp_asv_pime, ssu18_amp_tax_pime, by = "OTU")
ssu18_samp_data_t <- data.frame(sample_data(ssu18_ps_amp))
ssu18_amp_data_pime <- amp_load(ssu18_amp_asv_tax_pime, metadata = ssu18_samp_data_t)

ssu18_diversity_pime <-
  amp_alphadiv(
    ssu18_amp_data_pime,
    measure = "observed",
    richness = FALSE)

ssu18_diversity_pime <-
  ssu18_diversity_pime %>%
  dplyr::rename(
    "total_reads" = "Reads",
    "total_otus" = "ObservedOTUs")

ssu18_diversity_pime$coverage = cut(ssu18_diversity_pime$total_reads,
                          c(0, 5000, 25000, 100000, 200000),
                          labels = c(
                            "low (< 5k)", "medium (5-25k)",
                            "high (25-100k)", "extra_high (> 100k)"))
ssu18_diversity_pime <- ssu18_diversity_pime[order(ssu18_diversity_pime$SamName), ]
ssu18_amp_pime <- amp_load(ssu18_amp_asv_tax_pime,
                          metadata = ssu18_diversity_pime, tree = ssu18_ps_pime_tree)
ssu18_samp_pime <- ssu18_diversity_pime
ssu18_samp_pime <- ssu18_samp_pime %>% tibble::remove_rownames()
```

```{r, eval=TRUE, echo=FALSE}
ssu18_amp_pime
```

```{r, echo=FALSE}
saveRDS(ssu18_amp_pime, "files/pime/rdata/ssu18_amp_pime_otu.rds")
```

## Summary

Now, let's take a look at a table of sample information. Any header with the `_p` suffix is the *PIME filtered* data.

<br/>

```{r, echo=FALSE}
## elementId https://www.random.org/strings/
## This is for table only
ssu18_dummy_tab <- ssu18_samp_pime
ssu18_dummy_tab <- ssu18_dummy_tab %>%
  dplyr::rename("total_reads_p" = "total_reads") %>%
  dplyr::rename("total_otus_p" = "total_otus")
ssu18_dummy_tab$coverage <- NULL
#ssu18_samp_data_tab <- dplyr::left_join(ssu18_samp_data, ssu18_dummy_tab)
#ssu18_samp_data_tab <- ssu18_samp_data_tab[, c(1:7,13,8,14,9,15,10,16,11,17,12)]
```

```{r, echo=FALSE, layout="l-body-outset", eval=TRUE}
## elementId https://www.random.org/strings/
ssu18_samp_data_tab_2 <- ssu18_dummy_tab

datatable(ssu18_samp_data_tab_2, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('PIME Sample summary table.
            Use the buttons to navigate through the table or
            download a copy.')),
          elementId = "556vo9oxpic4q51hoksk",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = c(5, 15)
            )
          ) %>%
    DT::formatStyle(columns = colnames(ssu18_samp_data_tab_2),
                    fontSize = '80%')
```

Here is how the data sets changed through the PIME filtering process.

```{r, echo=FALSE, eval=TRUE}
ssu18_ps_pime_0 <- ssu18_ps_pime_split$`0`
ssu18_ps_pime_3 <- ssu18_ps_pime_split$`3`
ssu18_ps_pime_8 <- ssu18_ps_pime_split$`8`
tmp_objects <- data.frame(c("FULL data set", "Rarefied data", "PIME filtered data",
                            "PIME (0C)", "PIME (3C)", "PIME (8C)"))
tmp_samples <- c("ssu18_ps_work_otu", "ssu18_pime_ds", "ssu18_ps_pime",
                 "ssu18_ps_pime_0", "ssu18_ps_pime_3", "ssu18_ps_pime_8")
tmp_no_samp <- c()
for (i in tmp_samples) {
   tmp_get <- nsamples(get(i))
   tmp_no_samp <- c(append(tmp_no_samp, tmp_get))
}
tmp_no_samp <- data.frame(tmp_no_samp)

tmp_rc <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_rc <- c(append(tmp_rc, tmp_get))
}
tmp_rc <- data.frame(tmp_rc)
tmp_asv <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_asv <- c(append(tmp_asv, tmp_get))
}
tmp_asv <- data.frame(tmp_asv)

ssu18_otu_pime_sum <- dplyr::bind_cols(tmp_objects, tmp_samples) %>%
                      dplyr::bind_cols(., tmp_no_samp) %>%
                      dplyr::bind_cols(., tmp_rc) %>%
                      dplyr::bind_cols(., tmp_asv) %>%
  dplyr::rename("Description" = 1, "object name" = 2, "no. samples" = 3,
                "total reads" = 4, "total otus" = 5)
rm(list = ls(pattern = "tmp_"))
```

<br/>

```{r, echo=FALSE, layout="l-body-outset", eval=TRUE}
knitr::kable(ssu18_otu_pime_sum)
```

```{r, echo=FALSE}
saveRDS(ssu18_samp_pime, "files/pime/rdata/ssu18_samp_pime_otu.rds")
save.image("page_build/pime_ssu18_otu_wf.rdata")
```

```{r include=FALSE, eval=TRUE}
gdata::keep(ssu18_asv_pime_sum, its18_asv_pime_sum, ssu18_otu_pime_sum, sure = TRUE)
```

# ITS OTU Data Set

```{r, include=FALSE, eval=TRUE}
## Load to build page only #3
load("page_build/pime_its18_otu_wf.rdata")
```

```{r, include=FALSE}
## LOad only after workflow is complete #2
remove(list = ls())
load("files/pime/rdata/pime_its18_otu.rdata")
its18_samp_data <- readRDS("files/data-prep/rdata/its18_samp_data.rds")
its18_samp_data[, 7:9] <- NULL
rm(list = ls(pattern = "ssu18_"))
```

```{r include=FALSE}
## Initial Load for RUNNING PIME ONLY #1
remove(list = ls())
set.seed(119)
its18_ps_work_otu <- readRDS("files/otu/rdata/its18_ps_work_otu.rds")
its18_work_amp_otu <- readRDS("files/otu/rdata/its18_ps_work_otu_amp.rds")
```

We will use PIME (Prevalence Interval for Microbiome Evaluation) to create a filtered data set.

## Setup

First, choose a phyloseq object and a sample data frame

```{r}
its18_pime_ds <- its18_ps_work_otu
its18_which_pime <- "its18_pime_ds"
its18_pime_ds@phy_tree <- NULL
its18_pime_ds
```

```{r, echo=FALSE, eval=TRUE}
its18_ps_work_otu
```

```{r}
its18_pime_sample_d <- data.frame(rowSums(otu_table(its18_pime_ds_full)))
its18_pime_sample_d <- its18_pime_sample_d %>% dplyr::rename(total_reads = 1)

its18_pime_ds <- rarefy_even_depth(its18_pime_ds_full,
                               sample.size = min(its18_pime_sample_d$total_reads),
                               trimOTUs = TRUE, replace = FALSE,
                               rngseed = 119)
```

```
265 OTUs were removed because they are no longer 
present in any sample after random subsampling
```

```{r, echo=FALSE, eval=TRUE}
its18_pime_ds
```

The first step in PIME is to define if the microbial community presents a high relative abundance of taxa with low prevalence, which is considered as noise in PIME analysis. This is calculated by random forests analysis and is the baseline noise detection.

```{r}
its18_pime.oob.error <- pime.oob.error(its18_pime_ds, "TEMP")
```

```{r, echo=FALSE, eval=TRUE}
its18_pime.oob.error
```

## Split by Predictor Variable

```{r}
data.frame(sample_data(its18_pime_ds))
its18_per_variable_obj <- pime.split.by.variable(its18_pime_ds, "TEMP")
its18_per_variable_obj
```

```{r, echo=FALSE, eval=TRUE}
its18_per_variable_obj
```

## Calculate Prevalence Intervals

Using the output of `pime.split.by.variable`, we calculate the prevalence intervals with the function `pime.prevalence`. This function estimates the highest prevalence possible (no empty ASV table), calculates prevalence for taxa, starting at 5 maximum prevalence possible (no empty ASV table or dropping samples). After prevalence calculation, each prevalence interval are merged.

```{r}
its18_prevalences <- pime.prevalence(its18_per_variable_obj)
its18_prevalences
```

<details markdown="1">
<summary>Detailed results for all prevalences intervals  </summary>

```{r, echo=FALSE, eval=TRUE}
its18_prevalences
```
</details>

## Calculate Best Prevalence

Finally, we use the function `pime.best.prevalence` to calculate the best prevalence. The function uses randomForest to build random forests trees for samples classification and variable importance computation. It performs classifications for each prevalence interval returned by `pime.prevalence`. Variable importance is calculated, returning the Mean Decrease Accuracy (MDA), Mean Decrease Impurity (MDI), overall and by sample group, and taxonomy for each ASV. PIME keeps the top 30 variables with highest MDA each prevalence level.

```{r}
set.seed(1911)
its18_best.prev <- pime.best.prevalence(its18_prevalences, "TEMP")
```

```{r, echo=FALSE, eval=TRUE}
its18_best.prev$`OOB error`
```

```{r}
its18_what_is_best <- its18_best.prev$`OOB error`
its18_what_is_best[, c(2:4)] <- sapply(its18_what_is_best[, c(2:4)], as.numeric)
its18_what_is_best <- its18_what_is_best %>%
  dplyr::rename("OOB_error_rate" = "OOB error rate (%)")
its18_what_is_best$Interval <- str_replace_all(its18_what_is_best$Interval, "%", "")
its18_best <- with(its18_what_is_best, Interval[which.min(OOB_error_rate)])
its18_best <- paste("`", its18_best, "`", sep = "")
its18_prev_choice <- paste("its18_best.prev$`Importance`$", its18_best, sep = "")
its18_imp_best <- eval(parse(text = (its18_prev_choice)))
```

Looks like the lowest OOB error rate (%) that retains the most ASVs is `r min(its18_what_is_best$OOB_error_rate)`% from `r its18_best`. We will use this interval.

## Best Prevalence Summary

```{r, echo=FALSE}
its18_imp_best[, 13:14] <- list(NULL)
its18_imp_best <- its18_imp_best %>% dplyr::rename("OTU_ID" = "SequenceID")
```

</br>

```{r, layout='l-body-outset', echo=FALSE, eval=TRUE}
## elementId need to be unique https://www.random.org/strings/
its18_imp_best_t <- its18_imp_best %>%
  dplyr::rename("Control" = "X0", "Warm_3" = "X3", "Warm_8" = "X8")

datatable(its18_imp_best_t, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('Table of OTUs (max 30)
                                     from the chosen prevalence
                                     interval.')),
          elementId = "s5kf0c3ftddvm6uts8v2",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = list(c(5, 10, -1), c("5", "10", "All"))
            )
          ) %>%
  DT::formatStyle(columns = colnames(its18_imp_best_t), fontSize = '80%') %>%
  DT::formatRound(columns = c("Control", "Warm_3", "Warm_8",
                              "MeanDecreaseAccuracy",
                              "MeanDecreaseGini" ), digits = 4)
```

Here is a list of the top 30 OTUs

```{r, eval=TRUE, echo=FALSE}
its18_imp_best_t$ASV_ID
```
Now we need to create a phyloseq object of ASVs at this cutoff (`r its18_best`).

```{r}
its18_best_val <- str_replace_all(its18_best, "Prevalence ", "")
its18_best_val <- paste("its18_prevalences$", its18_best_val, sep = "")
its18_prevalence_best <- eval(parse(text = (its18_best_val)))
saveRDS(its18_prevalence_best, "files/pime/rdata/its18_otu_prevalence_best.rds")
```

And look at a summary of the data.

```{r, echo=FALSE, eval=TRUE}
its18_prevalence_best
```

```{r, echo=FALSE}
its18_min_read_ps <- min(readcount(its18_prevalence_best))
its18_max_read_ps <- max(readcount(its18_prevalence_best))
its18_total_reads_ps <- sum(readcount(its18_prevalence_best))
its18_mean_reads_ps <- round(mean(readcount(its18_prevalence_best)), digits = 0)
its18_median_reads_ps <- median(readcount(its18_prevalence_best))
its18_total_asvs_ps <- ntaxa(its18_prevalence_best)
its18_singleton_ps <- tryCatch(ntaxa(rare(its18_prevalence_best,
                                    detection = 1, prevalence = 0)),
                         error=function(err) NA)
its18_singleton_ps_perc <- tryCatch(round((100*(ntaxa(rare(its18_prevalence_best,
                                                     detection = 1,
                                                     prevalence = 0)) / ntaxa(ps))),
                                    digits = 3),
                              error=function(err) NA)
its18_sparsity_ps <- round(length(which(
  abundances(its18_prevalence_best) == 0))/length(abundances(its18_prevalence_best)),
  digits = 3)
```

| Metric                              | Results                         |
|-------------------------------------|---------------------------------|
| Min. number of reads                | `r its18_min_read_ps`           |
| Max. number of reads                | `r its18_max_read_ps`           |
| Total number of reads               | `r its18_total_reads_ps`        |
| Average number of reads             | `r its18_mean_reads_ps`         |
| Mean number of reads                | `r its18_mean_reads_ps`         |
| Median number of reads              | `r its18_median_reads_ps`       |
| Total OTUs                          | `r its18_total_asvs_ps`         |
| Sparsity                            | `r its18_sparsity_ps`           |


## Estimate Error in Prediction

Using  the function `pime.error.prediction` we can estimate the error in prediction. For each prevalence interval, this function randomizes the samples labels into arbitrary groupings using `n` random permutations, defined by the `bootstrap` value. For each, randomized and prevalence filtered, data set the OOB error rate is calculated to estimate whether the original differences in groups of samples occur by chance. Results are in a list containing a table and a box plot summarizing the results.

```{r}
its18_randomized <- pime.error.prediction(its18_pime_ds, "TEMP",
                                          bootstrap = 100, parallel = TRUE,
                                          max.prev = 95)
```

<br/>

```{r, echo=FALSE}
its18_oob_error <- its18_randomized$`Results table`
```

```{r, echo=FALSE, layout='l-body', echo=FALSE, eval=TRUE}
datatable(its18_oob_error, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('Table of
                                     prevalence interval.')),
          elementId = "l2jx58zcwtmxp4woac0l",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = list(c(5, 10, -1), c("5", "10", "All"))
            )
          ) %>%
  DT::formatStyle(columns = colnames(its18_oob_error), fontSize = '80%') %>%
  DT::formatRound(columns = 1:ncol(its18_oob_error), digits = 4)
```

```{r, echo=FALSE}
its18_randomized$Plot
```

It is also possible to estimate the variation of OOB error for each prevalence interval filtering. This is done by running the random forests classification for `n` times, determined by the `bootstrap` value. The function will return a box plot figure and a table for each classification error.

```{r}
its18_replicated.oob.error <- pime.oob.replicate(its18_prevalences, "TEMP",
                                         bootstrap = 100, parallel = TRUE)
```

```{r, echo=FALSE, eval=TRUE, fig.height=5}
its18_obb_orig <- its18_replicated.oob.error$Plot
its18_obb_rand <- its18_randomized$Plot

its18_obb_orig <- its18_obb_orig +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank()) +
  labs(title = "OOB error Original data set")

its18_obb_rand <- its18_obb_rand  +
  labs(title = "OOB error Randomized data set")
its18_obb_orig / its18_obb_rand
```

To obtain the confusion matrix from random forests classification use the following:

```{r}
its18_prev_confuse <- paste("its18_best.prev$`Confusion`$", its18_best, sep = "")
eval(parse(text = (its18_prev_confuse)))
```

## Save Phyloseq PIME objects

```{r}
its18_ps_pime <- its18_prevalence_best
its18_ps_pime_tree <- rtree(ntaxa(its18_ps_pime), rooted = TRUE,
                           tip.label = taxa_names(its18_ps_pime))
its18_ps_pime <- merge_phyloseq(its18_ps_pime,
                               sample_data,
                               its18_ps_pime_tree)
saveRDS(its18_ps_pime, "files/pime/rdata/its18_ps_pime_otu.rds")
```

### Split & save by predictor variable

```{r}
data.frame(sample_data(its18_ps_pime))
its18_ps_pime_split <- pime.split.by.variable(its18_ps_pime, "TEMP")
saveRDS(its18_ps_pime_split$`0`, "files/pime/rdata/its18_ps_pime_otu_0.rds")
saveRDS(its18_ps_pime_split$`3`, "files/pime/rdata/its18_ps_pime_otu_3.rds")
saveRDS(its18_ps_pime_split$`8`, "files/pime/rdata/its18_ps_pime_otu_8.rds")
```

```{r, echo=FALSE, eval=TRUE}
its18_ps_pime_split
#its18_ps_pime_split$`0`
#its18_ps_pime_split$`4`
#its18_ps_pime_split$`8`
```

```{r, echo=FALSE}
its18_pime_preval.tax <- tax_table(its18_ps_pime)
write.table(its18_pime_preval.tax,
            file="files/pime/tables/its18_otu_PIME_tax_table.txt",
            sep = "\t", quote = FALSE)

its18_pime_preval.asv <- otu_table(t(its18_ps_pime))
write.table(its18_pime_preval.asv,
            file="files/pime/tables/its18_otu_PIME_otu_table.txt",
            sep = "\t", quote = FALSE)

its18_pime_preval.samp <- sample_data(its18_ps_pime)
write.table(its18_pime_preval.samp,
            file="files/pime/tables/its18_otu_PIME_sample_data.txt",
            sep = "\t", quote = FALSE)
```

## Create Ampvis2 PIME Object

```{r, code_folding=TRUE}
its18_otu <- data.frame(t(otu_table(its18_ps_pime)))
its18_otu[] <- lapply(its18_otu, as.numeric)
its18_otu <- as.matrix(its18_otu)
its18_tax <- as.matrix(data.frame(tax_table(its18_ps_pime)))
its18_samples <- data.frame(sample_data(its18_ps_pime))
its18_ps_amp <- merge_phyloseq(otu_table(its18_otu, taxa_are_rows = TRUE),
                          tax_table(its18_tax, its18_tax),
                          sample_data(its18_samples))
```

```{r, code_folding=TRUE}
its18_amp_asv_pime  <- data.frame(otu_table(its18_ps_amp))
its18_amp_asv_pime <- its18_amp_asv_pime %>% tibble::rownames_to_column("OTU")
its18_amp_tax_pime  <- data.frame(tax_table(its18_ps_amp))
its18_amp_tax_pime <- its18_amp_tax_pime %>% tibble::rownames_to_column("OTU")
its18_amp_tax_pime$ASV_SEQ <- NULL
its18_amp_tax_pime$Species <- its18_amp_tax_pime$OTU
its18_amp_asv_tax_pime <- left_join(its18_amp_asv_pime, its18_amp_tax_pime, by = "OTU")
its18_samp_data_t <- data.frame(sample_data(its18_ps_amp))
its18_amp_data_pime <- amp_load(its18_amp_asv_tax_pime, metadata = its18_samp_data_t)

its18_diversity_pime <-
  amp_alphadiv(
    its18_amp_data_pime,
    measure = "observed",
    richness = FALSE)

its18_diversity_pime <-
  its18_diversity_pime %>%
  dplyr::rename(
    "total_reads" = "Reads",
    "total_otus" = "ObservedOTUs")

its18_diversity_pime$coverage = cut(its18_diversity_pime$total_reads,
                          c(0, 5000, 25000, 100000, 200000),
                          labels = c(
                            "low (< 5k)", "medium (5-25k)",
                            "high (25-100k)", "extra_high (> 100k)"))
its18_diversity_pime <- its18_diversity_pime[order(its18_diversity_pime$SamName), ]
its18_amp_pime <- amp_load(its18_amp_asv_tax_pime,
                          metadata = its18_diversity_pime, tree = its18_ps_pime_tree)
its18_samp_pime <- its18_diversity_pime
its18_samp_pime <- its18_samp_pime %>% tibble::remove_rownames()
```

```{r, eval=TRUE, echo=FALSE}
its18_amp_pime
```

```{r, echo=FALSE}
saveRDS(its18_amp_pime, "files/pime/rdata/its18_amp_pime_otu.rds")
```

## Summary

Now, let's take a look at a table of sample information. Any header with the `_p` suffix is the *PIME filtered* data.

<br/>

```{r, echo=FALSE}
## elementId https://www.random.org/strings/
## This is for table only
its18_dummy_tab <- its18_samp_pime
its18_dummy_tab <- its18_dummy_tab %>%
  dplyr::rename("total_reads_p" = "total_reads") %>%
  dplyr::rename("total_ots_p" = "total_otus")
its18_dummy_tab$coverage <- NULL
#its18_samp_data_tab <- dplyr::left_join(its18_samp_data, its18_dummy_tab)
#its18_samp_data_tab <- its18_samp_data_tab[, c(1:7,13,8,14,9,15,10,16,11,17,12)]
```

```{r, echo=FALSE, layout="l-body-outset", eval=TRUE}
## elementId https://www.random.org/strings/
its18_samp_data_tab_2 <- its18_dummy_tab
datatable(its18_samp_data_tab_2, width = "100%", escape = FALSE,
          rownames = FALSE, filter = 'top',
          caption = htmltools::tags$caption(
            style = 'caption-side: bottom; text-align: left;',
            'Table: ', htmltools::em('PIME sample summary table.
            Use the buttons to navigate through the table or
            download a copy.')),
          elementId = "yt9c0tca4c0l28yfj17m",
          extensions = 'Buttons', options = list(
            scrollX = TRUE,
            dom = 'Blfrtip',
            buttons = c('copy', 'csv', 'excel'),
            pageLength = 5,
            lengthMenu = c(5, 15)
            )
          ) %>%
    DT::formatStyle(columns = colnames(its18_samp_data_tab_2),
                    fontSize = '80%')
```

Here is how the data sets changed through the PIME filtering process.

```{r, echo=FALSE, eval=TRUE}
its18_ps_pime_0 <- its18_ps_pime_split$`0`
its18_ps_pime_3 <- its18_ps_pime_split$`3`
its18_ps_pime_8 <- its18_ps_pime_split$`8`
tmp_objects <- data.frame(c("FULL data set", "Rarefied data", "PIME filtered data",
                            "PIME (0C)", "PIME (3C)", "PIME (8C)"))
tmp_samples <- c("its18_ps_work_otu", "its18_pime_ds", "its18_ps_pime",
                 "its18_ps_pime_0", "its18_ps_pime_3", "its18_ps_pime_8")
tmp_no_samp <- c()
for (i in tmp_samples) {
   tmp_get <- nsamples(get(i))
   tmp_no_samp <- c(append(tmp_no_samp, tmp_get))
}
tmp_no_samp <- data.frame(tmp_no_samp)

tmp_rc <- c()
for (i in tmp_samples) {
   tmp_get <- sum(readcount(get(i)))
   tmp_rc <- c(append(tmp_rc, tmp_get))
}
tmp_rc <- data.frame(tmp_rc)
tmp_asv <- c()
for (i in tmp_samples) {
   tmp_get <- ntaxa(get(i))
   tmp_asv <- c(append(tmp_asv, tmp_get))
}
tmp_asv <- data.frame(tmp_asv)

its18_otu_pime_sum <- dplyr::bind_cols(tmp_objects, tmp_samples) %>%
                      dplyr::bind_cols(., tmp_no_samp) %>%
                      dplyr::bind_cols(., tmp_rc) %>%
                      dplyr::bind_cols(., tmp_asv) %>%
  dplyr::rename("Description" = 1, "object name" = 2, "no. samples" = 3,
                "total reads" = 4, "total otus" = 5)
rm(list = ls(pattern = "tmp_"))
```

<br/>

```{r, echo=FALSE, layout="l-body-outset", eval=TRUE}
knitr::kable(its18_otu_pime_sum)
```


```{r, echo=FALSE}
saveRDS(its18_samp_pime, "files/pime/rdata/its18_samp_pime_otu.rds")
save.image("page_build/pime_its18_otu_wf.rdata")
```

```{r, echo=FALSE}
gdata::keep(ssu18_asv_pime_sum, its18_asv_pime_sum, ssu18_otu_pime_sum, its18_otu_pime_sum, sure = TRUE)
save.image("page_build/pime-summary_high_temp_pime_wf.rdata")
```

```{r, echo=FALSE, eval=TRUE}
remove(list = ls())
```

##  Source Code {.appendix}

The source code for this page can be accessed on GitHub by [clicking this link](https://github.com/sweltr/high-temp/blob/master/pime.Rmd). Please note, that in order to process the data *and*  build the website, we needed to run the workflow and get the results. Then hard code the results and turn off the individual commands. So the raw file for this page is a bit messy---you have been warned.
